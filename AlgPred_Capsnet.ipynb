{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "AlgPred_Capsnet.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [
        "ErgjcKMIFPVW",
        "uhTBU77HY8S2",
        "PSv40vY-PJNB",
        "qnsdxi0URJ-5",
        "fPwHK_tM8LL3",
        "jqNkuULALbo2",
        "bCyu2iZc9KZ0",
        "cgdRJnAu9W8s",
        "9JM4cU8MIbxp",
        "T2N_H0chQ1qW",
        "_zniHofB1K_p"
      ],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "nRTsMJbM6jnu",
        "colab_type": "code",
        "outputId": "45b97a9b-bc00-48b6-f7fa-053e32c6487d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "import keras.backend as K\n",
        "import tensorflow as tf\n",
        "from keras import initializers, layers, regularizers\n",
        "import tensorflow as tf\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "from keras.utils import multi_gpu_model\n",
        "import numpy as np\n",
        "import datetime, os"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qww2o3W-DV7Q",
        "colab_type": "text"
      },
      "source": [
        "## Uploading Training Set (From local machine)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XnAN7oTbCpDz",
        "colab_type": "code",
        "outputId": "bb6dedb9-3d63-4073-a6a1-dd21c67f05f5",
        "colab": {
          "resources": {
            "http://localhost:8080/nbextensions/google.colab/files.js": {
              "data": "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",
              "ok": true,
              "headers": [
                [
                  "content-type",
                  "application/javascript"
                ]
              ],
              "status": 200,
              "status_text": ""
            }
          },
          "base_uri": "https://localhost:8080/",
          "height": 74
        }
      },
      "source": [
        "from google.colab import files\n",
        "uploaded = files.upload()\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-173e2abd-e80f-40a5-81a5-65b14366806f\" name=\"files[]\" multiple disabled />\n",
              "     <output id=\"result-173e2abd-e80f-40a5-81a5-65b14366806f\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script src=\"/nbextensions/google.colab/files.js\"></script> "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Saving AComp1.csv to AComp1.csv\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gq1py0rACrLM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pandas as pd\n",
        "import io\n",
        "df = pd.read_csv(io.BytesIO(uploaded['AComp1.csv']))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xm07NT-yD8Qq",
        "colab_type": "code",
        "outputId": "9476a17a-9608-41c1-80c8-f8710d1789ad",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 224
        }
      },
      "source": [
        "df.head()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>A</th>\n",
              "      <th>C</th>\n",
              "      <th>D</th>\n",
              "      <th>E</th>\n",
              "      <th>F</th>\n",
              "      <th>G</th>\n",
              "      <th>H</th>\n",
              "      <th>I</th>\n",
              "      <th>K</th>\n",
              "      <th>L</th>\n",
              "      <th>M</th>\n",
              "      <th>N</th>\n",
              "      <th>P</th>\n",
              "      <th>Q</th>\n",
              "      <th>R</th>\n",
              "      <th>S</th>\n",
              "      <th>T</th>\n",
              "      <th>V</th>\n",
              "      <th>W</th>\n",
              "      <th>Y</th>\n",
              "      <th>Label</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>5.41</td>\n",
              "      <td>0.00</td>\n",
              "      <td>8.11</td>\n",
              "      <td>11.49</td>\n",
              "      <td>6.08</td>\n",
              "      <td>3.38</td>\n",
              "      <td>2.03</td>\n",
              "      <td>9.46</td>\n",
              "      <td>13.51</td>\n",
              "      <td>10.14</td>\n",
              "      <td>5.41</td>\n",
              "      <td>2.70</td>\n",
              "      <td>2.03</td>\n",
              "      <td>4.05</td>\n",
              "      <td>4.05</td>\n",
              "      <td>2.03</td>\n",
              "      <td>2.70</td>\n",
              "      <td>5.41</td>\n",
              "      <td>0.00</td>\n",
              "      <td>2.03</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>6.88</td>\n",
              "      <td>0.00</td>\n",
              "      <td>5.62</td>\n",
              "      <td>8.75</td>\n",
              "      <td>5.00</td>\n",
              "      <td>8.75</td>\n",
              "      <td>2.50</td>\n",
              "      <td>8.12</td>\n",
              "      <td>9.38</td>\n",
              "      <td>5.62</td>\n",
              "      <td>1.25</td>\n",
              "      <td>5.62</td>\n",
              "      <td>5.00</td>\n",
              "      <td>1.25</td>\n",
              "      <td>1.88</td>\n",
              "      <td>6.88</td>\n",
              "      <td>5.62</td>\n",
              "      <td>7.50</td>\n",
              "      <td>0.00</td>\n",
              "      <td>4.38</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>9.84</td>\n",
              "      <td>0.26</td>\n",
              "      <td>5.96</td>\n",
              "      <td>5.44</td>\n",
              "      <td>5.18</td>\n",
              "      <td>5.70</td>\n",
              "      <td>0.78</td>\n",
              "      <td>5.18</td>\n",
              "      <td>6.99</td>\n",
              "      <td>10.10</td>\n",
              "      <td>2.85</td>\n",
              "      <td>4.40</td>\n",
              "      <td>3.63</td>\n",
              "      <td>3.63</td>\n",
              "      <td>2.07</td>\n",
              "      <td>7.77</td>\n",
              "      <td>10.62</td>\n",
              "      <td>4.92</td>\n",
              "      <td>0.52</td>\n",
              "      <td>4.15</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>3.91</td>\n",
              "      <td>1.95</td>\n",
              "      <td>0.33</td>\n",
              "      <td>1.63</td>\n",
              "      <td>3.26</td>\n",
              "      <td>2.28</td>\n",
              "      <td>1.30</td>\n",
              "      <td>5.21</td>\n",
              "      <td>0.65</td>\n",
              "      <td>8.14</td>\n",
              "      <td>0.65</td>\n",
              "      <td>1.95</td>\n",
              "      <td>15.64</td>\n",
              "      <td>34.20</td>\n",
              "      <td>1.95</td>\n",
              "      <td>5.21</td>\n",
              "      <td>2.28</td>\n",
              "      <td>5.21</td>\n",
              "      <td>0.33</td>\n",
              "      <td>3.91</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>6.99</td>\n",
              "      <td>6.99</td>\n",
              "      <td>1.40</td>\n",
              "      <td>6.99</td>\n",
              "      <td>3.50</td>\n",
              "      <td>4.90</td>\n",
              "      <td>0.70</td>\n",
              "      <td>3.50</td>\n",
              "      <td>2.10</td>\n",
              "      <td>9.79</td>\n",
              "      <td>4.20</td>\n",
              "      <td>5.59</td>\n",
              "      <td>9.79</td>\n",
              "      <td>6.29</td>\n",
              "      <td>4.20</td>\n",
              "      <td>5.59</td>\n",
              "      <td>6.29</td>\n",
              "      <td>5.59</td>\n",
              "      <td>1.40</td>\n",
              "      <td>4.20</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "      A     C     D      E     F     G  ...     S      T     V     W     Y  Label\n",
              "0  5.41  0.00  8.11  11.49  6.08  3.38  ...  2.03   2.70  5.41  0.00  2.03      1\n",
              "1  6.88  0.00  5.62   8.75  5.00  8.75  ...  6.88   5.62  7.50  0.00  4.38      1\n",
              "2  9.84  0.26  5.96   5.44  5.18  5.70  ...  7.77  10.62  4.92  0.52  4.15      1\n",
              "3  3.91  1.95  0.33   1.63  3.26  2.28  ...  5.21   2.28  5.21  0.33  3.91      1\n",
              "4  6.99  6.99  1.40   6.99  3.50  4.90  ...  5.59   6.29  5.59  1.40  4.20      1\n",
              "\n",
              "[5 rows x 21 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J4lxB4RDFHF9",
        "colab_type": "code",
        "outputId": "a530521d-8b1c-495e-a0f6-75180c1470e6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "df.shape"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(16120, 21)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cdL3KCtIFtQm",
        "colab_type": "text"
      },
      "source": [
        "## Uploading the Test set (on local machine)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QK_tBvzmFyEj",
        "colab_type": "code",
        "outputId": "6062ad20-e188-403d-adeb-2608360372c9",
        "colab": {
          "resources": {
            "http://localhost:8080/nbextensions/google.colab/files.js": {
              "data": "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",
              "ok": true,
              "headers": [
                [
                  "content-type",
                  "application/javascript"
                ]
              ],
              "status": 200,
              "status_text": ""
            }
          },
          "base_uri": "https://localhost:8080/",
          "height": 74
        }
      },
      "source": [
        "from google.colab import files\n",
        "uploaded2 = files.upload()\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-17b113a3-fe94-4bab-94d8-23dabca55e9e\" name=\"files[]\" multiple disabled />\n",
              "     <output id=\"result-17b113a3-fe94-4bab-94d8-23dabca55e9e\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script src=\"/nbextensions/google.colab/files.js\"></script> "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Saving test_AComp1.csv to test_AComp1.csv\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t9xr_mpjFyCw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df2 = pd.read_csv(io.BytesIO(uploaded2['test_AComp1.csv']))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P2tPcTmqFyAm",
        "colab_type": "code",
        "outputId": "5f5c7faf-f091-4634-d86b-af4fda852a86",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 224
        }
      },
      "source": [
        "df2.head()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>A</th>\n",
              "      <th>C</th>\n",
              "      <th>D</th>\n",
              "      <th>E</th>\n",
              "      <th>F</th>\n",
              "      <th>G</th>\n",
              "      <th>H</th>\n",
              "      <th>I</th>\n",
              "      <th>K</th>\n",
              "      <th>L</th>\n",
              "      <th>M</th>\n",
              "      <th>N</th>\n",
              "      <th>P</th>\n",
              "      <th>Q</th>\n",
              "      <th>R</th>\n",
              "      <th>S</th>\n",
              "      <th>T</th>\n",
              "      <th>V</th>\n",
              "      <th>W</th>\n",
              "      <th>Y</th>\n",
              "      <th>Label</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>5.77</td>\n",
              "      <td>4.62</td>\n",
              "      <td>3.85</td>\n",
              "      <td>5.77</td>\n",
              "      <td>2.31</td>\n",
              "      <td>8.85</td>\n",
              "      <td>3.46</td>\n",
              "      <td>3.46</td>\n",
              "      <td>6.92</td>\n",
              "      <td>10.38</td>\n",
              "      <td>3.46</td>\n",
              "      <td>3.46</td>\n",
              "      <td>6.92</td>\n",
              "      <td>3.46</td>\n",
              "      <td>3.46</td>\n",
              "      <td>5.77</td>\n",
              "      <td>5.77</td>\n",
              "      <td>7.31</td>\n",
              "      <td>3.08</td>\n",
              "      <td>1.92</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>5.99</td>\n",
              "      <td>2.30</td>\n",
              "      <td>8.29</td>\n",
              "      <td>5.99</td>\n",
              "      <td>6.91</td>\n",
              "      <td>7.83</td>\n",
              "      <td>0.92</td>\n",
              "      <td>7.83</td>\n",
              "      <td>5.99</td>\n",
              "      <td>7.83</td>\n",
              "      <td>0.92</td>\n",
              "      <td>4.61</td>\n",
              "      <td>4.61</td>\n",
              "      <td>2.30</td>\n",
              "      <td>5.07</td>\n",
              "      <td>7.37</td>\n",
              "      <td>5.07</td>\n",
              "      <td>7.37</td>\n",
              "      <td>0.92</td>\n",
              "      <td>1.84</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>11.49</td>\n",
              "      <td>6.76</td>\n",
              "      <td>3.38</td>\n",
              "      <td>4.05</td>\n",
              "      <td>2.70</td>\n",
              "      <td>6.76</td>\n",
              "      <td>2.03</td>\n",
              "      <td>4.05</td>\n",
              "      <td>2.03</td>\n",
              "      <td>10.14</td>\n",
              "      <td>2.70</td>\n",
              "      <td>2.70</td>\n",
              "      <td>6.76</td>\n",
              "      <td>5.41</td>\n",
              "      <td>6.08</td>\n",
              "      <td>6.08</td>\n",
              "      <td>5.41</td>\n",
              "      <td>6.08</td>\n",
              "      <td>0.68</td>\n",
              "      <td>4.73</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>10.45</td>\n",
              "      <td>5.97</td>\n",
              "      <td>3.73</td>\n",
              "      <td>2.24</td>\n",
              "      <td>0.75</td>\n",
              "      <td>6.72</td>\n",
              "      <td>2.24</td>\n",
              "      <td>4.48</td>\n",
              "      <td>3.73</td>\n",
              "      <td>10.45</td>\n",
              "      <td>2.99</td>\n",
              "      <td>7.46</td>\n",
              "      <td>5.22</td>\n",
              "      <td>4.48</td>\n",
              "      <td>5.97</td>\n",
              "      <td>8.96</td>\n",
              "      <td>3.73</td>\n",
              "      <td>8.96</td>\n",
              "      <td>0.00</td>\n",
              "      <td>1.49</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>5.04</td>\n",
              "      <td>6.20</td>\n",
              "      <td>3.10</td>\n",
              "      <td>9.30</td>\n",
              "      <td>1.94</td>\n",
              "      <td>6.59</td>\n",
              "      <td>0.78</td>\n",
              "      <td>5.43</td>\n",
              "      <td>2.71</td>\n",
              "      <td>5.81</td>\n",
              "      <td>1.94</td>\n",
              "      <td>2.71</td>\n",
              "      <td>2.71</td>\n",
              "      <td>17.05</td>\n",
              "      <td>9.69</td>\n",
              "      <td>9.30</td>\n",
              "      <td>3.49</td>\n",
              "      <td>4.65</td>\n",
              "      <td>0.00</td>\n",
              "      <td>1.55</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "       A     C     D     E     F     G  ...     S     T     V     W     Y  Label\n",
              "0   5.77  4.62  3.85  5.77  2.31  8.85  ...  5.77  5.77  7.31  3.08  1.92      1\n",
              "1   5.99  2.30  8.29  5.99  6.91  7.83  ...  7.37  5.07  7.37  0.92  1.84      1\n",
              "2  11.49  6.76  3.38  4.05  2.70  6.76  ...  6.08  5.41  6.08  0.68  4.73      1\n",
              "3  10.45  5.97  3.73  2.24  0.75  6.72  ...  8.96  3.73  8.96  0.00  1.49      1\n",
              "4   5.04  6.20  3.10  9.30  1.94  6.59  ...  9.30  3.49  4.65  0.00  1.55      1\n",
              "\n",
              "[5 rows x 21 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1ZgSr9nTqVK6",
        "colab_type": "code",
        "outputId": "cf630425-413d-4dcd-c230-eea1a37899f6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "df2.shape"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(4030, 21)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AT7uc3O-EO0H",
        "colab_type": "text"
      },
      "source": [
        "# Defining the Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uRjzldT7EAtP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "class Length(layers.Layer):\n",
        "    \"\"\"\n",
        "    Compute the length of vectors. This is used to compute a Tensor that has the same shape with y_true in margin_loss\n",
        "    inputs: shape=[dim_1, ..., dim_{n-1}, dim_n]\n",
        "    output: shape=[dim_1, ..., dim_{n-1}]\n",
        "    \"\"\"\n",
        "    def call(self, inputs, **kwargs):\n",
        "        return K.sqrt(K.sum(K.square(inputs), -1))\n",
        "\n",
        "    def compute_output_shape(self, input_shape):\n",
        "        return input_shape[:-1]\n",
        "\n",
        "class Mask(layers.Layer):\n",
        "    \"\"\"\n",
        "    Mask a Tensor with shape=[None, d1, d2] by the max value in axis=1.\n",
        "    Output shape: [None, d2]\n",
        "    \"\"\"\n",
        "    def call(self, inputs, **kwargs):\n",
        "        # use true label to select target capsule, shape=[batch_size, num_capsule]\n",
        "        if type(inputs) is list:  # true label is provided with shape = [batch_size, n_classes], i.e. one-hot code.\n",
        "            assert len(inputs) == 2\n",
        "            inputs, mask = inputs\n",
        "        else:  # if no true label, mask by the max length of vectors of capsules\n",
        "            x = inputs\n",
        "            # Enlarge the range of values in x to make max(new_x)=1 and others < 0\n",
        "            x = (x - K.max(x, 1, True)) / K.epsilon() + 1\n",
        "            mask = K.clip(x, 0, 1)  # the max value in x clipped to 1 and other to 0\n",
        "\n",
        "        # masked inputs, shape = [batch_size, dim_vector]\n",
        "        inputs_masked = K.batch_dot(inputs, mask, [1, 1])\n",
        "        return inputs_masked\n",
        "\n",
        "    def compute_output_shape(self, input_shape):\n",
        "        if type(input_shape[0]) is tuple:  # true label provided\n",
        "            return tuple([None, input_shape[0][-1]])\n",
        "        else:\n",
        "            return tuple([None, input_shape[-1]])\n",
        "\n",
        "\n",
        "def squash(vectors, axis=-1):\n",
        "    \"\"\"\n",
        "    The non-linear activation used in Capsule. It drives the length of a large vector to near 1 and small vector to 0\n",
        "    :param vectors: some vectors to be squashed, N-dim tensor\n",
        "    :param axis: the axis to squash\n",
        "    :return: a Tensor with same shape as input vectors\n",
        "    \"\"\"\n",
        "    s_squared_norm = K.sum(K.square(vectors), axis, keepdims=True)\n",
        "    scale = s_squared_norm / (1 + s_squared_norm) / K.sqrt(s_squared_norm)\n",
        "    return scale * vectors\n",
        "\n",
        "\n",
        "class CapsuleLayer(layers.Layer):\n",
        "    \"\"\"\n",
        "    The capsule layer. It is similar to Dense layer. Dense layer has `in_num` inputs, each is a scalar, the output of the \n",
        "    neuron from the former layer, and it has `out_num` output neurons. CapsuleLayer just expand the output of the neuron\n",
        "    from scalar to vector. So its input shape = [None, input_num_capsule, input_dim_vector] and output shape = \\\n",
        "    [None, num_capsule, dim_vector]. For Dense Layer, input_dim_vector = dim_vector = 1.\n",
        "    \n",
        "    :param num_capsule: number of capsules in this layer\n",
        "    :param dim_vector: dimension of the output vectors of the capsules in this layer\n",
        "    :param num_routings: number of iterations for the routing algorithm\n",
        "    \"\"\"\n",
        "    def __init__(self, num_capsule, dim_vector, num_routing=3,\n",
        "                 kernel_initializer='glorot_uniform',\n",
        "                 bias_initializer='zeros',\n",
        "                 **kwargs):\n",
        "        super(CapsuleLayer, self).__init__(**kwargs)\n",
        "        self.num_capsule = num_capsule\n",
        "        self.dim_vector = dim_vector\n",
        "        self.num_routing = num_routing\n",
        "        self.kernel_initializer = initializers.get(kernel_initializer)\n",
        "        self.bias_initializer = initializers.get(bias_initializer)\n",
        "\n",
        "    def build(self, input_shape):\n",
        "        assert len(input_shape) >= 3, \"The input Tensor should have shape=[None, input_num_capsule, input_dim_vector]\"\n",
        "        self.input_num_capsule = input_shape[1]\n",
        "        self.input_dim_vector = input_shape[2]\n",
        "\n",
        "        # Transform matrix\n",
        "        self.W = self.add_weight(shape=[self.input_num_capsule, self.num_capsule, self.input_dim_vector, self.dim_vector],\n",
        "                                 initializer=self.kernel_initializer,\n",
        "                                 name='W')\n",
        "\n",
        "        # Coupling coefficient. The redundant dimensions are just to facilitate subsequent matrix calculation.\n",
        "        self.bias = self.add_weight(shape=[1, self.input_num_capsule, self.num_capsule, 1, 1],\n",
        "                                    initializer=self.bias_initializer,\n",
        "                                    name='bias',\n",
        "                                    trainable=False)\n",
        "        self.built = True\n",
        "\n",
        "    def call(self, inputs, training=None):\n",
        "        # inputs.shape=[None, input_num_capsule, input_dim_vector]\n",
        "        # Expand dims to [None, input_num_capsule, 1, 1, input_dim_vector]\n",
        "        inputs_expand = K.expand_dims(K.expand_dims(inputs, 2), 2)\n",
        "\n",
        "        # Replicate num_capsule dimension to prepare being multiplied by W\n",
        "        # Now it has shape = [None, input_num_capsule, num_capsule, 1, input_dim_vector]\n",
        "        inputs_tiled = K.tile(inputs_expand, [1, 1, self.num_capsule, 1, 1])\n",
        "\n",
        "        \"\"\"  \n",
        "        # Compute `inputs * W` by expanding the first dim of W. More time-consuming and need batch_size.\n",
        "        # Now W has shape  = [batch_size, input_num_capsule, num_capsule, input_dim_vector, dim_vector]\n",
        "        w_tiled = K.tile(K.expand_dims(self.W, 0), [self.batch_size, 1, 1, 1, 1])\n",
        "        \n",
        "        # Transformed vectors, inputs_hat.shape = [None, input_num_capsule, num_capsule, 1, dim_vector]\n",
        "        inputs_hat = K.batch_dot(inputs_tiled, w_tiled, [4, 3])\n",
        "        \"\"\"\n",
        "        # Compute `inputs * W` by scanning inputs_tiled on dimension 0. This is faster but requires Tensorflow.\n",
        "        # inputs_hat.shape = [None, input_num_capsule, num_capsule, 1, dim_vector]\n",
        "        inputs_hat = tf.scan(lambda ac, x: K.batch_dot(x, self.W, [3, 2]),\n",
        "                             elems=inputs_tiled,\n",
        "                             initializer=K.zeros([self.input_num_capsule, self.num_capsule, 1, self.dim_vector]))\n",
        "        \"\"\"\n",
        "        # Routing algorithm V1. Use tf.while_loop in a dynamic way.\n",
        "        def body(i, b, outputs):\n",
        "            c = tf.nn.softmax(self.bias, dim=2)  # dim=2 is the num_capsule dimension\n",
        "            outputs = squash(K.sum(c * inputs_hat, 1, keepdims=True))\n",
        "            b = b + K.sum(inputs_hat * outputs, -1, keepdims=True)\n",
        "            return [i-1, b, outputs]\n",
        "\n",
        "        cond = lambda i, b, inputs_hat: i > 0\n",
        "        loop_vars = [K.constant(self.num_routing), self.bias, K.sum(inputs_hat, 1, keepdims=True)]\n",
        "        _, _, outputs = tf.while_loop(cond, body, loop_vars)\n",
        "        \"\"\"\n",
        "        # Routing algorithm V2. Use iteration. V2 and V1 both work without much difference on performance\n",
        "        assert self.num_routing > 0, 'The num_routing should be > 0.'\n",
        "        for i in range(self.num_routing):\n",
        "            c = tf.nn.softmax(self.bias, dim=2)  # dim=2 is the num_capsule dimension\n",
        "            # outputs.shape=[None, 1, num_capsule, 1, dim_vector]\n",
        "            outputs = squash(K.sum(c * inputs_hat, 1, keepdims=True))\n",
        "\n",
        "            # last iteration needs not compute bias which will not be passed to the graph any more anyway.\n",
        "            if i != self.num_routing - 1:\n",
        "                # self.bias = K.update_add(self.bias, K.sum(inputs_hat * outputs, [0, -1], keepdims=True))\n",
        "                self.bias += K.sum(inputs_hat * outputs, -1, keepdims=True)\n",
        "            # tf.summary.histogram('BigBee', self.bias)  # for debugging\n",
        "        return K.reshape(outputs, [-1, self.num_capsule, self.dim_vector])\n",
        "\n",
        "    def compute_output_shape(self, input_shape):\n",
        "        return tuple([None, self.num_capsule, self.dim_vector])\n",
        "\n",
        "\n",
        "def PrimaryCap(inputs, dim_vector, n_channels, kernel_size, strides, padding):\n",
        "    \"\"\"\n",
        "    Apply Conv1D `n_channels` times and concatenate all capsules\n",
        "    :param inputs: 4D tensor, shape=[None, width, height, channels]\n",
        "    :param dim_vector: the dim of the output vector of capsule\n",
        "    :param n_channels: the number of types of capsules\n",
        "    :return: output tensor, shape=[None, num_capsule, dim_vector]\n",
        "    \"\"\"\n",
        "    output = layers.Conv1D(filters=dim_vector*n_channels, kernel_size=kernel_size, strides=strides, padding=padding)(inputs)\n",
        "    outputs = layers.Reshape(target_shape=[-1, dim_vector])(output)\n",
        "    return layers.Lambda(squash)(outputs)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zpRG3VCPEUlC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# BUILDING THE MODEL\n",
        "\n",
        "from keras import layers, models\n",
        "from keras import backend as K\n",
        "from keras.utils import to_categorical\n",
        "from keras import callbacks\n",
        "\n",
        "\n",
        "def CapsNet(input_shape, n_class, num_routing):\n",
        "    \"\"\"\n",
        "    A Capsule Network on MNIST.\n",
        "    :param input_shape: data shape, 4d, [None, width, height, channels]\n",
        "    :param n_class: number of classes\n",
        "    :param num_routing: number of routing iterations\n",
        "    :return: A Keras Model with 2 inputs and 2 outputs\n",
        "    \"\"\"\n",
        "    x = layers.Input(shape=input_shape)\n",
        "    print(input_shape, x.shape)\n",
        "\n",
        "    # Layer 1: Just a conventional Conv1D layer\n",
        "    #Add Batch Norm before Activation\n",
        "    \n",
        "    bn = layers.normalization.BatchNormalization()(x)\n",
        "    conv1 = layers.Conv1D(filters=32, kernel_size=4, strides=1, padding='valid', activation='relu', name='conv1')(bn)\n",
        "\n",
        "    # Layer 2: Conv1D layer with `squash` activation, then reshape to [None, num_capsule, dim_vector]\n",
        "    primarycaps = PrimaryCap(conv1, dim_vector=8, n_channels=32, kernel_size=9, strides=2, padding='valid')\n",
        "\n",
        "    # Layer 3: Capsule layer. Routing algorithm works here.\n",
        "    digitcaps = CapsuleLayer(num_capsule=n_class, dim_vector=16, num_routing=num_routing, name='digitcaps')(primarycaps)\n",
        "\n",
        "    # Layer 4: This is an auxiliary layer to replace each capsule with its length. Just to match the true label's shape.\n",
        "    # If using tensorflow, this will not be necessary. :)\n",
        "    out_caps = Length(name='out_caps')(digitcaps)\n",
        "\n",
        "    # Decoder network.\n",
        "    y = layers.Input(shape=(n_class,))\n",
        "    masked = Mask()([digitcaps, y])  # The true label is used to mask the output of capsule layer.\n",
        "    bn2 = layers.BatchNormalization()(masked)\n",
        "    x_recon = layers.Dense(64, activation='relu')(bn2)\n",
        "    x_recon = layers.Dense(128, activation='relu')(x_recon)\n",
        "    #x_recon = layers.Dropout(0.5)(x_recon)\n",
        "    bn3 = layers.BatchNormalization()(x_recon)\n",
        "    x_recon = layers.Dense(20, activation='sigmoid')(bn3)\n",
        "    #x_recon = layers.Flatten()(x_recon)\n",
        "    x_recon = layers.Reshape(target_shape=[20, 1], name='out_recon')(x_recon)\n",
        "    \n",
        "\n",
        "    # two-input-two-output keras Model\n",
        "    return models.Model([x, y], [out_caps, x_recon])\n",
        "\n",
        "\n",
        "## Defining the Loss Function\n",
        "\n",
        "def margin_loss(y_true, y_pred):\n",
        "    \"\"\"\n",
        "    Margin loss for Eq.(4). When y_true[i, :] contains not just one `1`, this loss should work too. Not test it.\n",
        "    :param y_true: [None, n_classes]\n",
        "    :param y_pred: [None, num_capsule]\n",
        "    :return: a scalar loss value.\n",
        "    \"\"\"\n",
        "    L = y_true * K.square(K.maximum(0., 0.9 - y_pred)) + \\\n",
        "        0.5 * (1 - y_true) * K.square(K.maximum(0., y_pred - 0.1))\n",
        "\n",
        "    return K.mean(K.sum(L, 1))\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z_b3pVM3EWz8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def train(model, data, epoch_size_frac=1.0):\n",
        "    \"\"\"\n",
        "    Training a CapsuleNet\n",
        "    :param model: the CapsuleNet model\n",
        "    :param data: a tuple containing training and testing data, like `((x_train, y_train), (x_test, y_test))`\n",
        "    :param args: arguments\n",
        "    :return: The trained model\n",
        "    \"\"\"\n",
        "    # unpacking the data\n",
        "    #(x_train, y_train), (x_test, y_test) = data\n",
        "    (x_train, y_train) = data\n",
        "    \n",
        "    global y_pred\n",
        "\n",
        "    # callbacks\n",
        "    log = callbacks.CSVLogger('log.csv')\n",
        "    checkpoint = callbacks.ModelCheckpoint('weights-{epoch:02d}.h5',\n",
        "                                           save_best_only=True, save_weights_only=True, verbose=10)\n",
        "    lr_decay = callbacks.LearningRateScheduler(schedule=lambda epoch: 0.001 * np.exp(-epoch / 10.))\n",
        "    \n",
        "    logdir = os.path.join(\"logs\", datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\"))\n",
        "    tb = tf.keras.callbacks.TensorBoard(logdir, histogram_freq=1)\n",
        "    \n",
        "    cb = callbacks.EarlyStopping(monitor = 'loss', mode = 'min', patience = 15, restore_best_weights=True)\n",
        "\n",
        "    # compile the model\n",
        "    model.compile(optimizer='adam',\n",
        "                  loss=[margin_loss, 'mse'],\n",
        "                  loss_weights=[1., 0.0005],\n",
        "                  metrics={'out_caps': 'accuracy'})\n",
        "    \n",
        "    \n",
        "    ''''model.fit([x_train, y_train], [y_train, x_train], batch_size=32, epochs=2,\n",
        "              validation_data=[[x_test, y_test], [y_test, x_test]])'''\n",
        "    \n",
        "    model.fit([x_train, y_train], [y_train, x_train], batch_size=32, epochs=150,validation_split = 0.2, callbacks = [cb])\n",
        "              \n",
        "    \n",
        "    \n",
        "    model.save_weights('trained_model.h5')\n",
        "    print('Trained model saved to \\'trained_model.h5\\'')\n",
        "    \n",
        "    return model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TUYQOym-EYcn",
        "colab_type": "code",
        "outputId": "af9edafd-d486-4a83-945e-8901a6e53bf1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "  model = CapsNet(input_shape=[20, 1],\n",
        "                n_class=2,\n",
        "                num_routing=3)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[20, 1] (?, 20, 1)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nAbIV2z9Eauo",
        "colab_type": "code",
        "outputId": "035a320d-c493-475c-8ff4-f2811d0d60ea",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 663
        }
      },
      "source": [
        "model.summary()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_15 (InputLayer)           (None, 20, 1)        0                                            \n",
            "__________________________________________________________________________________________________\n",
            "conv1 (Conv1D)                  (None, 17, 32)       160         input_15[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "conv1d_8 (Conv1D)               (None, 5, 256)       73984       conv1[0][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "reshape_8 (Reshape)             (None, 160, 8)       0           conv1d_8[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "lambda_8 (Lambda)               (None, 160, 8)       0           reshape_8[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "digitcaps (CapsuleLayer)        (None, 2, 16)        41280       lambda_8[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "input_16 (InputLayer)           (None, 2)            0                                            \n",
            "__________________________________________________________________________________________________\n",
            "mask_8 (Mask)                   (None, 16)           0           digitcaps[0][0]                  \n",
            "                                                                 input_16[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "dense_22 (Dense)                (None, 128)          2176        mask_8[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "dropout_15 (Dropout)            (None, 128)          0           dense_22[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "dense_23 (Dense)                (None, 128)          16512       dropout_15[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "dropout_16 (Dropout)            (None, 128)          0           dense_23[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "dense_24 (Dense)                (None, 20)           2580        dropout_16[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "out_caps (Length)               (None, 2)            0           digitcaps[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "out_recon (Reshape)             (None, 20, 1)        0           dense_24[0][0]                   \n",
            "==================================================================================================\n",
            "Total params: 136,692\n",
            "Trainable params: 136,372\n",
            "Non-trainable params: 320\n",
            "__________________________________________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ErgjcKMIFPVW",
        "colab_type": "text"
      },
      "source": [
        "## Working with Data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7dDsFrapEcs9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "x_train = df.iloc[:,0:20].values\n",
        "y_train = df.iloc[:,20].values"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YEPlJrTtFaRg",
        "colab_type": "code",
        "outputId": "137495e8-b1c2-46be-c462-0dbabf239a93",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "import numpy as np\n",
        "unique, counts = np.unique(y_train, return_counts=True)\n",
        "dict(zip(unique, counts))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{0: 8060, 1: 8060}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 76
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E3kBJaPnHk8C",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "x_test = df2.iloc[:, 0:20].values\n",
        "y_test = df2.iloc[:, 20].values"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DW4UwpPGIAAh",
        "colab_type": "code",
        "outputId": "a0178d73-132d-4a65-c3c6-eb0c57495f6f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "x_train.shape, x_test.shape, y_train.shape, y_test.shape"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((16120, 20), (4030, 20), (16120,), (4030,))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 78
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1UIdJQKlH7ch",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Reshaping the feature samples\n",
        "x_train_reshape = x_train.reshape(16120, 20, 1)\n",
        "y_train_reshape = y_train.reshape(16120, 1)\n",
        "#x_test_reshape = x_test.reshape(4030, 20, 1)\n",
        "#y_test_reshape = y_test.reshape(4030, 1)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SNY6b0AvIKEn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Reshaping the labels\n",
        "y_train_ = tf.keras.utils.to_categorical(y_train_reshape,num_classes=2)\n",
        "#y_test_ = tf.keras.utils.to_categorical(y_test_reshape,num_classes=2)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yY0PhcyUINOP",
        "colab_type": "code",
        "outputId": "206c3f36-147a-4561-e177-047b0bca0346",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 3930
        }
      },
      "source": [
        "train(model=model, data=((x_train_reshape, y_train_)), epoch_size_frac = 0.5)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 12896 samples, validate on 3224 samples\n",
            "Epoch 1/150\n",
            "12896/12896 [==============================] - 13s 1ms/step - loss: 0.1570 - out_caps_loss: 0.1427 - out_recon_loss: 28.6689 - out_caps_acc: 0.7970 - val_loss: 0.1115 - val_out_caps_loss: 0.0979 - val_out_recon_loss: 27.2057 - val_out_caps_acc: 0.8524\n",
            "Epoch 2/150\n",
            "12896/12896 [==============================] - 12s 907us/step - loss: 0.1108 - out_caps_loss: 0.0971 - out_recon_loss: 27.3914 - out_caps_acc: 0.8556 - val_loss: 0.1062 - val_out_caps_loss: 0.0926 - val_out_recon_loss: 27.1940 - val_out_caps_acc: 0.8623\n",
            "Epoch 3/150\n",
            "12896/12896 [==============================] - 11s 829us/step - loss: 0.1027 - out_caps_loss: 0.0890 - out_recon_loss: 27.3787 - out_caps_acc: 0.8715 - val_loss: 0.1004 - val_out_caps_loss: 0.0868 - val_out_recon_loss: 27.1890 - val_out_caps_acc: 0.8741\n",
            "Epoch 4/150\n",
            "12896/12896 [==============================] - 11s 830us/step - loss: 0.0972 - out_caps_loss: 0.0835 - out_recon_loss: 27.3756 - out_caps_acc: 0.8790 - val_loss: 0.1026 - val_out_caps_loss: 0.0890 - val_out_recon_loss: 27.1875 - val_out_caps_acc: 0.8710\n",
            "Epoch 5/150\n",
            "12896/12896 [==============================] - 11s 833us/step - loss: 0.0930 - out_caps_loss: 0.0793 - out_recon_loss: 27.3745 - out_caps_acc: 0.8850 - val_loss: 0.0933 - val_out_caps_loss: 0.0797 - val_out_recon_loss: 27.1866 - val_out_caps_acc: 0.8828\n",
            "Epoch 6/150\n",
            "12896/12896 [==============================] - 11s 831us/step - loss: 0.0899 - out_caps_loss: 0.0762 - out_recon_loss: 27.3740 - out_caps_acc: 0.8884 - val_loss: 0.0916 - val_out_caps_loss: 0.0781 - val_out_recon_loss: 27.1861 - val_out_caps_acc: 0.8815\n",
            "Epoch 7/150\n",
            "12896/12896 [==============================] - 11s 825us/step - loss: 0.0873 - out_caps_loss: 0.0737 - out_recon_loss: 27.3736 - out_caps_acc: 0.8926 - val_loss: 0.0946 - val_out_caps_loss: 0.0810 - val_out_recon_loss: 27.1857 - val_out_caps_acc: 0.8818\n",
            "Epoch 8/150\n",
            "12896/12896 [==============================] - 11s 830us/step - loss: 0.0840 - out_caps_loss: 0.0703 - out_recon_loss: 27.3730 - out_caps_acc: 0.8986 - val_loss: 0.1029 - val_out_caps_loss: 0.0893 - val_out_recon_loss: 27.1855 - val_out_caps_acc: 0.8694\n",
            "Epoch 9/150\n",
            "12896/12896 [==============================] - 12s 918us/step - loss: 0.0821 - out_caps_loss: 0.0684 - out_recon_loss: 27.3722 - out_caps_acc: 0.9029 - val_loss: 0.0984 - val_out_caps_loss: 0.0849 - val_out_recon_loss: 27.1846 - val_out_caps_acc: 0.8700\n",
            "Epoch 10/150\n",
            "12896/12896 [==============================] - 11s 851us/step - loss: 0.0801 - out_caps_loss: 0.0664 - out_recon_loss: 27.3719 - out_caps_acc: 0.9029 - val_loss: 0.0875 - val_out_caps_loss: 0.0739 - val_out_recon_loss: 27.1843 - val_out_caps_acc: 0.8914\n",
            "Epoch 11/150\n",
            "12896/12896 [==============================] - 11s 834us/step - loss: 0.0789 - out_caps_loss: 0.0653 - out_recon_loss: 27.3718 - out_caps_acc: 0.9053 - val_loss: 0.0854 - val_out_caps_loss: 0.0718 - val_out_recon_loss: 27.1845 - val_out_caps_acc: 0.8939\n",
            "Epoch 12/150\n",
            "12896/12896 [==============================] - 11s 820us/step - loss: 0.0762 - out_caps_loss: 0.0625 - out_recon_loss: 27.3715 - out_caps_acc: 0.9080 - val_loss: 0.0822 - val_out_caps_loss: 0.0686 - val_out_recon_loss: 27.1837 - val_out_caps_acc: 0.8973\n",
            "Epoch 13/150\n",
            "12896/12896 [==============================] - 11s 824us/step - loss: 0.0742 - out_caps_loss: 0.0605 - out_recon_loss: 27.3714 - out_caps_acc: 0.9138 - val_loss: 0.0846 - val_out_caps_loss: 0.0710 - val_out_recon_loss: 27.1839 - val_out_caps_acc: 0.8939\n",
            "Epoch 14/150\n",
            "12896/12896 [==============================] - 11s 836us/step - loss: 0.0728 - out_caps_loss: 0.0591 - out_recon_loss: 27.3708 - out_caps_acc: 0.9142 - val_loss: 0.0822 - val_out_caps_loss: 0.0686 - val_out_recon_loss: 27.1840 - val_out_caps_acc: 0.8998\n",
            "Epoch 15/150\n",
            "12896/12896 [==============================] - 12s 898us/step - loss: 0.0721 - out_caps_loss: 0.0584 - out_recon_loss: 27.3712 - out_caps_acc: 0.9165 - val_loss: 0.0829 - val_out_caps_loss: 0.0693 - val_out_recon_loss: 27.1836 - val_out_caps_acc: 0.8989\n",
            "Epoch 16/150\n",
            "12896/12896 [==============================] - 12s 931us/step - loss: 0.0706 - out_caps_loss: 0.0569 - out_recon_loss: 27.3707 - out_caps_acc: 0.9193 - val_loss: 0.0876 - val_out_caps_loss: 0.0741 - val_out_recon_loss: 27.1836 - val_out_caps_acc: 0.8880\n",
            "Epoch 17/150\n",
            "12896/12896 [==============================] - 11s 863us/step - loss: 0.0686 - out_caps_loss: 0.0549 - out_recon_loss: 27.3708 - out_caps_acc: 0.9211 - val_loss: 0.0774 - val_out_caps_loss: 0.0638 - val_out_recon_loss: 27.1837 - val_out_caps_acc: 0.9057\n",
            "Epoch 18/150\n",
            "12896/12896 [==============================] - 11s 820us/step - loss: 0.0676 - out_caps_loss: 0.0540 - out_recon_loss: 27.3706 - out_caps_acc: 0.9232 - val_loss: 0.0771 - val_out_caps_loss: 0.0635 - val_out_recon_loss: 27.1846 - val_out_caps_acc: 0.9097\n",
            "Epoch 19/150\n",
            "12896/12896 [==============================] - 11s 821us/step - loss: 0.0659 - out_caps_loss: 0.0522 - out_recon_loss: 27.3706 - out_caps_acc: 0.9260 - val_loss: 0.0855 - val_out_caps_loss: 0.0719 - val_out_recon_loss: 27.1846 - val_out_caps_acc: 0.8924\n",
            "Epoch 20/150\n",
            "12896/12896 [==============================] - 11s 822us/step - loss: 0.0643 - out_caps_loss: 0.0506 - out_recon_loss: 27.3703 - out_caps_acc: 0.9265 - val_loss: 0.0794 - val_out_caps_loss: 0.0658 - val_out_recon_loss: 27.1834 - val_out_caps_acc: 0.8998\n",
            "Epoch 21/150\n",
            "12896/12896 [==============================] - 11s 817us/step - loss: 0.0641 - out_caps_loss: 0.0504 - out_recon_loss: 27.3706 - out_caps_acc: 0.9271 - val_loss: 0.0815 - val_out_caps_loss: 0.0679 - val_out_recon_loss: 27.1831 - val_out_caps_acc: 0.8973\n",
            "Epoch 22/150\n",
            "12896/12896 [==============================] - 10s 814us/step - loss: 0.0632 - out_caps_loss: 0.0495 - out_recon_loss: 27.3710 - out_caps_acc: 0.9280 - val_loss: 0.0778 - val_out_caps_loss: 0.0642 - val_out_recon_loss: 27.1832 - val_out_caps_acc: 0.9082\n",
            "Epoch 23/150\n",
            "12896/12896 [==============================] - 11s 817us/step - loss: 0.0612 - out_caps_loss: 0.0475 - out_recon_loss: 27.3698 - out_caps_acc: 0.9318 - val_loss: 0.0776 - val_out_caps_loss: 0.0640 - val_out_recon_loss: 27.1842 - val_out_caps_acc: 0.9076\n",
            "Epoch 24/150\n",
            "12896/12896 [==============================] - 12s 923us/step - loss: 0.0606 - out_caps_loss: 0.0469 - out_recon_loss: 27.3700 - out_caps_acc: 0.9315 - val_loss: 0.0756 - val_out_caps_loss: 0.0620 - val_out_recon_loss: 27.1833 - val_out_caps_acc: 0.9107\n",
            "Epoch 25/150\n",
            "12896/12896 [==============================] - 11s 820us/step - loss: 0.0596 - out_caps_loss: 0.0459 - out_recon_loss: 27.3701 - out_caps_acc: 0.9349 - val_loss: 0.0801 - val_out_caps_loss: 0.0665 - val_out_recon_loss: 27.1828 - val_out_caps_acc: 0.9001\n",
            "Epoch 26/150\n",
            "12896/12896 [==============================] - 11s 849us/step - loss: 0.0592 - out_caps_loss: 0.0455 - out_recon_loss: 27.3696 - out_caps_acc: 0.9353 - val_loss: 0.0719 - val_out_caps_loss: 0.0583 - val_out_recon_loss: 27.1838 - val_out_caps_acc: 0.9147\n",
            "Epoch 27/150\n",
            "12896/12896 [==============================] - 11s 845us/step - loss: 0.0585 - out_caps_loss: 0.0448 - out_recon_loss: 27.3696 - out_caps_acc: 0.9354 - val_loss: 0.0741 - val_out_caps_loss: 0.0606 - val_out_recon_loss: 27.1837 - val_out_caps_acc: 0.9104\n",
            "Epoch 28/150\n",
            "12896/12896 [==============================] - 10s 814us/step - loss: 0.0579 - out_caps_loss: 0.0442 - out_recon_loss: 27.3696 - out_caps_acc: 0.9377 - val_loss: 0.0721 - val_out_caps_loss: 0.0585 - val_out_recon_loss: 27.1840 - val_out_caps_acc: 0.9190\n",
            "Epoch 29/150\n",
            "12896/12896 [==============================] - 10s 809us/step - loss: 0.0571 - out_caps_loss: 0.0434 - out_recon_loss: 27.3696 - out_caps_acc: 0.9403 - val_loss: 0.0771 - val_out_caps_loss: 0.0635 - val_out_recon_loss: 27.1854 - val_out_caps_acc: 0.9069\n",
            "Epoch 30/150\n",
            "12896/12896 [==============================] - 11s 815us/step - loss: 0.0559 - out_caps_loss: 0.0422 - out_recon_loss: 27.3697 - out_caps_acc: 0.9415 - val_loss: 0.0756 - val_out_caps_loss: 0.0620 - val_out_recon_loss: 27.1858 - val_out_caps_acc: 0.9094\n",
            "Epoch 31/150\n",
            "12896/12896 [==============================] - 11s 883us/step - loss: 0.0547 - out_caps_loss: 0.0410 - out_recon_loss: 27.3698 - out_caps_acc: 0.9414 - val_loss: 0.0721 - val_out_caps_loss: 0.0585 - val_out_recon_loss: 27.1856 - val_out_caps_acc: 0.9144\n",
            "Epoch 32/150\n",
            "12896/12896 [==============================] - 11s 850us/step - loss: 0.0538 - out_caps_loss: 0.0401 - out_recon_loss: 27.3695 - out_caps_acc: 0.9431 - val_loss: 0.0686 - val_out_caps_loss: 0.0550 - val_out_recon_loss: 27.1832 - val_out_caps_acc: 0.9215\n",
            "Epoch 33/150\n",
            "12896/12896 [==============================] - 10s 812us/step - loss: 0.0531 - out_caps_loss: 0.0394 - out_recon_loss: 27.3702 - out_caps_acc: 0.9445 - val_loss: 0.0709 - val_out_caps_loss: 0.0573 - val_out_recon_loss: 27.1848 - val_out_caps_acc: 0.9206\n",
            "Epoch 34/150\n",
            "12896/12896 [==============================] - 10s 809us/step - loss: 0.0542 - out_caps_loss: 0.0405 - out_recon_loss: 27.3696 - out_caps_acc: 0.9430 - val_loss: 0.0788 - val_out_caps_loss: 0.0652 - val_out_recon_loss: 27.1832 - val_out_caps_acc: 0.9029\n",
            "Epoch 35/150\n",
            "12896/12896 [==============================] - 10s 811us/step - loss: 0.0526 - out_caps_loss: 0.0390 - out_recon_loss: 27.3698 - out_caps_acc: 0.9450 - val_loss: 0.0711 - val_out_caps_loss: 0.0575 - val_out_recon_loss: 27.1830 - val_out_caps_acc: 0.9159\n",
            "Epoch 36/150\n",
            "12896/12896 [==============================] - 10s 810us/step - loss: 0.0514 - out_caps_loss: 0.0377 - out_recon_loss: 27.3693 - out_caps_acc: 0.9468 - val_loss: 0.0701 - val_out_caps_loss: 0.0565 - val_out_recon_loss: 27.1859 - val_out_caps_acc: 0.9225\n",
            "Epoch 37/150\n",
            "12896/12896 [==============================] - 10s 810us/step - loss: 0.0510 - out_caps_loss: 0.0373 - out_recon_loss: 27.3689 - out_caps_acc: 0.9471 - val_loss: 0.0691 - val_out_caps_loss: 0.0555 - val_out_recon_loss: 27.1824 - val_out_caps_acc: 0.9246\n",
            "Epoch 38/150\n",
            "12896/12896 [==============================] - 10s 811us/step - loss: 0.0504 - out_caps_loss: 0.0367 - out_recon_loss: 27.3690 - out_caps_acc: 0.9491 - val_loss: 0.0735 - val_out_caps_loss: 0.0599 - val_out_recon_loss: 27.1834 - val_out_caps_acc: 0.9166\n",
            "Epoch 39/150\n",
            "12896/12896 [==============================] - 12s 919us/step - loss: 0.0509 - out_caps_loss: 0.0372 - out_recon_loss: 27.3689 - out_caps_acc: 0.9486 - val_loss: 0.0701 - val_out_caps_loss: 0.0565 - val_out_recon_loss: 27.1845 - val_out_caps_acc: 0.9181\n",
            "Epoch 40/150\n",
            "12896/12896 [==============================] - 10s 811us/step - loss: 0.0487 - out_caps_loss: 0.0350 - out_recon_loss: 27.3694 - out_caps_acc: 0.9515 - val_loss: 0.0662 - val_out_caps_loss: 0.0527 - val_out_recon_loss: 27.1835 - val_out_caps_acc: 0.9256\n",
            "Epoch 41/150\n",
            "12896/12896 [==============================] - 10s 812us/step - loss: 0.0492 - out_caps_loss: 0.0355 - out_recon_loss: 27.3690 - out_caps_acc: 0.9510 - val_loss: 0.0747 - val_out_caps_loss: 0.0611 - val_out_recon_loss: 27.1843 - val_out_caps_acc: 0.9116\n",
            "Epoch 42/150\n",
            "12896/12896 [==============================] - 10s 812us/step - loss: 0.0487 - out_caps_loss: 0.0350 - out_recon_loss: 27.3691 - out_caps_acc: 0.9517 - val_loss: 0.0676 - val_out_caps_loss: 0.0540 - val_out_recon_loss: 27.1828 - val_out_caps_acc: 0.9240\n",
            "Epoch 43/150\n",
            "12896/12896 [==============================] - 10s 809us/step - loss: 0.0476 - out_caps_loss: 0.0339 - out_recon_loss: 27.3689 - out_caps_acc: 0.9539 - val_loss: 0.0737 - val_out_caps_loss: 0.0601 - val_out_recon_loss: 27.1835 - val_out_caps_acc: 0.9153\n",
            "Epoch 44/150\n",
            "12896/12896 [==============================] - 12s 911us/step - loss: 0.0459 - out_caps_loss: 0.0322 - out_recon_loss: 27.3688 - out_caps_acc: 0.9561 - val_loss: 0.0731 - val_out_caps_loss: 0.0595 - val_out_recon_loss: 27.1862 - val_out_caps_acc: 0.9194\n",
            "Epoch 45/150\n",
            "12896/12896 [==============================] - 11s 832us/step - loss: 0.0474 - out_caps_loss: 0.0337 - out_recon_loss: 27.3689 - out_caps_acc: 0.9535 - val_loss: 0.0649 - val_out_caps_loss: 0.0513 - val_out_recon_loss: 27.1827 - val_out_caps_acc: 0.9283\n",
            "Epoch 46/150\n",
            "12896/12896 [==============================] - 11s 885us/step - loss: 0.0479 - out_caps_loss: 0.0342 - out_recon_loss: 27.3690 - out_caps_acc: 0.9529 - val_loss: 0.0677 - val_out_caps_loss: 0.0542 - val_out_recon_loss: 27.1821 - val_out_caps_acc: 0.9206\n",
            "Epoch 47/150\n",
            "12896/12896 [==============================] - 11s 848us/step - loss: 0.0454 - out_caps_loss: 0.0317 - out_recon_loss: 27.3690 - out_caps_acc: 0.9563 - val_loss: 0.0678 - val_out_caps_loss: 0.0542 - val_out_recon_loss: 27.1825 - val_out_caps_acc: 0.9228\n",
            "Epoch 48/150\n",
            "12896/12896 [==============================] - 10s 812us/step - loss: 0.0468 - out_caps_loss: 0.0331 - out_recon_loss: 27.3689 - out_caps_acc: 0.9556 - val_loss: 0.0698 - val_out_caps_loss: 0.0562 - val_out_recon_loss: 27.1833 - val_out_caps_acc: 0.9234\n",
            "Epoch 49/150\n",
            "12896/12896 [==============================] - 10s 811us/step - loss: 0.0455 - out_caps_loss: 0.0319 - out_recon_loss: 27.3687 - out_caps_acc: 0.9569 - val_loss: 0.0820 - val_out_caps_loss: 0.0684 - val_out_recon_loss: 27.1822 - val_out_caps_acc: 0.9035\n",
            "Epoch 50/150\n",
            "12896/12896 [==============================] - 10s 814us/step - loss: 0.0463 - out_caps_loss: 0.0326 - out_recon_loss: 27.3689 - out_caps_acc: 0.9560 - val_loss: 0.0686 - val_out_caps_loss: 0.0550 - val_out_recon_loss: 27.1828 - val_out_caps_acc: 0.9218\n",
            "Epoch 51/150\n",
            "12896/12896 [==============================] - 11s 819us/step - loss: 0.0439 - out_caps_loss: 0.0302 - out_recon_loss: 27.3690 - out_caps_acc: 0.9595 - val_loss: 0.0632 - val_out_caps_loss: 0.0496 - val_out_recon_loss: 27.1824 - val_out_caps_acc: 0.9333\n",
            "Epoch 52/150\n",
            "12896/12896 [==============================] - 10s 813us/step - loss: 0.0431 - out_caps_loss: 0.0294 - out_recon_loss: 27.3688 - out_caps_acc: 0.9598 - val_loss: 0.0652 - val_out_caps_loss: 0.0516 - val_out_recon_loss: 27.1830 - val_out_caps_acc: 0.9268\n",
            "Epoch 53/150\n",
            "12896/12896 [==============================] - 11s 820us/step - loss: 0.0439 - out_caps_loss: 0.0302 - out_recon_loss: 27.3686 - out_caps_acc: 0.9593 - val_loss: 0.0681 - val_out_caps_loss: 0.0545 - val_out_recon_loss: 27.1833 - val_out_caps_acc: 0.9231\n",
            "Epoch 54/150\n",
            "12896/12896 [==============================] - 12s 910us/step - loss: 0.0437 - out_caps_loss: 0.0300 - out_recon_loss: 27.3687 - out_caps_acc: 0.9598 - val_loss: 0.0643 - val_out_caps_loss: 0.0507 - val_out_recon_loss: 27.1840 - val_out_caps_acc: 0.9271\n",
            "Epoch 55/150\n",
            "12896/12896 [==============================] - 11s 879us/step - loss: 0.0439 - out_caps_loss: 0.0302 - out_recon_loss: 27.3689 - out_caps_acc: 0.9598 - val_loss: 0.0643 - val_out_caps_loss: 0.0507 - val_out_recon_loss: 27.1824 - val_out_caps_acc: 0.9330\n",
            "Epoch 56/150\n",
            "12896/12896 [==============================] - 11s 816us/step - loss: 0.0431 - out_caps_loss: 0.0294 - out_recon_loss: 27.3688 - out_caps_acc: 0.9600 - val_loss: 0.0672 - val_out_caps_loss: 0.0536 - val_out_recon_loss: 27.1822 - val_out_caps_acc: 0.9265\n",
            "Epoch 57/150\n",
            "12896/12896 [==============================] - 11s 835us/step - loss: 0.0433 - out_caps_loss: 0.0296 - out_recon_loss: 27.3682 - out_caps_acc: 0.9605 - val_loss: 0.0679 - val_out_caps_loss: 0.0543 - val_out_recon_loss: 27.1818 - val_out_caps_acc: 0.9231\n",
            "Epoch 58/150\n",
            "12896/12896 [==============================] - 11s 824us/step - loss: 0.0416 - out_caps_loss: 0.0279 - out_recon_loss: 27.3687 - out_caps_acc: 0.9625 - val_loss: 0.0688 - val_out_caps_loss: 0.0552 - val_out_recon_loss: 27.1825 - val_out_caps_acc: 0.9203\n",
            "Epoch 59/150\n",
            "12896/12896 [==============================] - 11s 818us/step - loss: 0.0415 - out_caps_loss: 0.0279 - out_recon_loss: 27.3683 - out_caps_acc: 0.9632 - val_loss: 0.0876 - val_out_caps_loss: 0.0740 - val_out_recon_loss: 27.1827 - val_out_caps_acc: 0.8998\n",
            "Epoch 60/150\n",
            "12896/12896 [==============================] - 11s 815us/step - loss: 0.0423 - out_caps_loss: 0.0286 - out_recon_loss: 27.3682 - out_caps_acc: 0.9620 - val_loss: 0.0678 - val_out_caps_loss: 0.0542 - val_out_recon_loss: 27.1834 - val_out_caps_acc: 0.9259\n",
            "Epoch 61/150\n",
            "12896/12896 [==============================] - 12s 902us/step - loss: 0.0417 - out_caps_loss: 0.0280 - out_recon_loss: 27.3682 - out_caps_acc: 0.9632 - val_loss: 0.0641 - val_out_caps_loss: 0.0505 - val_out_recon_loss: 27.1817 - val_out_caps_acc: 0.9280\n",
            "Epoch 62/150\n",
            "12896/12896 [==============================] - 11s 835us/step - loss: 0.0411 - out_caps_loss: 0.0274 - out_recon_loss: 27.3681 - out_caps_acc: 0.9646 - val_loss: 0.0669 - val_out_caps_loss: 0.0533 - val_out_recon_loss: 27.1829 - val_out_caps_acc: 0.9265\n",
            "Epoch 63/150\n",
            "12896/12896 [==============================] - 11s 819us/step - loss: 0.0393 - out_caps_loss: 0.0256 - out_recon_loss: 27.3683 - out_caps_acc: 0.9663 - val_loss: 0.0687 - val_out_caps_loss: 0.0551 - val_out_recon_loss: 27.1825 - val_out_caps_acc: 0.9243\n",
            "Epoch 64/150\n",
            "12896/12896 [==============================] - 11s 820us/step - loss: 0.0395 - out_caps_loss: 0.0258 - out_recon_loss: 27.3684 - out_caps_acc: 0.9652 - val_loss: 0.0629 - val_out_caps_loss: 0.0493 - val_out_recon_loss: 27.1819 - val_out_caps_acc: 0.9358\n",
            "Epoch 65/150\n",
            "12896/12896 [==============================] - 11s 820us/step - loss: 0.0409 - out_caps_loss: 0.0272 - out_recon_loss: 27.3683 - out_caps_acc: 0.9640 - val_loss: 0.0617 - val_out_caps_loss: 0.0482 - val_out_recon_loss: 27.1826 - val_out_caps_acc: 0.9302\n",
            "Epoch 66/150\n",
            "12896/12896 [==============================] - 11s 817us/step - loss: 0.0390 - out_caps_loss: 0.0253 - out_recon_loss: 27.3683 - out_caps_acc: 0.9664 - val_loss: 0.0659 - val_out_caps_loss: 0.0523 - val_out_recon_loss: 27.1826 - val_out_caps_acc: 0.9305\n",
            "Epoch 67/150\n",
            "12896/12896 [==============================] - 11s 815us/step - loss: 0.0392 - out_caps_loss: 0.0255 - out_recon_loss: 27.3683 - out_caps_acc: 0.9654 - val_loss: 0.0669 - val_out_caps_loss: 0.0533 - val_out_recon_loss: 27.1817 - val_out_caps_acc: 0.9271\n",
            "Epoch 68/150\n",
            "12896/12896 [==============================] - 11s 837us/step - loss: 0.0392 - out_caps_loss: 0.0255 - out_recon_loss: 27.3680 - out_caps_acc: 0.9667 - val_loss: 0.0621 - val_out_caps_loss: 0.0485 - val_out_recon_loss: 27.1829 - val_out_caps_acc: 0.9346\n",
            "Epoch 69/150\n",
            "12896/12896 [==============================] - 12s 904us/step - loss: 0.0384 - out_caps_loss: 0.0247 - out_recon_loss: 27.3681 - out_caps_acc: 0.9676 - val_loss: 0.0623 - val_out_caps_loss: 0.0487 - val_out_recon_loss: 27.1829 - val_out_caps_acc: 0.9342\n",
            "Epoch 70/150\n",
            "12896/12896 [==============================] - 11s 816us/step - loss: 0.0396 - out_caps_loss: 0.0259 - out_recon_loss: 27.3682 - out_caps_acc: 0.9645 - val_loss: 0.0747 - val_out_caps_loss: 0.0611 - val_out_recon_loss: 27.1850 - val_out_caps_acc: 0.9172\n",
            "Epoch 71/150\n",
            "12896/12896 [==============================] - 11s 815us/step - loss: 0.0374 - out_caps_loss: 0.0237 - out_recon_loss: 27.3681 - out_caps_acc: 0.9686 - val_loss: 0.0691 - val_out_caps_loss: 0.0555 - val_out_recon_loss: 27.1819 - val_out_caps_acc: 0.9249\n",
            "Epoch 72/150\n",
            "12896/12896 [==============================] - 11s 816us/step - loss: 0.0392 - out_caps_loss: 0.0255 - out_recon_loss: 27.3682 - out_caps_acc: 0.9663 - val_loss: 0.0756 - val_out_caps_loss: 0.0620 - val_out_recon_loss: 27.1818 - val_out_caps_acc: 0.9150\n",
            "Epoch 73/150\n",
            "12896/12896 [==============================] - 12s 943us/step - loss: 0.0378 - out_caps_loss: 0.0241 - out_recon_loss: 27.3678 - out_caps_acc: 0.9688 - val_loss: 0.0668 - val_out_caps_loss: 0.0532 - val_out_recon_loss: 27.1819 - val_out_caps_acc: 0.9290\n",
            "Epoch 74/150\n",
            "12896/12896 [==============================] - 11s 816us/step - loss: 0.0393 - out_caps_loss: 0.0256 - out_recon_loss: 27.3679 - out_caps_acc: 0.9646 - val_loss: 0.0676 - val_out_caps_loss: 0.0540 - val_out_recon_loss: 27.1823 - val_out_caps_acc: 0.9277\n",
            "Epoch 75/150\n",
            "12896/12896 [==============================] - 11s 816us/step - loss: 0.0373 - out_caps_loss: 0.0237 - out_recon_loss: 27.3678 - out_caps_acc: 0.9682 - val_loss: 0.0781 - val_out_caps_loss: 0.0645 - val_out_recon_loss: 27.1817 - val_out_caps_acc: 0.9088\n",
            "Epoch 76/150\n",
            "12896/12896 [==============================] - 12s 919us/step - loss: 0.0375 - out_caps_loss: 0.0238 - out_recon_loss: 27.3682 - out_caps_acc: 0.9678 - val_loss: 0.0674 - val_out_caps_loss: 0.0538 - val_out_recon_loss: 27.1826 - val_out_caps_acc: 0.9240\n",
            "Epoch 77/150\n",
            "12896/12896 [==============================] - 11s 821us/step - loss: 0.0369 - out_caps_loss: 0.0232 - out_recon_loss: 27.3679 - out_caps_acc: 0.9705 - val_loss: 0.0692 - val_out_caps_loss: 0.0556 - val_out_recon_loss: 27.1817 - val_out_caps_acc: 0.9259\n",
            "Epoch 78/150\n",
            "12896/12896 [==============================] - 11s 816us/step - loss: 0.0378 - out_caps_loss: 0.0241 - out_recon_loss: 27.3678 - out_caps_acc: 0.9678 - val_loss: 0.0681 - val_out_caps_loss: 0.0545 - val_out_recon_loss: 27.1817 - val_out_caps_acc: 0.9218\n",
            "Epoch 79/150\n",
            "12896/12896 [==============================] - 10s 812us/step - loss: 0.0358 - out_caps_loss: 0.0221 - out_recon_loss: 27.3679 - out_caps_acc: 0.9717 - val_loss: 0.0646 - val_out_caps_loss: 0.0510 - val_out_recon_loss: 27.1830 - val_out_caps_acc: 0.9336\n",
            "Epoch 80/150\n",
            "12896/12896 [==============================] - 11s 817us/step - loss: 0.0353 - out_caps_loss: 0.0216 - out_recon_loss: 27.3677 - out_caps_acc: 0.9716 - val_loss: 0.0604 - val_out_caps_loss: 0.0468 - val_out_recon_loss: 27.1818 - val_out_caps_acc: 0.9380\n",
            "Epoch 81/150\n",
            "12896/12896 [==============================] - 11s 822us/step - loss: 0.0360 - out_caps_loss: 0.0223 - out_recon_loss: 27.3676 - out_caps_acc: 0.9705 - val_loss: 0.0638 - val_out_caps_loss: 0.0502 - val_out_recon_loss: 27.1830 - val_out_caps_acc: 0.9321\n",
            "Epoch 82/150\n",
            "12896/12896 [==============================] - 11s 821us/step - loss: 0.0371 - out_caps_loss: 0.0234 - out_recon_loss: 27.3677 - out_caps_acc: 0.9695 - val_loss: 0.0708 - val_out_caps_loss: 0.0572 - val_out_recon_loss: 27.1827 - val_out_caps_acc: 0.9215\n",
            "Epoch 83/150\n",
            "12896/12896 [==============================] - 11s 890us/step - loss: 0.0339 - out_caps_loss: 0.0203 - out_recon_loss: 27.3678 - out_caps_acc: 0.9743 - val_loss: 0.0634 - val_out_caps_loss: 0.0498 - val_out_recon_loss: 27.1815 - val_out_caps_acc: 0.9318\n",
            "Epoch 84/150\n",
            "12896/12896 [==============================] - 11s 882us/step - loss: 0.0363 - out_caps_loss: 0.0226 - out_recon_loss: 27.3677 - out_caps_acc: 0.9713 - val_loss: 0.0667 - val_out_caps_loss: 0.0531 - val_out_recon_loss: 27.1813 - val_out_caps_acc: 0.9256\n",
            "Epoch 85/150\n",
            "12896/12896 [==============================] - 11s 819us/step - loss: 0.0363 - out_caps_loss: 0.0226 - out_recon_loss: 27.3680 - out_caps_acc: 0.9694 - val_loss: 0.0678 - val_out_caps_loss: 0.0542 - val_out_recon_loss: 27.1819 - val_out_caps_acc: 0.9256\n",
            "Epoch 86/150\n",
            "12896/12896 [==============================] - 11s 824us/step - loss: 0.0352 - out_caps_loss: 0.0215 - out_recon_loss: 27.3678 - out_caps_acc: 0.9720 - val_loss: 0.0694 - val_out_caps_loss: 0.0558 - val_out_recon_loss: 27.1822 - val_out_caps_acc: 0.9249\n",
            "Epoch 87/150\n",
            "12896/12896 [==============================] - 11s 830us/step - loss: 0.0343 - out_caps_loss: 0.0207 - out_recon_loss: 27.3676 - out_caps_acc: 0.9725 - val_loss: 0.0602 - val_out_caps_loss: 0.0466 - val_out_recon_loss: 27.1817 - val_out_caps_acc: 0.9377\n",
            "Epoch 88/150\n",
            "12896/12896 [==============================] - 11s 824us/step - loss: 0.0350 - out_caps_loss: 0.0214 - out_recon_loss: 27.3677 - out_caps_acc: 0.9719 - val_loss: 0.0611 - val_out_caps_loss: 0.0475 - val_out_recon_loss: 27.1815 - val_out_caps_acc: 0.9330\n",
            "Epoch 89/150\n",
            "12896/12896 [==============================] - 11s 823us/step - loss: 0.0344 - out_caps_loss: 0.0207 - out_recon_loss: 27.3672 - out_caps_acc: 0.9732 - val_loss: 0.0673 - val_out_caps_loss: 0.0537 - val_out_recon_loss: 27.1829 - val_out_caps_acc: 0.9277\n",
            "Epoch 90/150\n",
            "12896/12896 [==============================] - 11s 821us/step - loss: 0.0345 - out_caps_loss: 0.0208 - out_recon_loss: 27.3675 - out_caps_acc: 0.9733 - val_loss: 0.0615 - val_out_caps_loss: 0.0479 - val_out_recon_loss: 27.1813 - val_out_caps_acc: 0.9373\n",
            "Epoch 91/150\n",
            "12896/12896 [==============================] - 12s 921us/step - loss: 0.0346 - out_caps_loss: 0.0210 - out_recon_loss: 27.3675 - out_caps_acc: 0.9731 - val_loss: 0.0702 - val_out_caps_loss: 0.0566 - val_out_recon_loss: 27.1815 - val_out_caps_acc: 0.9206\n",
            "Epoch 92/150\n",
            "12896/12896 [==============================] - 11s 823us/step - loss: 0.0338 - out_caps_loss: 0.0202 - out_recon_loss: 27.3675 - out_caps_acc: 0.9740 - val_loss: 0.0733 - val_out_caps_loss: 0.0597 - val_out_recon_loss: 27.1836 - val_out_caps_acc: 0.9225\n",
            "Epoch 93/150\n",
            "12896/12896 [==============================] - 11s 823us/step - loss: 0.0341 - out_caps_loss: 0.0204 - out_recon_loss: 27.3675 - out_caps_acc: 0.9736 - val_loss: 0.0707 - val_out_caps_loss: 0.0571 - val_out_recon_loss: 27.1824 - val_out_caps_acc: 0.9172\n",
            "Epoch 94/150\n",
            "12896/12896 [==============================] - 11s 821us/step - loss: 0.0362 - out_caps_loss: 0.0225 - out_recon_loss: 27.3674 - out_caps_acc: 0.9708 - val_loss: 0.0606 - val_out_caps_loss: 0.0470 - val_out_recon_loss: 27.1820 - val_out_caps_acc: 0.9398\n",
            "Epoch 95/150\n",
            "12896/12896 [==============================] - 11s 817us/step - loss: 0.0337 - out_caps_loss: 0.0200 - out_recon_loss: 27.3676 - out_caps_acc: 0.9738 - val_loss: 0.0627 - val_out_caps_loss: 0.0491 - val_out_recon_loss: 27.1820 - val_out_caps_acc: 0.9361\n",
            "Epoch 96/150\n",
            "12896/12896 [==============================] - 11s 816us/step - loss: 0.0322 - out_caps_loss: 0.0186 - out_recon_loss: 27.3676 - out_caps_acc: 0.9769 - val_loss: 0.0598 - val_out_caps_loss: 0.0462 - val_out_recon_loss: 27.1813 - val_out_caps_acc: 0.9417\n",
            "Epoch 97/150\n",
            "12896/12896 [==============================] - 11s 817us/step - loss: 0.0337 - out_caps_loss: 0.0200 - out_recon_loss: 27.3675 - out_caps_acc: 0.9730 - val_loss: 0.0685 - val_out_caps_loss: 0.0549 - val_out_recon_loss: 27.1831 - val_out_caps_acc: 0.9215\n",
            "Epoch 98/150\n",
            "12896/12896 [==============================] - 11s 865us/step - loss: 0.0318 - out_caps_loss: 0.0181 - out_recon_loss: 27.3674 - out_caps_acc: 0.9765 - val_loss: 0.0589 - val_out_caps_loss: 0.0453 - val_out_recon_loss: 27.1817 - val_out_caps_acc: 0.9392\n",
            "Epoch 99/150\n",
            "12896/12896 [==============================] - 11s 867us/step - loss: 0.0341 - out_caps_loss: 0.0204 - out_recon_loss: 27.3673 - out_caps_acc: 0.9739 - val_loss: 0.0649 - val_out_caps_loss: 0.0513 - val_out_recon_loss: 27.1816 - val_out_caps_acc: 0.9299\n",
            "Epoch 100/150\n",
            "12896/12896 [==============================] - 11s 816us/step - loss: 0.0313 - out_caps_loss: 0.0176 - out_recon_loss: 27.3672 - out_caps_acc: 0.9781 - val_loss: 0.0649 - val_out_caps_loss: 0.0513 - val_out_recon_loss: 27.1818 - val_out_caps_acc: 0.9333\n",
            "Epoch 101/150\n",
            "12896/12896 [==============================] - 11s 845us/step - loss: 0.0321 - out_caps_loss: 0.0184 - out_recon_loss: 27.3673 - out_caps_acc: 0.9763 - val_loss: 0.0604 - val_out_caps_loss: 0.0469 - val_out_recon_loss: 27.1816 - val_out_caps_acc: 0.9386\n",
            "Epoch 102/150\n",
            "12896/12896 [==============================] - 12s 912us/step - loss: 0.0331 - out_caps_loss: 0.0194 - out_recon_loss: 27.3673 - out_caps_acc: 0.9751 - val_loss: 0.0659 - val_out_caps_loss: 0.0523 - val_out_recon_loss: 27.1830 - val_out_caps_acc: 0.9252\n",
            "Epoch 103/150\n",
            "12896/12896 [==============================] - 11s 816us/step - loss: 0.0331 - out_caps_loss: 0.0194 - out_recon_loss: 27.3677 - out_caps_acc: 0.9758 - val_loss: 0.0765 - val_out_caps_loss: 0.0629 - val_out_recon_loss: 27.1815 - val_out_caps_acc: 0.9172\n",
            "Epoch 104/150\n",
            "12896/12896 [==============================] - 10s 811us/step - loss: 0.0327 - out_caps_loss: 0.0190 - out_recon_loss: 27.3677 - out_caps_acc: 0.9750 - val_loss: 0.0660 - val_out_caps_loss: 0.0524 - val_out_recon_loss: 27.1828 - val_out_caps_acc: 0.9287\n",
            "Epoch 105/150\n",
            "12896/12896 [==============================] - 11s 815us/step - loss: 0.0318 - out_caps_loss: 0.0181 - out_recon_loss: 27.3672 - out_caps_acc: 0.9773 - val_loss: 0.0639 - val_out_caps_loss: 0.0503 - val_out_recon_loss: 27.1843 - val_out_caps_acc: 0.9352\n",
            "Epoch 106/150\n",
            "12896/12896 [==============================] - 12s 915us/step - loss: 0.0308 - out_caps_loss: 0.0171 - out_recon_loss: 27.3674 - out_caps_acc: 0.9786 - val_loss: 0.0607 - val_out_caps_loss: 0.0472 - val_out_recon_loss: 27.1831 - val_out_caps_acc: 0.9364\n",
            "Epoch 107/150\n",
            "12896/12896 [==============================] - 10s 809us/step - loss: 0.0326 - out_caps_loss: 0.0189 - out_recon_loss: 27.3676 - out_caps_acc: 0.9762 - val_loss: 0.0721 - val_out_caps_loss: 0.0585 - val_out_recon_loss: 27.1819 - val_out_caps_acc: 0.9225\n",
            "Epoch 108/150\n",
            "12896/12896 [==============================] - 11s 815us/step - loss: 0.0303 - out_caps_loss: 0.0166 - out_recon_loss: 27.3678 - out_caps_acc: 0.9784 - val_loss: 0.0633 - val_out_caps_loss: 0.0498 - val_out_recon_loss: 27.1814 - val_out_caps_acc: 0.9342\n",
            "Epoch 109/150\n",
            "12896/12896 [==============================] - 10s 813us/step - loss: 0.0307 - out_caps_loss: 0.0170 - out_recon_loss: 27.3675 - out_caps_acc: 0.9787 - val_loss: 0.0651 - val_out_caps_loss: 0.0515 - val_out_recon_loss: 27.1837 - val_out_caps_acc: 0.9287\n",
            "Epoch 110/150\n",
            "12896/12896 [==============================] - 10s 808us/step - loss: 0.0307 - out_caps_loss: 0.0171 - out_recon_loss: 27.3675 - out_caps_acc: 0.9786 - val_loss: 0.0735 - val_out_caps_loss: 0.0599 - val_out_recon_loss: 27.1823 - val_out_caps_acc: 0.9200\n",
            "Epoch 111/150\n",
            "12896/12896 [==============================] - 11s 841us/step - loss: 0.0316 - out_caps_loss: 0.0179 - out_recon_loss: 27.3675 - out_caps_acc: 0.9777 - val_loss: 0.0634 - val_out_caps_loss: 0.0498 - val_out_recon_loss: 27.1816 - val_out_caps_acc: 0.9333\n",
            "Epoch 112/150\n",
            "12896/12896 [==============================] - 11s 835us/step - loss: 0.0323 - out_caps_loss: 0.0186 - out_recon_loss: 27.3675 - out_caps_acc: 0.9761 - val_loss: 0.0696 - val_out_caps_loss: 0.0560 - val_out_recon_loss: 27.1815 - val_out_caps_acc: 0.9228\n",
            "Epoch 113/150\n",
            "12896/12896 [==============================] - 11s 883us/step - loss: 0.0301 - out_caps_loss: 0.0165 - out_recon_loss: 27.3673 - out_caps_acc: 0.9795 - val_loss: 0.0611 - val_out_caps_loss: 0.0475 - val_out_recon_loss: 27.1825 - val_out_caps_acc: 0.9386\n",
            "Trained model saved to 'trained_model.h5'\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.engine.training.Model at 0x7f3e50abbf28>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 81
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uhTBU77HY8S2",
        "colab_type": "text"
      },
      "source": [
        "# Testing on the data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ulR7KiVsY-aN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "x_test = df2.iloc[:, 0:20].values\n",
        "y_test = df2.iloc[:, 20].values"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T3iGIOfdY-Xy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "layer_name = 'out_caps'\n",
        "intermediate_layer_model = models.Model(inputs=model.input[0],\n",
        "                                 outputs=model.get_layer(layer_name).output)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nZaAcVh5pOGQ",
        "colab_type": "code",
        "outputId": "ed4c62bf-662d-4771-b780-78302eff8940",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "x_test.shape"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(4030, 20)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 116
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oBPX1mfnY-Vu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "x_test_reshape = x_test.reshape(4030, 20, 1)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B98_i1CdxvND",
        "colab_type": "code",
        "outputId": "a12981e3-7e3f-4df0-ffa6-55121a4a32e8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "x_test_reshape.shape"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(4030, 20, 1)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 118
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bBoUOs4DpeEZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "y_pred = intermediate_layer_model.predict(x_test_reshape) "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UnmmCWjy4Gez",
        "colab_type": "code",
        "outputId": "029a9300-ae7e-491c-ba3b-a33ac1b7ee1c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "cm3 = confusion_matrix(y_test, np.argmax(y_pred, axis=1))\n",
        "cm3"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[1931,   84],\n",
              "       [  86, 1929]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 140
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Gp6WLHSf4F5S",
        "colab_type": "code",
        "outputId": "5e0585f4-4893-4011-a5df-8140992b5718",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "np.argmax(y_pred, axis = 1)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([1, 1, 1, ..., 1, 0, 0])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 40
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uRZCmEOO4F0I",
        "colab_type": "code",
        "outputId": "142490fe-4239-4696-d80e-ca0fe9baf86a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "from sklearn.metrics import roc_auc_score\n",
        "roc_auc_score(y_test, np.argmax(y_pred, axis = 1))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.9397022332506204"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 41
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PSv40vY-PJNB",
        "colab_type": "text"
      },
      "source": [
        "## Testing Data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qnsdxi0URJ-5",
        "colab_type": "text"
      },
      "source": [
        "## Uploading another file as an independent test set"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JPuYRLpFITj8",
        "colab_type": "code",
        "outputId": "2c67375a-35d4-4186-8283-e9c9f4fbb51c",
        "colab": {
          "resources": {
            "http://localhost:8080/nbextensions/google.colab/files.js": {
              "data": "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",
              "ok": true,
              "headers": [
                [
                  "content-type",
                  "application/javascript"
                ]
              ],
              "status": 200,
              "status_text": ""
            }
          },
          "base_uri": "https://localhost:8080/",
          "height": 74
        }
      },
      "source": [
        "from google.colab import files\n",
        "uploaded2 = files.upload()\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-382da004-0946-46ef-9165-4e4ea0421b05\" name=\"files[]\" multiple disabled />\n",
              "     <output id=\"result-382da004-0946-46ef-9165-4e4ea0421b05\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script src=\"/nbextensions/google.colab/files.js\"></script> "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Saving test_AComp2.csv to test_AComp2.csv\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GF_oFafHRSJT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pandas as pd\n",
        "import io\n",
        "df3 = pd.read_csv(io.BytesIO(uploaded2['test_AComp2.csv']))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A53POXIARiJA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "x_test_data = df3.iloc[:, 0:20].values\n",
        "y_test_data = df3.iloc[:, 20].values"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cGR2a4KQRsdE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "y_true = y_test_data"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0OSi1TR9Rtfm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "layer_name = 'out_caps'\n",
        "intermediate_layer_model2 = models.Model(inputs=model.input[0],\n",
        "                                 outputs=model.get_layer(layer_name).output)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1KlxbPLKR9J-",
        "colab_type": "code",
        "outputId": "7ced8086-0c16-4f14-8e83-cd42765ee6ea",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "x_test_data.shape"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(4030, 20)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 32
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fDjG4hdbR4D9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "x_test_reshape2 = x_test_data.reshape(4030, 20, 1)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nzjXbCS5SB6x",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "y_pred = intermediate_layer_model2.predict(x_test_reshape2)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0LjmKtaPTzUu",
        "colab_type": "code",
        "outputId": "d0e89e1d-225c-4586-c932-ff4004df55b2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "cm = confusion_matrix(np.argmax(y_test_, axis=1), np.argmax(y_pred, axis=1))\n",
        "cm"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[1851,  164],\n",
              "       [  79, 1936]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 44
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SPdvD_-lSt20",
        "colab_type": "code",
        "outputId": "3bdaf15e-755d-4081-c63f-d1d203ac1b48",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "y_true.shape, y_pred.shape"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((4030,), (4030, 2))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 37
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FAs9W4yOTNvT",
        "colab_type": "code",
        "outputId": "8d0f59e7-c247-4d14-aa01-4a200b2fcd8b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 136
        }
      },
      "source": [
        "y_pred"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0.32652962, 0.9261913 ],\n",
              "       [0.1351722 , 0.8690385 ],\n",
              "       [0.02157693, 0.8851954 ],\n",
              "       ...,\n",
              "       [0.8924629 , 0.12365953],\n",
              "       [0.8798937 , 0.58914596],\n",
              "       [0.9349014 , 0.22312114]], dtype=float32)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 41
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iaJHaVZ3SqoM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "final_predNew = (y_pred >= 0.5).astype(np.int)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X6E8HbMcTkUi",
        "colab_type": "code",
        "outputId": "d95fb2f9-c50a-4fd5-95cf-30094e598dcb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 136
        }
      },
      "source": [
        "final_predNew"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0, 1],\n",
              "       [0, 1],\n",
              "       [0, 1],\n",
              "       ...,\n",
              "       [1, 0],\n",
              "       [1, 1],\n",
              "       [1, 0]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 43
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i9EndR3hTmpe",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fPwHK_tM8LL3",
        "colab_type": "text"
      },
      "source": [
        "# Gradient Boosting Classification"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b6fWtgKy8NMN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.ensemble import GradientBoostingClassifier\n",
        "gbr = GradientBoostingClassifier(n_estimators=1000, learning_rate=0.01,max_depth=5, random_state=0)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-hrlGsvr8X8t",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.model_selection import cross_val_score, StratifiedKFold\n",
        "scores_gbr_mse = cross_val_score(gbr,x_train , y_train, cv=5, scoring='accuracy')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pbNGaZ279VfR",
        "colab_type": "code",
        "outputId": "3ebe73a8-e1f1-470f-8836-d108fd89666d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "scores_gbr_mse"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([0.9351737 , 0.93145161, 0.9280397 , 0.92307692, 0.9233871 ])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 55
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OTNaTHzW9ZPL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.ensemble import AdaBoostClassifier\n",
        "abc = AdaBoostClassifier()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7H73Xm-c_ILu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.model_selection import GridSearchCV\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oTP6OgNJ_gf1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "parameters = {'n_estimators':(2000, 2500, 3000, 3500, 4000)}"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iZki_1_QA6Lo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "clf1 = GridSearchCV(gbr, parameters, cv = 5)\n",
        "clf2 = GridSearchCV(abc, parameters, cv = 5)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R5qDdhM4BHmG",
        "colab_type": "code",
        "outputId": "2b7676a9-4ad6-47da-adb8-92b0876fb3a5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 391
        }
      },
      "source": [
        "clf1.fit(x_train, y_train)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "GridSearchCV(cv=5, error_score='raise-deprecating',\n",
              "             estimator=GradientBoostingClassifier(criterion='friedman_mse',\n",
              "                                                  init=None, learning_rate=0.01,\n",
              "                                                  loss='deviance', max_depth=5,\n",
              "                                                  max_features=None,\n",
              "                                                  max_leaf_nodes=None,\n",
              "                                                  min_impurity_decrease=0.0,\n",
              "                                                  min_impurity_split=None,\n",
              "                                                  min_samples_leaf=1,\n",
              "                                                  min_samples_split=2,\n",
              "                                                  min_weight_fraction_leaf=0.0,\n",
              "                                                  n_estimators=1000,\n",
              "                                                  n_iter_no_change=None,\n",
              "                                                  presort='auto',\n",
              "                                                  random_state=0, subsample=1.0,\n",
              "                                                  tol=0.0001,\n",
              "                                                  validation_fraction=0.1,\n",
              "                                                  verbose=0, warm_start=False),\n",
              "             iid='warn', n_jobs=None,\n",
              "             param_grid={'n_estimators': (2000, 2500, 3000, 3500, 4000)},\n",
              "             pre_dispatch='2*n_jobs', refit=True, return_train_score=False,\n",
              "             scoring=None, verbose=0)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 61
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UMVEVlDyOE15",
        "colab_type": "code",
        "outputId": "11fce5af-7356-4b46-e909-35732b7d3592",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "y_pred1 = clf1.predict(x_test)\n",
        "from sklearn.metrics import accuracy_score\n",
        "accuracy_score(y_test, y_pred1)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.9506203473945409"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 65
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dAQ-wLpcBTEF",
        "colab_type": "code",
        "outputId": "50d6af61-8561-42c0-acfd-b1eae89b4c15",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 190
        }
      },
      "source": [
        "clf2.fit(x_train, y_train)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "GridSearchCV(cv=5, error_score='raise-deprecating',\n",
              "             estimator=AdaBoostClassifier(algorithm='SAMME.R',\n",
              "                                          base_estimator=None,\n",
              "                                          learning_rate=1.0, n_estimators=50,\n",
              "                                          random_state=None),\n",
              "             iid='warn', n_jobs=None,\n",
              "             param_grid={'n_estimators': (2000, 2500, 3000, 3500, 4000)},\n",
              "             pre_dispatch='2*n_jobs', refit=True, return_train_score=False,\n",
              "             scoring=None, verbose=0)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 66
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_nPm7Lw6OofJ",
        "colab_type": "code",
        "outputId": "527d2789-a847-4e25-ca0d-a76efd780af4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "y_pred2 = clf2.predict(x_test)\n",
        "from sklearn.metrics import accuracy_score\n",
        "accuracy_score(y_test, y_pred2)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.9022332506203474"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 68
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jqNkuULALbo2",
        "colab_type": "text"
      },
      "source": [
        "# Training and Testing on Set 2"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l5e_L_ZIY4Jn",
        "colab_type": "code",
        "outputId": "39e98118-bca6-4b89-af0b-a18117ba419e",
        "colab": {
          "resources": {
            "http://localhost:8080/nbextensions/google.colab/files.js": {
              "data": "Ly8gQ29weXJpZ2h0IDIwMTcgR29vZ2xlIExMQwovLwovLyBMaWNlbnNlZCB1bmRlciB0aGUgQXBhY2hlIExpY2Vuc2UsIFZlcnNpb24gMi4wICh0aGUgIkxpY2Vuc2UiKTsKLy8geW91IG1heSBub3QgdXNlIHRoaXMgZmlsZSBleGNlcHQgaW4gY29tcGxpYW5jZSB3aXRoIHRoZSBMaWNlbnNlLgovLyBZb3UgbWF5IG9idGFpbiBhIGNvcHkgb2YgdGhlIExpY2Vuc2UgYXQKLy8KLy8gICAgICBodHRwOi8vd3d3LmFwYWNoZS5vcmcvbGljZW5zZXMvTElDRU5TRS0yLjAKLy8KLy8gVW5sZXNzIHJlcXVpcmVkIGJ5IGFwcGxpY2FibGUgbGF3IG9yIGFncmVlZCB0byBpbiB3cml0aW5nLCBzb2Z0d2FyZQovLyBkaXN0cmlidXRlZCB1bmRlciB0aGUgTGljZW5zZSBpcyBkaXN0cmlidXRlZCBvbiBhbiAiQVMgSVMiIEJBU0lTLAovLyBXSVRIT1VUIFdBUlJBTlRJRVMgT1IgQ09ORElUSU9OUyBPRiBBTlkgS0lORCwgZWl0aGVyIGV4cHJlc3Mgb3IgaW1wbGllZC4KLy8gU2VlIHRoZSBMaWNlbnNlIGZvciB0aGUgc3BlY2lmaWMgbGFuZ3VhZ2UgZ292ZXJuaW5nIHBlcm1pc3Npb25zIGFuZAovLyBsaW1pdGF0aW9ucyB1bmRlciB0aGUgTGljZW5zZS4KCi8qKgogKiBAZmlsZW92ZXJ2aWV3IEhlbHBlcnMgZm9yIGdvb2dsZS5jb2xhYiBQeXRob24gbW9kdWxlLgogKi8KKGZ1bmN0aW9uKHNjb3BlKSB7CmZ1bmN0aW9uIHNwYW4odGV4dCwgc3R5bGVBdHRyaWJ1dGVzID0ge30pIHsKICBjb25zdCBlbGVtZW50ID0gZG9jdW1lbnQuY3JlYXRlRWxlbWVudCgnc3BhbicpOwogIGVsZW1lbnQudGV4dENvbnRlbnQgPSB0ZXh0OwogIGZvciAoY29uc3Qga2V5IG9mIE9iamVjdC5rZXlzKHN0eWxlQXR0cmlidXRlcykpIHsKICAgIGVsZW1lbnQuc3R5bGVba2V5XSA9IHN0eWxlQXR0cmlidXRlc1trZXldOwogIH0KICByZXR1cm4gZWxlbWVudDsKfQoKLy8gTWF4IG51bWJlciBvZiBieXRlcyB3aGljaCB3aWxsIGJlIHVwbG9hZGVkIGF0IGEgdGltZS4KY29uc3QgTUFYX1BBWUxPQURfU0laRSA9IDEwMCAqIDEwMjQ7Ci8vIE1heCBhbW91bnQgb2YgdGltZSB0byBibG9jayB3YWl0aW5nIGZvciB0aGUgdXNlci4KY29uc3QgRklMRV9DSEFOR0VfVElNRU9VVF9NUyA9IDMwICogMTAwMDsKCmZ1bmN0aW9uIF91cGxvYWRGaWxlcyhpbnB1dElkLCBvdXRwdXRJZCkgewogIGNvbnN0IHN0ZXBzID0gdXBsb2FkRmlsZXNTdGVwKGlucHV0SWQsIG91dHB1dElkKTsKICBjb25zdCBvdXRwdXRFbGVtZW50ID0gZG9jdW1lbnQuZ2V0RWxlbWVudEJ5SWQob3V0cHV0SWQpOwogIC8vIENhY2hlIHN0ZXBzIG9uIHRoZSBvdXRwdXRFbGVtZW50IHRvIG1ha2UgaXQgYXZhaWxhYmxlIGZvciB0aGUgbmV4dCBjYWxsCiAgLy8gdG8gdXBsb2FkRmlsZXNDb250aW51ZSBmcm9tIFB5dGhvbi4KICBvdXRwdXRFbGVtZW50LnN0ZXBzID0gc3RlcHM7CgogIHJldHVybiBfdXBsb2FkRmlsZXNDb250aW51ZShvdXRwdXRJZCk7Cn0KCi8vIFRoaXMgaXMgcm91Z2hseSBhbiBhc3luYyBnZW5lcmF0b3IgKG5vdCBzdXBwb3J0ZWQgaW4gdGhlIGJyb3dzZXIgeWV0KSwKLy8gd2hlcmUgdGhlcmUgYXJlIG11bHRpcGxlIGFzeW5jaHJvbm91cyBzdGVwcyBhbmQgdGhlIFB5dGhvbiBzaWRlIGlzIGdvaW5nCi8vIHRvIHBvbGwgZm9yIGNvbXBsZXRpb24gb2YgZWFjaCBzdGVwLgovLyBUaGlzIHVzZXMgYSBQcm9taXNlIHRvIGJsb2NrIHRoZSBweXRob24gc2lkZSBvbiBjb21wbGV0aW9uIG9mIGVhY2ggc3RlcCwKLy8gdGhlbiBwYXNzZXMgdGhlIHJlc3VsdCBvZiB0aGUgcHJldmlvdXMgc3RlcCBhcyB0aGUgaW5wdXQgdG8gdGhlIG5leHQgc3RlcC4KZnVuY3Rpb24gX3VwbG9hZEZpbGVzQ29udGludWUob3V0cHV0SWQpIHsKICBjb25zdCBvdXRwdXRFbGVtZW50ID0gZG9jdW1lbnQuZ2V0RWxlbWVudEJ5SWQob3V0cHV0SWQpOwogIGNvbnN0IHN0ZXBzID0gb3V0cHV0RWxlbWVudC5zdGVwczsKCiAgY29uc3QgbmV4dCA9IHN0ZXBzLm5leHQob3V0cHV0RWxlbWVudC5sYXN0UHJvbWlzZVZhbHVlKTsKICByZXR1cm4gUHJvbWlzZS5yZXNvbHZlKG5leHQudmFsdWUucHJvbWlzZSkudGhlbigodmFsdWUpID0+IHsKICAgIC8vIENhY2hlIHRoZSBsYXN0IHByb21pc2UgdmFsdWUgdG8gbWFrZSBpdCBhdmFpbGFibGUgdG8gdGhlIG5leHQKICAgIC8vIHN0ZXAgb2YgdGhlIGdlbmVyYXRvci4KICAgIG91dHB1dEVsZW1lbnQubGFzdFByb21pc2VWYWx1ZSA9IHZhbHVlOwogICAgcmV0dXJuIG5leHQudmFsdWUucmVzcG9uc2U7CiAgfSk7Cn0KCi8qKgogKiBHZW5lcmF0b3IgZnVuY3Rpb24gd2hpY2ggaXMgY2FsbGVkIGJldHdlZW4gZWFjaCBhc3luYyBzdGVwIG9mIHRoZSB1cGxvYWQKICogcHJvY2Vzcy4KICogQHBhcmFtIHtzdHJpbmd9IGlucHV0SWQgRWxlbWVudCBJRCBvZiB0aGUgaW5wdXQgZmlsZSBwaWNrZXIgZWxlbWVudC4KICogQHBhcmFtIHtzdHJpbmd9IG91dHB1dElkIEVsZW1lbnQgSUQgb2YgdGhlIG91dHB1dCBkaXNwbGF5LgogKiBAcmV0dXJuIHshSXRlcmFibGU8IU9iamVjdD59IEl0ZXJhYmxlIG9mIG5leHQgc3RlcHMuCiAqLwpmdW5jdGlvbiogdXBsb2FkRmlsZXNTdGVwKGlucHV0SWQsIG91dHB1dElkKSB7CiAgY29uc3QgaW5wdXRFbGVtZW50ID0gZG9jdW1lbnQuZ2V0RWxlbWVudEJ5SWQoaW5wdXRJZCk7CiAgaW5wdXRFbGVtZW50LmRpc2FibGVkID0gZmFsc2U7CgogIGNvbnN0IG91dHB1dEVsZW1lbnQgPSBkb2N1bWVudC5nZXRFbGVtZW50QnlJZChvdXRwdXRJZCk7CiAgb3V0cHV0RWxlbWVudC5pbm5lckhUTUwgPSAnJzsKCiAgY29uc3QgcGlja2VkUHJvbWlzZSA9IG5ldyBQcm9taXNlKChyZXNvbHZlKSA9PiB7CiAgICBpbnB1dEVsZW1lbnQuYWRkRXZlbnRMaXN0ZW5lcignY2hhbmdlJywgKGUpID0+IHsKICAgICAgcmVzb2x2ZShlLnRhcmdldC5maWxlcyk7CiAgICB9KTsKICB9KTsKCiAgY29uc3QgY2FuY2VsID0gZG9jdW1lbnQuY3JlYXRlRWxlbWVudCgnYnV0dG9uJyk7CiAgaW5wdXRFbGVtZW50LnBhcmVudEVsZW1lbnQuYXBwZW5kQ2hpbGQoY2FuY2VsKTsKICBjYW5jZWwudGV4dENvbnRlbnQgPSAnQ2FuY2VsIHVwbG9hZCc7CiAgY29uc3QgY2FuY2VsUHJvbWlzZSA9IG5ldyBQcm9taXNlKChyZXNvbHZlKSA9PiB7CiAgICBjYW5jZWwub25jbGljayA9ICgpID0+IHsKICAgICAgcmVzb2x2ZShudWxsKTsKICAgIH07CiAgfSk7CgogIC8vIENhbmNlbCB1cGxvYWQgaWYgdXNlciBoYXNuJ3QgcGlja2VkIGFueXRoaW5nIGluIHRpbWVvdXQuCiAgY29uc3QgdGltZW91dFByb21pc2UgPSBuZXcgUHJvbWlzZSgocmVzb2x2ZSkgPT4gewogICAgc2V0VGltZW91dCgoKSA9PiB7CiAgICAgIHJlc29sdmUobnVsbCk7CiAgICB9LCBGSUxFX0NIQU5HRV9USU1FT1VUX01TKTsKICB9KTsKCiAgLy8gV2FpdCBmb3IgdGhlIHVzZXIgdG8gcGljayB0aGUgZmlsZXMuCiAgY29uc3QgZmlsZXMgPSB5aWVsZCB7CiAgICBwcm9taXNlOiBQcm9taXNlLnJhY2UoW3BpY2tlZFByb21pc2UsIHRpbWVvdXRQcm9taXNlLCBjYW5jZWxQcm9taXNlXSksCiAgICByZXNwb25zZTogewogICAgICBhY3Rpb246ICdzdGFydGluZycsCiAgICB9CiAgfTsKCiAgaWYgKCFmaWxlcykgewogICAgcmV0dXJuIHsKICAgICAgcmVzcG9uc2U6IHsKICAgICAgICBhY3Rpb246ICdjb21wbGV0ZScsCiAgICAgIH0KICAgIH07CiAgfQoKICBjYW5jZWwucmVtb3ZlKCk7CgogIC8vIERpc2FibGUgdGhlIGlucHV0IGVsZW1lbnQgc2luY2UgZnVydGhlciBwaWNrcyBhcmUgbm90IGFsbG93ZWQuCiAgaW5wdXRFbGVtZW50LmRpc2FibGVkID0gdHJ1ZTsKCiAgZm9yIChjb25zdCBmaWxlIG9mIGZpbGVzKSB7CiAgICBjb25zdCBsaSA9IGRvY3VtZW50LmNyZWF0ZUVsZW1lbnQoJ2xpJyk7CiAgICBsaS5hcHBlbmQoc3BhbihmaWxlLm5hbWUsIHtmb250V2VpZ2h0OiAnYm9sZCd9KSk7CiAgICBsaS5hcHBlbmQoc3BhbigKICAgICAgICBgKCR7ZmlsZS50eXBlIHx8ICduL2EnfSkgLSAke2ZpbGUuc2l6ZX0gYnl0ZXMsIGAgKwogICAgICAgIGBsYXN0IG1vZGlmaWVkOiAkewogICAgICAgICAgICBmaWxlLmxhc3RNb2RpZmllZERhdGUgPyBmaWxlLmxhc3RNb2RpZmllZERhdGUudG9Mb2NhbGVEYXRlU3RyaW5nKCkgOgogICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAnbi9hJ30gLSBgKSk7CiAgICBjb25zdCBwZXJjZW50ID0gc3BhbignMCUgZG9uZScpOwogICAgbGkuYXBwZW5kQ2hpbGQocGVyY2VudCk7CgogICAgb3V0cHV0RWxlbWVudC5hcHBlbmRDaGlsZChsaSk7CgogICAgY29uc3QgZmlsZURhdGFQcm9taXNlID0gbmV3IFByb21pc2UoKHJlc29sdmUpID0+IHsKICAgICAgY29uc3QgcmVhZGVyID0gbmV3IEZpbGVSZWFkZXIoKTsKICAgICAgcmVhZGVyLm9ubG9hZCA9IChlKSA9PiB7CiAgICAgICAgcmVzb2x2ZShlLnRhcmdldC5yZXN1bHQpOwogICAgICB9OwogICAgICByZWFkZXIucmVhZEFzQXJyYXlCdWZmZXIoZmlsZSk7CiAgICB9KTsKICAgIC8vIFdhaXQgZm9yIHRoZSBkYXRhIHRvIGJlIHJlYWR5LgogICAgbGV0IGZpbGVEYXRhID0geWllbGQgewogICAgICBwcm9taXNlOiBmaWxlRGF0YVByb21pc2UsCiAgICAgIHJlc3BvbnNlOiB7CiAgICAgICAgYWN0aW9uOiAnY29udGludWUnLAogICAgICB9CiAgICB9OwoKICAgIC8vIFVzZSBhIGNodW5rZWQgc2VuZGluZyB0byBhdm9pZCBtZXNzYWdlIHNpemUgbGltaXRzLiBTZWUgYi82MjExNTY2MC4KICAgIGxldCBwb3NpdGlvbiA9IDA7CiAgICB3aGlsZSAocG9zaXRpb24gPCBmaWxlRGF0YS5ieXRlTGVuZ3RoKSB7CiAgICAgIGNvbnN0IGxlbmd0aCA9IE1hdGgubWluKGZpbGVEYXRhLmJ5dGVMZW5ndGggLSBwb3NpdGlvbiwgTUFYX1BBWUxPQURfU0laRSk7CiAgICAgIGNvbnN0IGNodW5rID0gbmV3IFVpbnQ4QXJyYXkoZmlsZURhdGEsIHBvc2l0aW9uLCBsZW5ndGgpOwogICAgICBwb3NpdGlvbiArPSBsZW5ndGg7CgogICAgICBjb25zdCBiYXNlNjQgPSBidG9hKFN0cmluZy5mcm9tQ2hhckNvZGUuYXBwbHkobnVsbCwgY2h1bmspKTsKICAgICAgeWllbGQgewogICAgICAgIHJlc3BvbnNlOiB7CiAgICAgICAgICBhY3Rpb246ICdhcHBlbmQnLAogICAgICAgICAgZmlsZTogZmlsZS5uYW1lLAogICAgICAgICAgZGF0YTogYmFzZTY0LAogICAgICAgIH0sCiAgICAgIH07CiAgICAgIHBlcmNlbnQudGV4dENvbnRlbnQgPQogICAgICAgICAgYCR7TWF0aC5yb3VuZCgocG9zaXRpb24gLyBmaWxlRGF0YS5ieXRlTGVuZ3RoKSAqIDEwMCl9JSBkb25lYDsKICAgIH0KICB9CgogIC8vIEFsbCBkb25lLgogIHlpZWxkIHsKICAgIHJlc3BvbnNlOiB7CiAgICAgIGFjdGlvbjogJ2NvbXBsZXRlJywKICAgIH0KICB9Owp9CgpzY29wZS5nb29nbGUgPSBzY29wZS5nb29nbGUgfHwge307CnNjb3BlLmdvb2dsZS5jb2xhYiA9IHNjb3BlLmdvb2dsZS5jb2xhYiB8fCB7fTsKc2NvcGUuZ29vZ2xlLmNvbGFiLl9maWxlcyA9IHsKICBfdXBsb2FkRmlsZXMsCiAgX3VwbG9hZEZpbGVzQ29udGludWUsCn07Cn0pKHNlbGYpOwo=",
              "ok": true,
              "headers": [
                [
                  "content-type",
                  "application/javascript"
                ]
              ],
              "status": 200,
              "status_text": ""
            }
          },
          "base_uri": "https://localhost:8080/",
          "height": 74
        }
      },
      "source": [
        "from google.colab import files\n",
        "uploaded = files.upload()\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-c3638ed7-6c5a-42c0-aa09-15933124a1cc\" name=\"files[]\" multiple disabled />\n",
              "     <output id=\"result-c3638ed7-6c5a-42c0-aa09-15933124a1cc\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script src=\"/nbextensions/google.colab/files.js\"></script> "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Saving AComp2.csv to AComp2.csv\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P-cNaSAULmQk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pandas as pd\n",
        "import io\n",
        "df3 = pd.read_csv(io.BytesIO(uploaded['AComp2.csv']))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QnlZPMF8Lszy",
        "colab_type": "code",
        "outputId": "988bf775-8c8f-4ed6-c475-f6cdf630f5ed",
        "colab": {
          "resources": {
            "http://localhost:8080/nbextensions/google.colab/files.js": {
              "data": "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",
              "ok": true,
              "headers": [
                [
                  "content-type",
                  "application/javascript"
                ]
              ],
              "status": 200,
              "status_text": ""
            }
          },
          "base_uri": "https://localhost:8080/",
          "height": 74
        }
      },
      "source": [
        "from google.colab import files\n",
        "uploaded2 = files.upload()\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-d0d91f22-6a10-4fd9-a34f-cb1751bbe8c2\" name=\"files[]\" multiple disabled />\n",
              "     <output id=\"result-d0d91f22-6a10-4fd9-a34f-cb1751bbe8c2\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script src=\"/nbextensions/google.colab/files.js\"></script> "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Saving test_AComp2.csv to test_AComp2.csv\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mFZ8WLkIL397",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df4 = pd.read_csv(io.BytesIO(uploaded2['test_AComp2.csv']))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0RA2a7P7MBml",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "x_train2 = df3.iloc[:,0:20].values\n",
        "y_train2 = df3.iloc[:,20].values"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yrom5t5NMIgZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "x_test2 = df4.iloc[:,0:20].values\n",
        "y_test2 = df4.iloc[:,20].values"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PBo42O3QMTNM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Reshaping the feature samples\n",
        "x_train_reshape2 = x_train2.reshape(16120, 20, 1)\n",
        "y_train_reshape2 = y_train2.reshape(16120, 1)\n",
        "#x_test_reshape = x_test.reshape(4030, 20, 1)\n",
        "#y_test_reshape = y_test.reshape(4030, 1)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yFTlAQJJMYsL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Reshaping the labels\n",
        "y_train2_ = tf.keras.utils.to_categorical(y_train_reshape2,num_classes=2)\n",
        "#y_test_ = tf.keras.utils.to_categorical(y_test_reshape,num_classes=2)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FLAoVY7rMimw",
        "colab_type": "code",
        "outputId": "be88997b-d46a-4a8a-b216-b5b55ae93145",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "  model2 = CapsNet(input_shape=[20, 1],\n",
        "                n_class=2,\n",
        "                num_routing=3)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[20, 1] (?, 20, 1)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cNvEgR0kMofo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train(model=model2, data=((x_train_reshape2, y_train2_)), epoch_size_frac = 0.5)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Iwk3nhGEMtpc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "layer_name = 'out_caps'\n",
        "intermediate_layer_model2 = models.Model(inputs=model2.input[0],\n",
        "                                 outputs=model2.get_layer(layer_name).output)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Nai4OGPLPzRg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "x_test_reshape2 = x_test2.reshape(4030, 20, 1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m13C8xBMQA-P",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "y_pred2 = intermediate_layer_model2.predict(x_test_reshape2)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zs-XiZhmQOck",
        "colab_type": "code",
        "outputId": "15ba9073-5120-4eee-8973-2aa6edb5b7e8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "cm2 = confusion_matrix(y_test2, np.argmax(y_pred2, axis=1))\n",
        "cm2"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[1898,  117],\n",
              "       [ 105, 1910]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 164
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bCyu2iZc9KZ0",
        "colab_type": "text"
      },
      "source": [
        "# Training and Testing on Set 3"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eROgOeDKQYf7",
        "colab_type": "code",
        "outputId": "dfa9ae84-f023-4f99-b6f0-71a3752e05ba",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "model3 = CapsNet(input_shape=[20, 1],\n",
        "                n_class=2,\n",
        "                num_routing=3)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[20, 1] (?, 20, 1)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cgdRJnAu9W8s",
        "colab_type": "text"
      },
      "source": [
        "## Loading the training and testing sets"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JYWkzUle9VvU",
        "colab_type": "code",
        "outputId": "3abfa058-9a13-4776-b13d-7b456164cec8",
        "colab": {
          "resources": {
            "http://localhost:8080/nbextensions/google.colab/files.js": {
              "data": "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",
              "ok": true,
              "headers": [
                [
                  "content-type",
                  "application/javascript"
                ]
              ],
              "status": 200,
              "status_text": ""
            }
          },
          "base_uri": "https://localhost:8080/",
          "height": 74
        }
      },
      "source": [
        "from google.colab import files\n",
        "uploaded = files.upload()\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-c82775a8-2ada-4c05-96cd-f4caf2be867f\" name=\"files[]\" multiple disabled />\n",
              "     <output id=\"result-c82775a8-2ada-4c05-96cd-f4caf2be867f\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script src=\"/nbextensions/google.colab/files.js\"></script> "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Saving AComp3.csv to AComp3.csv\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zxVOuAjN9bkp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pandas as pd\n",
        "import io\n",
        "df4 = pd.read_csv(io.BytesIO(uploaded['AComp3.csv']))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Wx6frPxI9j7k",
        "colab_type": "code",
        "outputId": "67677e1b-7709-4e58-b699-45a275ea6bcd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 224
        }
      },
      "source": [
        "df4.head()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>A</th>\n",
              "      <th>C</th>\n",
              "      <th>D</th>\n",
              "      <th>E</th>\n",
              "      <th>F</th>\n",
              "      <th>G</th>\n",
              "      <th>H</th>\n",
              "      <th>I</th>\n",
              "      <th>K</th>\n",
              "      <th>L</th>\n",
              "      <th>M</th>\n",
              "      <th>N</th>\n",
              "      <th>P</th>\n",
              "      <th>Q</th>\n",
              "      <th>R</th>\n",
              "      <th>S</th>\n",
              "      <th>T</th>\n",
              "      <th>V</th>\n",
              "      <th>W</th>\n",
              "      <th>Y</th>\n",
              "      <th>Label</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>5.41</td>\n",
              "      <td>0.00</td>\n",
              "      <td>8.11</td>\n",
              "      <td>11.49</td>\n",
              "      <td>6.08</td>\n",
              "      <td>3.38</td>\n",
              "      <td>2.03</td>\n",
              "      <td>9.46</td>\n",
              "      <td>13.51</td>\n",
              "      <td>10.14</td>\n",
              "      <td>5.41</td>\n",
              "      <td>2.70</td>\n",
              "      <td>2.03</td>\n",
              "      <td>4.05</td>\n",
              "      <td>4.05</td>\n",
              "      <td>2.03</td>\n",
              "      <td>2.70</td>\n",
              "      <td>5.41</td>\n",
              "      <td>0.00</td>\n",
              "      <td>2.03</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>6.88</td>\n",
              "      <td>0.00</td>\n",
              "      <td>5.62</td>\n",
              "      <td>8.75</td>\n",
              "      <td>5.00</td>\n",
              "      <td>8.75</td>\n",
              "      <td>2.50</td>\n",
              "      <td>8.12</td>\n",
              "      <td>9.38</td>\n",
              "      <td>5.62</td>\n",
              "      <td>1.25</td>\n",
              "      <td>5.62</td>\n",
              "      <td>5.00</td>\n",
              "      <td>1.25</td>\n",
              "      <td>1.88</td>\n",
              "      <td>6.88</td>\n",
              "      <td>5.62</td>\n",
              "      <td>7.50</td>\n",
              "      <td>0.00</td>\n",
              "      <td>4.38</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>9.84</td>\n",
              "      <td>0.26</td>\n",
              "      <td>5.96</td>\n",
              "      <td>5.44</td>\n",
              "      <td>5.18</td>\n",
              "      <td>5.70</td>\n",
              "      <td>0.78</td>\n",
              "      <td>5.18</td>\n",
              "      <td>6.99</td>\n",
              "      <td>10.10</td>\n",
              "      <td>2.85</td>\n",
              "      <td>4.40</td>\n",
              "      <td>3.63</td>\n",
              "      <td>3.63</td>\n",
              "      <td>2.07</td>\n",
              "      <td>7.77</td>\n",
              "      <td>10.62</td>\n",
              "      <td>4.92</td>\n",
              "      <td>0.52</td>\n",
              "      <td>4.15</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>3.91</td>\n",
              "      <td>1.95</td>\n",
              "      <td>0.33</td>\n",
              "      <td>1.63</td>\n",
              "      <td>3.26</td>\n",
              "      <td>2.28</td>\n",
              "      <td>1.30</td>\n",
              "      <td>5.21</td>\n",
              "      <td>0.65</td>\n",
              "      <td>8.14</td>\n",
              "      <td>0.65</td>\n",
              "      <td>1.95</td>\n",
              "      <td>15.64</td>\n",
              "      <td>34.20</td>\n",
              "      <td>1.95</td>\n",
              "      <td>5.21</td>\n",
              "      <td>2.28</td>\n",
              "      <td>5.21</td>\n",
              "      <td>0.33</td>\n",
              "      <td>3.91</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>6.99</td>\n",
              "      <td>6.99</td>\n",
              "      <td>1.40</td>\n",
              "      <td>6.99</td>\n",
              "      <td>3.50</td>\n",
              "      <td>4.90</td>\n",
              "      <td>0.70</td>\n",
              "      <td>3.50</td>\n",
              "      <td>2.10</td>\n",
              "      <td>9.79</td>\n",
              "      <td>4.20</td>\n",
              "      <td>5.59</td>\n",
              "      <td>9.79</td>\n",
              "      <td>6.29</td>\n",
              "      <td>4.20</td>\n",
              "      <td>5.59</td>\n",
              "      <td>6.29</td>\n",
              "      <td>5.59</td>\n",
              "      <td>1.40</td>\n",
              "      <td>4.20</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "      A     C     D      E     F     G  ...     S      T     V     W     Y  Label\n",
              "0  5.41  0.00  8.11  11.49  6.08  3.38  ...  2.03   2.70  5.41  0.00  2.03      1\n",
              "1  6.88  0.00  5.62   8.75  5.00  8.75  ...  6.88   5.62  7.50  0.00  4.38      1\n",
              "2  9.84  0.26  5.96   5.44  5.18  5.70  ...  7.77  10.62  4.92  0.52  4.15      1\n",
              "3  3.91  1.95  0.33   1.63  3.26  2.28  ...  5.21   2.28  5.21  0.33  3.91      1\n",
              "4  6.99  6.99  1.40   6.99  3.50  4.90  ...  5.59   6.29  5.59  1.40  4.20      1\n",
              "\n",
              "[5 rows x 21 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NO0eefGe9lR9",
        "colab_type": "code",
        "outputId": "d3f903c2-7708-4c9b-fd56-0e66851dad30",
        "colab": {
          "resources": {
            "http://localhost:8080/nbextensions/google.colab/files.js": {
              "data": "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",
              "ok": true,
              "headers": [
                [
                  "content-type",
                  "application/javascript"
                ]
              ],
              "status": 200,
              "status_text": ""
            }
          },
          "base_uri": "https://localhost:8080/",
          "height": 74
        }
      },
      "source": [
        "uploaded = files.upload()\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-4a51fd19-55e9-43fa-b944-29ae505919b9\" name=\"files[]\" multiple disabled />\n",
              "     <output id=\"result-4a51fd19-55e9-43fa-b944-29ae505919b9\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script src=\"/nbextensions/google.colab/files.js\"></script> "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Saving test_AComp3.csv to test_AComp3.csv\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WGPzImze9oxQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df5 = pd.read_csv(io.BytesIO(uploaded['test_AComp3.csv']))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VCqUlUPN9u8U",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "x_train3 = df4.iloc[:, 0:20].values\n",
        "y_train3 = df4.iloc[:, 20].values"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R0VFHd2a93pU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "x_test3 = df5.iloc[:, 0:20].values\n",
        "y_test3 = df5.iloc[:, 20].values"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qEGoc-Vj94dz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Reshaping the feature samples\n",
        "x_train_reshape3 = x_train3.reshape(16120, 20, 1)\n",
        "y_train_reshape3 = y_train3.reshape(16120, 1)\n",
        "#x_test_reshape = x_test.reshape(4030, 20, 1)\n",
        "#y_test_reshape = y_test.reshape(4030, 1)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X_-eJx0U-GnD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Reshaping the Labels\n",
        "y_train3_ = tf.keras.utils.to_categorical(y_train_reshape3,num_classes=2)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ujiUSGf0-OYc",
        "colab_type": "code",
        "outputId": "38b2c6ee-ae69-4d0b-cad7-1724cbb72ff2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 2893
        }
      },
      "source": [
        "train(model=model3, data=((x_train_reshape3, y_train3_)), epoch_size_frac = 0.5)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.cast instead.\n",
            "Train on 12896 samples, validate on 3224 samples\n",
            "Epoch 1/100\n",
            "12896/12896 [==============================] - 16s 1ms/step - loss: 0.1374 - out_caps_loss: 0.1235 - out_recon_loss: 27.9311 - out_caps_acc: 0.8059 - val_loss: 0.1258 - val_out_caps_loss: 0.1122 - val_out_recon_loss: 27.2655 - val_out_caps_acc: 0.8334\n",
            "Epoch 2/100\n",
            "12896/12896 [==============================] - 10s 761us/step - loss: 0.1111 - out_caps_loss: 0.0974 - out_recon_loss: 27.3860 - out_caps_acc: 0.8571 - val_loss: 0.1107 - val_out_caps_loss: 0.0970 - val_out_recon_loss: 27.2642 - val_out_caps_acc: 0.8623\n",
            "Epoch 3/100\n",
            "12896/12896 [==============================] - 10s 765us/step - loss: 0.1032 - out_caps_loss: 0.0895 - out_recon_loss: 27.3853 - out_caps_acc: 0.8740 - val_loss: 0.1033 - val_out_caps_loss: 0.0896 - val_out_recon_loss: 27.2636 - val_out_caps_acc: 0.8703\n",
            "Epoch 4/100\n",
            "12896/12896 [==============================] - 10s 765us/step - loss: 0.0964 - out_caps_loss: 0.0827 - out_recon_loss: 27.3851 - out_caps_acc: 0.8833 - val_loss: 0.0974 - val_out_caps_loss: 0.0838 - val_out_recon_loss: 27.2635 - val_out_caps_acc: 0.8800\n",
            "Epoch 5/100\n",
            "12896/12896 [==============================] - 10s 783us/step - loss: 0.0912 - out_caps_loss: 0.0775 - out_recon_loss: 27.3850 - out_caps_acc: 0.8924 - val_loss: 0.0961 - val_out_caps_loss: 0.0825 - val_out_recon_loss: 27.2635 - val_out_caps_acc: 0.8837\n",
            "Epoch 6/100\n",
            "12896/12896 [==============================] - 10s 800us/step - loss: 0.0877 - out_caps_loss: 0.0740 - out_recon_loss: 27.3849 - out_caps_acc: 0.8982 - val_loss: 0.0919 - val_out_caps_loss: 0.0782 - val_out_recon_loss: 27.2634 - val_out_caps_acc: 0.8883\n",
            "Epoch 7/100\n",
            "12896/12896 [==============================] - 10s 764us/step - loss: 0.0843 - out_caps_loss: 0.0706 - out_recon_loss: 27.3849 - out_caps_acc: 0.9018 - val_loss: 0.0953 - val_out_caps_loss: 0.0816 - val_out_recon_loss: 27.2636 - val_out_caps_acc: 0.8831\n",
            "Epoch 8/100\n",
            "12896/12896 [==============================] - 10s 753us/step - loss: 0.0811 - out_caps_loss: 0.0674 - out_recon_loss: 27.3843 - out_caps_acc: 0.9071 - val_loss: 0.0836 - val_out_caps_loss: 0.0700 - val_out_recon_loss: 27.2625 - val_out_caps_acc: 0.8992\n",
            "Epoch 9/100\n",
            "12896/12896 [==============================] - 10s 760us/step - loss: 0.0788 - out_caps_loss: 0.0651 - out_recon_loss: 27.3827 - out_caps_acc: 0.9093 - val_loss: 0.0835 - val_out_caps_loss: 0.0699 - val_out_recon_loss: 27.2618 - val_out_caps_acc: 0.9001\n",
            "Epoch 10/100\n",
            "12896/12896 [==============================] - 10s 756us/step - loss: 0.0749 - out_caps_loss: 0.0613 - out_recon_loss: 27.3824 - out_caps_acc: 0.9161 - val_loss: 0.0887 - val_out_caps_loss: 0.0750 - val_out_recon_loss: 27.2613 - val_out_caps_acc: 0.8911\n",
            "Epoch 11/100\n",
            "12896/12896 [==============================] - 10s 760us/step - loss: 0.0737 - out_caps_loss: 0.0600 - out_recon_loss: 27.3819 - out_caps_acc: 0.9164 - val_loss: 0.0806 - val_out_caps_loss: 0.0670 - val_out_recon_loss: 27.2612 - val_out_caps_acc: 0.9020\n",
            "Epoch 12/100\n",
            "12896/12896 [==============================] - 10s 763us/step - loss: 0.0708 - out_caps_loss: 0.0572 - out_recon_loss: 27.3815 - out_caps_acc: 0.9220 - val_loss: 0.0895 - val_out_caps_loss: 0.0758 - val_out_recon_loss: 27.2609 - val_out_caps_acc: 0.8939\n",
            "Epoch 13/100\n",
            "12896/12896 [==============================] - 10s 772us/step - loss: 0.0682 - out_caps_loss: 0.0545 - out_recon_loss: 27.3813 - out_caps_acc: 0.9246 - val_loss: 0.0756 - val_out_caps_loss: 0.0619 - val_out_recon_loss: 27.2610 - val_out_caps_acc: 0.9110\n",
            "Epoch 14/100\n",
            "12896/12896 [==============================] - 10s 805us/step - loss: 0.0669 - out_caps_loss: 0.0532 - out_recon_loss: 27.3813 - out_caps_acc: 0.9297 - val_loss: 0.0736 - val_out_caps_loss: 0.0599 - val_out_recon_loss: 27.2606 - val_out_caps_acc: 0.9178\n",
            "Epoch 15/100\n",
            "12896/12896 [==============================] - 10s 758us/step - loss: 0.0651 - out_caps_loss: 0.0514 - out_recon_loss: 27.3812 - out_caps_acc: 0.9296 - val_loss: 0.0782 - val_out_caps_loss: 0.0645 - val_out_recon_loss: 27.2605 - val_out_caps_acc: 0.9069\n",
            "Epoch 16/100\n",
            "12896/12896 [==============================] - 10s 754us/step - loss: 0.0628 - out_caps_loss: 0.0491 - out_recon_loss: 27.3808 - out_caps_acc: 0.9342 - val_loss: 0.0833 - val_out_caps_loss: 0.0697 - val_out_recon_loss: 27.2603 - val_out_caps_acc: 0.9042\n",
            "Epoch 17/100\n",
            "12896/12896 [==============================] - 10s 760us/step - loss: 0.0614 - out_caps_loss: 0.0477 - out_recon_loss: 27.3804 - out_caps_acc: 0.9366 - val_loss: 0.0795 - val_out_caps_loss: 0.0659 - val_out_recon_loss: 27.2606 - val_out_caps_acc: 0.9094\n",
            "Epoch 18/100\n",
            "12896/12896 [==============================] - 10s 764us/step - loss: 0.0604 - out_caps_loss: 0.0467 - out_recon_loss: 27.3805 - out_caps_acc: 0.9396 - val_loss: 0.0744 - val_out_caps_loss: 0.0608 - val_out_recon_loss: 27.2600 - val_out_caps_acc: 0.9113\n",
            "Epoch 19/100\n",
            "12896/12896 [==============================] - 10s 754us/step - loss: 0.0581 - out_caps_loss: 0.0444 - out_recon_loss: 27.3803 - out_caps_acc: 0.9432 - val_loss: 0.0784 - val_out_caps_loss: 0.0648 - val_out_recon_loss: 27.2606 - val_out_caps_acc: 0.9079\n",
            "Epoch 20/100\n",
            "12896/12896 [==============================] - 10s 756us/step - loss: 0.0565 - out_caps_loss: 0.0428 - out_recon_loss: 27.3802 - out_caps_acc: 0.9435 - val_loss: 0.0776 - val_out_caps_loss: 0.0640 - val_out_recon_loss: 27.2602 - val_out_caps_acc: 0.9073\n",
            "Epoch 21/100\n",
            "12896/12896 [==============================] - 10s 765us/step - loss: 0.0557 - out_caps_loss: 0.0420 - out_recon_loss: 27.3802 - out_caps_acc: 0.9449 - val_loss: 0.0742 - val_out_caps_loss: 0.0606 - val_out_recon_loss: 27.2603 - val_out_caps_acc: 0.9104\n",
            "Epoch 22/100\n",
            "12896/12896 [==============================] - 10s 810us/step - loss: 0.0550 - out_caps_loss: 0.0413 - out_recon_loss: 27.3801 - out_caps_acc: 0.9473 - val_loss: 0.0689 - val_out_caps_loss: 0.0553 - val_out_recon_loss: 27.2604 - val_out_caps_acc: 0.9262\n",
            "Epoch 23/100\n",
            "12896/12896 [==============================] - 10s 755us/step - loss: 0.0527 - out_caps_loss: 0.0390 - out_recon_loss: 27.3800 - out_caps_acc: 0.9489 - val_loss: 0.0694 - val_out_caps_loss: 0.0558 - val_out_recon_loss: 27.2599 - val_out_caps_acc: 0.9203\n",
            "Epoch 24/100\n",
            "12896/12896 [==============================] - 10s 753us/step - loss: 0.0523 - out_caps_loss: 0.0386 - out_recon_loss: 27.3800 - out_caps_acc: 0.9499 - val_loss: 0.0696 - val_out_caps_loss: 0.0560 - val_out_recon_loss: 27.2600 - val_out_caps_acc: 0.9206\n",
            "Epoch 25/100\n",
            "12896/12896 [==============================] - 10s 751us/step - loss: 0.0506 - out_caps_loss: 0.0369 - out_recon_loss: 27.3799 - out_caps_acc: 0.9522 - val_loss: 0.0668 - val_out_caps_loss: 0.0531 - val_out_recon_loss: 27.2598 - val_out_caps_acc: 0.9259\n",
            "Epoch 26/100\n",
            "12896/12896 [==============================] - 10s 759us/step - loss: 0.0497 - out_caps_loss: 0.0360 - out_recon_loss: 27.3798 - out_caps_acc: 0.9536 - val_loss: 0.0683 - val_out_caps_loss: 0.0547 - val_out_recon_loss: 27.2598 - val_out_caps_acc: 0.9265\n",
            "Epoch 27/100\n",
            "12896/12896 [==============================] - 10s 745us/step - loss: 0.0489 - out_caps_loss: 0.0353 - out_recon_loss: 27.3798 - out_caps_acc: 0.9551 - val_loss: 0.0820 - val_out_caps_loss: 0.0684 - val_out_recon_loss: 27.2608 - val_out_caps_acc: 0.8995\n",
            "Epoch 28/100\n",
            "12896/12896 [==============================] - 9s 734us/step - loss: 0.0474 - out_caps_loss: 0.0337 - out_recon_loss: 27.3799 - out_caps_acc: 0.9570 - val_loss: 0.0707 - val_out_caps_loss: 0.0571 - val_out_recon_loss: 27.2598 - val_out_caps_acc: 0.9200\n",
            "Epoch 29/100\n",
            "12896/12896 [==============================] - 10s 781us/step - loss: 0.0472 - out_caps_loss: 0.0336 - out_recon_loss: 27.3800 - out_caps_acc: 0.9574 - val_loss: 0.0656 - val_out_caps_loss: 0.0520 - val_out_recon_loss: 27.2600 - val_out_caps_acc: 0.9311\n",
            "Epoch 30/100\n",
            "12896/12896 [==============================] - 10s 809us/step - loss: 0.0460 - out_caps_loss: 0.0323 - out_recon_loss: 27.3797 - out_caps_acc: 0.9592 - val_loss: 0.0667 - val_out_caps_loss: 0.0530 - val_out_recon_loss: 27.2600 - val_out_caps_acc: 0.9237\n",
            "Epoch 31/100\n",
            "12896/12896 [==============================] - 11s 830us/step - loss: 0.0444 - out_caps_loss: 0.0307 - out_recon_loss: 27.3800 - out_caps_acc: 0.9625 - val_loss: 0.0642 - val_out_caps_loss: 0.0506 - val_out_recon_loss: 27.2600 - val_out_caps_acc: 0.9318\n",
            "Epoch 32/100\n",
            "12896/12896 [==============================] - 10s 765us/step - loss: 0.0447 - out_caps_loss: 0.0310 - out_recon_loss: 27.3800 - out_caps_acc: 0.9621 - val_loss: 0.0652 - val_out_caps_loss: 0.0516 - val_out_recon_loss: 27.2604 - val_out_caps_acc: 0.9296\n",
            "Epoch 33/100\n",
            "12896/12896 [==============================] - 10s 748us/step - loss: 0.0435 - out_caps_loss: 0.0298 - out_recon_loss: 27.3800 - out_caps_acc: 0.9643 - val_loss: 0.0680 - val_out_caps_loss: 0.0544 - val_out_recon_loss: 27.2601 - val_out_caps_acc: 0.9237\n",
            "Epoch 34/100\n",
            "12896/12896 [==============================] - 10s 738us/step - loss: 0.0428 - out_caps_loss: 0.0291 - out_recon_loss: 27.3803 - out_caps_acc: 0.9643 - val_loss: 0.0638 - val_out_caps_loss: 0.0502 - val_out_recon_loss: 27.2600 - val_out_caps_acc: 0.9302\n",
            "Epoch 35/100\n",
            "12896/12896 [==============================] - 10s 748us/step - loss: 0.0419 - out_caps_loss: 0.0282 - out_recon_loss: 27.3802 - out_caps_acc: 0.9662 - val_loss: 0.0622 - val_out_caps_loss: 0.0486 - val_out_recon_loss: 27.2600 - val_out_caps_acc: 0.9339\n",
            "Epoch 36/100\n",
            "12896/12896 [==============================] - 10s 750us/step - loss: 0.0413 - out_caps_loss: 0.0276 - out_recon_loss: 27.3802 - out_caps_acc: 0.9685 - val_loss: 0.0640 - val_out_caps_loss: 0.0504 - val_out_recon_loss: 27.2604 - val_out_caps_acc: 0.9361\n",
            "Epoch 37/100\n",
            "12896/12896 [==============================] - 10s 746us/step - loss: 0.0411 - out_caps_loss: 0.0274 - out_recon_loss: 27.3802 - out_caps_acc: 0.9677 - val_loss: 0.0618 - val_out_caps_loss: 0.0482 - val_out_recon_loss: 27.2600 - val_out_caps_acc: 0.9342\n",
            "Epoch 38/100\n",
            "12896/12896 [==============================] - 10s 803us/step - loss: 0.0392 - out_caps_loss: 0.0255 - out_recon_loss: 27.3801 - out_caps_acc: 0.9701 - val_loss: 0.0591 - val_out_caps_loss: 0.0455 - val_out_recon_loss: 27.2604 - val_out_caps_acc: 0.9367\n",
            "Epoch 39/100\n",
            "12896/12896 [==============================] - 10s 764us/step - loss: 0.0399 - out_caps_loss: 0.0262 - out_recon_loss: 27.3803 - out_caps_acc: 0.9703 - val_loss: 0.0692 - val_out_caps_loss: 0.0556 - val_out_recon_loss: 27.2605 - val_out_caps_acc: 0.9252\n",
            "Epoch 40/100\n",
            "12896/12896 [==============================] - 10s 742us/step - loss: 0.0384 - out_caps_loss: 0.0248 - out_recon_loss: 27.3804 - out_caps_acc: 0.9712 - val_loss: 0.0635 - val_out_caps_loss: 0.0498 - val_out_recon_loss: 27.2608 - val_out_caps_acc: 0.9271\n",
            "Epoch 41/100\n",
            "12896/12896 [==============================] - 10s 740us/step - loss: 0.0371 - out_caps_loss: 0.0234 - out_recon_loss: 27.3806 - out_caps_acc: 0.9731 - val_loss: 0.0619 - val_out_caps_loss: 0.0482 - val_out_recon_loss: 27.2605 - val_out_caps_acc: 0.9370\n",
            "Epoch 42/100\n",
            "12896/12896 [==============================] - 10s 741us/step - loss: 0.0381 - out_caps_loss: 0.0244 - out_recon_loss: 27.3805 - out_caps_acc: 0.9723 - val_loss: 0.0645 - val_out_caps_loss: 0.0509 - val_out_recon_loss: 27.2609 - val_out_caps_acc: 0.9333\n",
            "Epoch 43/100\n",
            "12896/12896 [==============================] - 10s 744us/step - loss: 0.0366 - out_caps_loss: 0.0229 - out_recon_loss: 27.3804 - out_caps_acc: 0.9739 - val_loss: 0.0651 - val_out_caps_loss: 0.0515 - val_out_recon_loss: 27.2607 - val_out_caps_acc: 0.9256\n",
            "Epoch 44/100\n",
            "12896/12896 [==============================] - 10s 737us/step - loss: 0.0363 - out_caps_loss: 0.0226 - out_recon_loss: 27.3803 - out_caps_acc: 0.9751 - val_loss: 0.0618 - val_out_caps_loss: 0.0481 - val_out_recon_loss: 27.2600 - val_out_caps_acc: 0.9367\n",
            "Epoch 45/100\n",
            "12896/12896 [==============================] - 10s 748us/step - loss: 0.0355 - out_caps_loss: 0.0218 - out_recon_loss: 27.3805 - out_caps_acc: 0.9767 - val_loss: 0.0607 - val_out_caps_loss: 0.0471 - val_out_recon_loss: 27.2604 - val_out_caps_acc: 0.9386\n",
            "Epoch 46/100\n",
            "12896/12896 [==============================] - 10s 787us/step - loss: 0.0357 - out_caps_loss: 0.0220 - out_recon_loss: 27.3805 - out_caps_acc: 0.9761 - val_loss: 0.0601 - val_out_caps_loss: 0.0464 - val_out_recon_loss: 27.2603 - val_out_caps_acc: 0.9383\n",
            "Epoch 47/100\n",
            "12896/12896 [==============================] - 10s 775us/step - loss: 0.0347 - out_caps_loss: 0.0210 - out_recon_loss: 27.3805 - out_caps_acc: 0.9768 - val_loss: 0.0608 - val_out_caps_loss: 0.0472 - val_out_recon_loss: 27.2602 - val_out_caps_acc: 0.9339\n",
            "Epoch 48/100\n",
            "12896/12896 [==============================] - 10s 739us/step - loss: 0.0344 - out_caps_loss: 0.0207 - out_recon_loss: 27.3804 - out_caps_acc: 0.9770 - val_loss: 0.0607 - val_out_caps_loss: 0.0471 - val_out_recon_loss: 27.2603 - val_out_caps_acc: 0.9342\n",
            "Epoch 49/100\n",
            "12896/12896 [==============================] - 10s 737us/step - loss: 0.0337 - out_caps_loss: 0.0200 - out_recon_loss: 27.3805 - out_caps_acc: 0.9775 - val_loss: 0.0558 - val_out_caps_loss: 0.0422 - val_out_recon_loss: 27.2603 - val_out_caps_acc: 0.9442\n",
            "Epoch 50/100\n",
            "12896/12896 [==============================] - 10s 741us/step - loss: 0.0329 - out_caps_loss: 0.0192 - out_recon_loss: 27.3804 - out_caps_acc: 0.9796 - val_loss: 0.0581 - val_out_caps_loss: 0.0444 - val_out_recon_loss: 27.2608 - val_out_caps_acc: 0.9411\n",
            "Epoch 51/100\n",
            "12896/12896 [==============================] - 10s 746us/step - loss: 0.0324 - out_caps_loss: 0.0187 - out_recon_loss: 27.3805 - out_caps_acc: 0.9804 - val_loss: 0.0595 - val_out_caps_loss: 0.0459 - val_out_recon_loss: 27.2610 - val_out_caps_acc: 0.9389\n",
            "Epoch 52/100\n",
            "12896/12896 [==============================] - 10s 739us/step - loss: 0.0316 - out_caps_loss: 0.0179 - out_recon_loss: 27.3805 - out_caps_acc: 0.9808 - val_loss: 0.0602 - val_out_caps_loss: 0.0466 - val_out_recon_loss: 27.2602 - val_out_caps_acc: 0.9342\n",
            "Epoch 53/100\n",
            "12896/12896 [==============================] - 10s 738us/step - loss: 0.0319 - out_caps_loss: 0.0182 - out_recon_loss: 27.3802 - out_caps_acc: 0.9812 - val_loss: 0.0679 - val_out_caps_loss: 0.0543 - val_out_recon_loss: 27.2605 - val_out_caps_acc: 0.9262\n",
            "Epoch 54/100\n",
            "12896/12896 [==============================] - 10s 746us/step - loss: 0.0325 - out_caps_loss: 0.0188 - out_recon_loss: 27.3803 - out_caps_acc: 0.9802 - val_loss: 0.0753 - val_out_caps_loss: 0.0616 - val_out_recon_loss: 27.2605 - val_out_caps_acc: 0.9091\n",
            "Epoch 55/100\n",
            "12896/12896 [==============================] - 10s 800us/step - loss: 0.0311 - out_caps_loss: 0.0175 - out_recon_loss: 27.3805 - out_caps_acc: 0.9818 - val_loss: 0.0584 - val_out_caps_loss: 0.0448 - val_out_recon_loss: 27.2602 - val_out_caps_acc: 0.9367\n",
            "Epoch 56/100\n",
            "12896/12896 [==============================] - 9s 731us/step - loss: 0.0305 - out_caps_loss: 0.0168 - out_recon_loss: 27.3805 - out_caps_acc: 0.9819 - val_loss: 0.0614 - val_out_caps_loss: 0.0477 - val_out_recon_loss: 27.2602 - val_out_caps_acc: 0.9330\n",
            "Epoch 57/100\n",
            "12896/12896 [==============================] - 9s 734us/step - loss: 0.0303 - out_caps_loss: 0.0166 - out_recon_loss: 27.3804 - out_caps_acc: 0.9837 - val_loss: 0.0673 - val_out_caps_loss: 0.0537 - val_out_recon_loss: 27.2604 - val_out_caps_acc: 0.9228\n",
            "Epoch 58/100\n",
            "12896/12896 [==============================] - 9s 735us/step - loss: 0.0300 - out_caps_loss: 0.0163 - out_recon_loss: 27.3804 - out_caps_acc: 0.9819 - val_loss: 0.0592 - val_out_caps_loss: 0.0456 - val_out_recon_loss: 27.2604 - val_out_caps_acc: 0.9380\n",
            "Epoch 59/100\n",
            "12896/12896 [==============================] - 9s 729us/step - loss: 0.0291 - out_caps_loss: 0.0154 - out_recon_loss: 27.3805 - out_caps_acc: 0.9842 - val_loss: 0.0663 - val_out_caps_loss: 0.0527 - val_out_recon_loss: 27.2604 - val_out_caps_acc: 0.9243\n",
            "Epoch 60/100\n",
            "12896/12896 [==============================] - 10s 782us/step - loss: 0.0297 - out_caps_loss: 0.0160 - out_recon_loss: 27.3805 - out_caps_acc: 0.9837 - val_loss: 0.0555 - val_out_caps_loss: 0.0418 - val_out_recon_loss: 27.2604 - val_out_caps_acc: 0.9389\n",
            "Epoch 61/100\n",
            "12896/12896 [==============================] - 9s 733us/step - loss: 0.0294 - out_caps_loss: 0.0157 - out_recon_loss: 27.3804 - out_caps_acc: 0.9833 - val_loss: 0.0639 - val_out_caps_loss: 0.0502 - val_out_recon_loss: 27.2600 - val_out_caps_acc: 0.9287\n",
            "Epoch 62/100\n",
            "12896/12896 [==============================] - 9s 732us/step - loss: 0.0290 - out_caps_loss: 0.0154 - out_recon_loss: 27.3802 - out_caps_acc: 0.9846 - val_loss: 0.0594 - val_out_caps_loss: 0.0458 - val_out_recon_loss: 27.2601 - val_out_caps_acc: 0.9364\n",
            "Epoch 63/100\n",
            "12896/12896 [==============================] - 11s 824us/step - loss: 0.0278 - out_caps_loss: 0.0141 - out_recon_loss: 27.3803 - out_caps_acc: 0.9846 - val_loss: 0.0556 - val_out_caps_loss: 0.0420 - val_out_recon_loss: 27.2600 - val_out_caps_acc: 0.9439\n",
            "Epoch 64/100\n",
            "12896/12896 [==============================] - 10s 756us/step - loss: 0.0283 - out_caps_loss: 0.0146 - out_recon_loss: 27.3803 - out_caps_acc: 0.9845 - val_loss: 0.0676 - val_out_caps_loss: 0.0540 - val_out_recon_loss: 27.2598 - val_out_caps_acc: 0.9243\n",
            "Epoch 65/100\n",
            "12896/12896 [==============================] - 9s 728us/step - loss: 0.0281 - out_caps_loss: 0.0144 - out_recon_loss: 27.3801 - out_caps_acc: 0.9849 - val_loss: 0.0597 - val_out_caps_loss: 0.0461 - val_out_recon_loss: 27.2599 - val_out_caps_acc: 0.9373\n",
            "Epoch 66/100\n",
            "12896/12896 [==============================] - 9s 725us/step - loss: 0.0263 - out_caps_loss: 0.0126 - out_recon_loss: 27.3801 - out_caps_acc: 0.9868 - val_loss: 0.0537 - val_out_caps_loss: 0.0401 - val_out_recon_loss: 27.2600 - val_out_caps_acc: 0.9473\n",
            "Epoch 67/100\n",
            "12896/12896 [==============================] - 9s 723us/step - loss: 0.0280 - out_caps_loss: 0.0143 - out_recon_loss: 27.3801 - out_caps_acc: 0.9850 - val_loss: 0.0603 - val_out_caps_loss: 0.0467 - val_out_recon_loss: 27.2598 - val_out_caps_acc: 0.9336\n",
            "Epoch 68/100\n",
            "12896/12896 [==============================] - 9s 728us/step - loss: 0.0278 - out_caps_loss: 0.0141 - out_recon_loss: 27.3801 - out_caps_acc: 0.9851 - val_loss: 0.0670 - val_out_caps_loss: 0.0533 - val_out_recon_loss: 27.2601 - val_out_caps_acc: 0.9228\n",
            "Epoch 69/100\n",
            "12896/12896 [==============================] - 9s 723us/step - loss: 0.0263 - out_caps_loss: 0.0126 - out_recon_loss: 27.3800 - out_caps_acc: 0.9875 - val_loss: 0.0568 - val_out_caps_loss: 0.0432 - val_out_recon_loss: 27.2602 - val_out_caps_acc: 0.9417\n",
            "Epoch 70/100\n",
            "12896/12896 [==============================] - 10s 748us/step - loss: 0.0262 - out_caps_loss: 0.0125 - out_recon_loss: 27.3801 - out_caps_acc: 0.9874 - val_loss: 0.0626 - val_out_caps_loss: 0.0489 - val_out_recon_loss: 27.2602 - val_out_caps_acc: 0.9318\n",
            "Epoch 71/100\n",
            "12896/12896 [==============================] - 10s 782us/step - loss: 0.0264 - out_caps_loss: 0.0128 - out_recon_loss: 27.3798 - out_caps_acc: 0.9877 - val_loss: 0.0581 - val_out_caps_loss: 0.0445 - val_out_recon_loss: 27.2600 - val_out_caps_acc: 0.9423\n",
            "Epoch 72/100\n",
            "12896/12896 [==============================] - 10s 780us/step - loss: 0.0263 - out_caps_loss: 0.0126 - out_recon_loss: 27.3799 - out_caps_acc: 0.9869 - val_loss: 0.0660 - val_out_caps_loss: 0.0524 - val_out_recon_loss: 27.2603 - val_out_caps_acc: 0.9268\n",
            "Epoch 73/100\n",
            "12896/12896 [==============================] - 9s 730us/step - loss: 0.0260 - out_caps_loss: 0.0123 - out_recon_loss: 27.3799 - out_caps_acc: 0.9876 - val_loss: 0.0624 - val_out_caps_loss: 0.0488 - val_out_recon_loss: 27.2601 - val_out_caps_acc: 0.9318\n",
            "Epoch 74/100\n",
            "12896/12896 [==============================] - 10s 763us/step - loss: 0.0253 - out_caps_loss: 0.0116 - out_recon_loss: 27.3799 - out_caps_acc: 0.9877 - val_loss: 0.0593 - val_out_caps_loss: 0.0457 - val_out_recon_loss: 27.2599 - val_out_caps_acc: 0.9330\n",
            "Epoch 75/100\n",
            "12896/12896 [==============================] - 10s 763us/step - loss: 0.0256 - out_caps_loss: 0.0119 - out_recon_loss: 27.3798 - out_caps_acc: 0.9881 - val_loss: 0.0653 - val_out_caps_loss: 0.0517 - val_out_recon_loss: 27.2598 - val_out_caps_acc: 0.9246\n",
            "Epoch 76/100\n",
            "12896/12896 [==============================] - 10s 757us/step - loss: 0.0249 - out_caps_loss: 0.0112 - out_recon_loss: 27.3797 - out_caps_acc: 0.9885 - val_loss: 0.0567 - val_out_caps_loss: 0.0431 - val_out_recon_loss: 27.2597 - val_out_caps_acc: 0.9401\n",
            "Epoch 77/100\n",
            "12896/12896 [==============================] - 9s 735us/step - loss: 0.0248 - out_caps_loss: 0.0111 - out_recon_loss: 27.3796 - out_caps_acc: 0.9886 - val_loss: 0.0605 - val_out_caps_loss: 0.0469 - val_out_recon_loss: 27.2601 - val_out_caps_acc: 0.9370\n",
            "Epoch 78/100\n",
            "12896/12896 [==============================] - 9s 730us/step - loss: 0.0257 - out_caps_loss: 0.0120 - out_recon_loss: 27.3797 - out_caps_acc: 0.9883 - val_loss: 0.0591 - val_out_caps_loss: 0.0455 - val_out_recon_loss: 27.2599 - val_out_caps_acc: 0.9367\n",
            "Epoch 79/100\n",
            "12896/12896 [==============================] - 10s 749us/step - loss: 0.0245 - out_caps_loss: 0.0108 - out_recon_loss: 27.3796 - out_caps_acc: 0.9889 - val_loss: 0.0587 - val_out_caps_loss: 0.0451 - val_out_recon_loss: 27.2597 - val_out_caps_acc: 0.9349\n",
            "Epoch 80/100\n",
            "12896/12896 [==============================] - 10s 798us/step - loss: 0.0245 - out_caps_loss: 0.0108 - out_recon_loss: 27.3796 - out_caps_acc: 0.9886 - val_loss: 0.0551 - val_out_caps_loss: 0.0414 - val_out_recon_loss: 27.2600 - val_out_caps_acc: 0.9467\n",
            "Epoch 81/100\n",
            "12896/12896 [==============================] - 9s 736us/step - loss: 0.0232 - out_caps_loss: 0.0095 - out_recon_loss: 27.3795 - out_caps_acc: 0.9901 - val_loss: 0.0559 - val_out_caps_loss: 0.0423 - val_out_recon_loss: 27.2597 - val_out_caps_acc: 0.9420\n",
            "Trained model saved to 'trained_model.h5'\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.engine.training.Model at 0x7f3e68003518>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jRUm7e5z-gMK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Testing\n",
        "x_test_reshape3 = x_test3.reshape(4030, 20, 1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FCeR9NfzHmuj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "layer_name = 'out_caps'\n",
        "intermediate_layer_model3 = models.Model(inputs=model3.input[0],\n",
        "                                 outputs=model3.get_layer(layer_name).output)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nFgwcy7GIKaD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "y_pred3 = intermediate_layer_model3.predict(x_test_reshape3)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PVf9CuxRIPIL",
        "colab_type": "code",
        "outputId": "6b99e509-80af-47a2-d8ba-6816a5af6a68",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "cm3 = confusion_matrix(y_test3, np.argmax(y_pred3, axis=1))\n",
        "cm3"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[1938,   77],\n",
              "       [ 103, 1912]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9JM4cU8MIbxp",
        "colab_type": "text"
      },
      "source": [
        "# Training and Testing on Set 4"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mfFC1gamIT-D",
        "colab_type": "code",
        "outputId": "4efd15de-989a-447f-97d1-4c83c51d78b6",
        "colab": {
          "resources": {
            "http://localhost:8080/nbextensions/google.colab/files.js": {
              "data": "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",
              "ok": true,
              "headers": [
                [
                  "content-type",
                  "application/javascript"
                ]
              ],
              "status": 200,
              "status_text": ""
            }
          },
          "base_uri": "https://localhost:8080/",
          "height": 74
        }
      },
      "source": [
        "uploaded = files.upload()\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-12eb6db1-35a0-4dce-ba2b-194acac3e6d5\" name=\"files[]\" multiple disabled />\n",
              "     <output id=\"result-12eb6db1-35a0-4dce-ba2b-194acac3e6d5\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script src=\"/nbextensions/google.colab/files.js\"></script> "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Saving AComp4.csv to AComp4.csv\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P4ErUcXTIo5q",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df6 = pd.read_csv(io.BytesIO(uploaded['AComp4.csv']))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "90Ttg43TIsdU",
        "colab_type": "code",
        "outputId": "5dce013a-943e-4255-aefb-ccd416980822",
        "colab": {
          "resources": {
            "http://localhost:8080/nbextensions/google.colab/files.js": {
              "data": "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",
              "ok": true,
              "headers": [
                [
                  "content-type",
                  "application/javascript"
                ]
              ],
              "status": 200,
              "status_text": ""
            }
          },
          "base_uri": "https://localhost:8080/",
          "height": 74
        }
      },
      "source": [
        "uploaded = files.upload()\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-d1e19238-1c21-4971-86da-4e73780b7642\" name=\"files[]\" multiple disabled />\n",
              "     <output id=\"result-d1e19238-1c21-4971-86da-4e73780b7642\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script src=\"/nbextensions/google.colab/files.js\"></script> "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Saving test_AComp4.csv to test_AComp4.csv\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1JnZkLudIudL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df7 = pd.read_csv(io.BytesIO(uploaded['test_AComp4.csv']))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tdIrTfzrI4Pp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "x_train4 = df6.iloc[:, 0:20].values\n",
        "y_train4 = df6.iloc[:, 20].values\n",
        "x_test4 = df7.iloc[:, 0:20].values"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dQUqY8eOJ0A8",
        "colab_type": "code",
        "outputId": "0e3d3fff-28ec-48c9-a578-6157bdc7f2a8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "x_train4.shape, y_train4.shape, x_test4.shape"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((16120, 20), (16120,), (4030, 20))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 39
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9jzXlWkZJMkI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Reshaping the train and test set\n",
        "x_train_reshape4 = x_train4.reshape(16120, 20, 1)\n",
        "y_train_reshape4 = y_train4.reshape(16120, 1)\n",
        "x_test_reshape4 = x_test4.reshape(4030, 20, 1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zcVaVcq-Jv-i",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Reshaping the Labels\n",
        "y_train4_ = tf.keras.utils.to_categorical(y_train_reshape4,num_classes=2)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x959AfDwK8T7",
        "colab_type": "code",
        "outputId": "ee1ee982-8774-41cf-e050-0773d4323e04",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "model4 = CapsNet(input_shape=[20, 1],\n",
        "                n_class=2,\n",
        "                num_routing=3)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[20, 1] (?, 20, 1)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MV7xZeyUK34S",
        "colab_type": "code",
        "outputId": "0f48075c-15b2-4a39-9852-a0759cb5c314",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 3488
        }
      },
      "source": [
        "train(model=model4, data=((x_train_reshape4, y_train4_)), epoch_size_frac = 0.5)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 12896 samples, validate on 3224 samples\n",
            "Epoch 1/100\n",
            "12896/12896 [==============================] - 13s 994us/step - loss: 0.1539 - out_caps_loss: 0.1396 - out_recon_loss: 28.6590 - out_caps_acc: 0.7943 - val_loss: 0.1226 - val_out_caps_loss: 0.1090 - val_out_recon_loss: 27.2924 - val_out_caps_acc: 0.8421\n",
            "Epoch 2/100\n",
            "12896/12896 [==============================] - 11s 867us/step - loss: 0.1097 - out_caps_loss: 0.0960 - out_recon_loss: 27.4350 - out_caps_acc: 0.8576 - val_loss: 0.1102 - val_out_caps_loss: 0.0966 - val_out_recon_loss: 27.2685 - val_out_caps_acc: 0.8564\n",
            "Epoch 3/100\n",
            "12896/12896 [==============================] - 11s 831us/step - loss: 0.1031 - out_caps_loss: 0.0894 - out_recon_loss: 27.4219 - out_caps_acc: 0.8721 - val_loss: 0.1104 - val_out_caps_loss: 0.0968 - val_out_recon_loss: 27.2636 - val_out_caps_acc: 0.8583\n",
            "Epoch 4/100\n",
            "12896/12896 [==============================] - 11s 891us/step - loss: 0.0975 - out_caps_loss: 0.0838 - out_recon_loss: 27.4187 - out_caps_acc: 0.8807 - val_loss: 0.0998 - val_out_caps_loss: 0.0862 - val_out_recon_loss: 27.2621 - val_out_caps_acc: 0.8778\n",
            "Epoch 5/100\n",
            "12896/12896 [==============================] - 11s 861us/step - loss: 0.0929 - out_caps_loss: 0.0792 - out_recon_loss: 27.4170 - out_caps_acc: 0.8861 - val_loss: 0.1024 - val_out_caps_loss: 0.0888 - val_out_recon_loss: 27.2614 - val_out_caps_acc: 0.8700\n",
            "Epoch 6/100\n",
            "12896/12896 [==============================] - 11s 827us/step - loss: 0.0897 - out_caps_loss: 0.0760 - out_recon_loss: 27.4165 - out_caps_acc: 0.8931 - val_loss: 0.0922 - val_out_caps_loss: 0.0786 - val_out_recon_loss: 27.2606 - val_out_caps_acc: 0.8840\n",
            "Epoch 7/100\n",
            "12896/12896 [==============================] - 11s 824us/step - loss: 0.0863 - out_caps_loss: 0.0726 - out_recon_loss: 27.4159 - out_caps_acc: 0.8968 - val_loss: 0.0929 - val_out_caps_loss: 0.0793 - val_out_recon_loss: 27.2601 - val_out_caps_acc: 0.8846\n",
            "Epoch 8/100\n",
            "12896/12896 [==============================] - 11s 819us/step - loss: 0.0837 - out_caps_loss: 0.0700 - out_recon_loss: 27.4153 - out_caps_acc: 0.9000 - val_loss: 0.0892 - val_out_caps_loss: 0.0755 - val_out_recon_loss: 27.2599 - val_out_caps_acc: 0.8911\n",
            "Epoch 9/100\n",
            "12896/12896 [==============================] - 12s 940us/step - loss: 0.0820 - out_caps_loss: 0.0683 - out_recon_loss: 27.4150 - out_caps_acc: 0.9025 - val_loss: 0.0866 - val_out_caps_loss: 0.0730 - val_out_recon_loss: 27.2595 - val_out_caps_acc: 0.8927\n",
            "Epoch 10/100\n",
            "12896/12896 [==============================] - 11s 825us/step - loss: 0.0794 - out_caps_loss: 0.0657 - out_recon_loss: 27.4147 - out_caps_acc: 0.9064 - val_loss: 0.0831 - val_out_caps_loss: 0.0695 - val_out_recon_loss: 27.2592 - val_out_caps_acc: 0.8958\n",
            "Epoch 11/100\n",
            "12896/12896 [==============================] - 11s 849us/step - loss: 0.0778 - out_caps_loss: 0.0641 - out_recon_loss: 27.4148 - out_caps_acc: 0.9075 - val_loss: 0.0823 - val_out_caps_loss: 0.0687 - val_out_recon_loss: 27.2590 - val_out_caps_acc: 0.8917\n",
            "Epoch 12/100\n",
            "12896/12896 [==============================] - 11s 885us/step - loss: 0.0756 - out_caps_loss: 0.0619 - out_recon_loss: 27.4143 - out_caps_acc: 0.9112 - val_loss: 0.0845 - val_out_caps_loss: 0.0709 - val_out_recon_loss: 27.2590 - val_out_caps_acc: 0.8995\n",
            "Epoch 13/100\n",
            "12896/12896 [==============================] - 11s 820us/step - loss: 0.0740 - out_caps_loss: 0.0603 - out_recon_loss: 27.4140 - out_caps_acc: 0.9118 - val_loss: 0.0830 - val_out_caps_loss: 0.0694 - val_out_recon_loss: 27.2583 - val_out_caps_acc: 0.9001\n",
            "Epoch 14/100\n",
            "12896/12896 [==============================] - 11s 827us/step - loss: 0.0725 - out_caps_loss: 0.0588 - out_recon_loss: 27.4144 - out_caps_acc: 0.9159 - val_loss: 0.0895 - val_out_caps_loss: 0.0759 - val_out_recon_loss: 27.2583 - val_out_caps_acc: 0.8927\n",
            "Epoch 15/100\n",
            "12896/12896 [==============================] - 11s 826us/step - loss: 0.0709 - out_caps_loss: 0.0572 - out_recon_loss: 27.4141 - out_caps_acc: 0.9195 - val_loss: 0.0774 - val_out_caps_loss: 0.0638 - val_out_recon_loss: 27.2586 - val_out_caps_acc: 0.9085\n",
            "Epoch 16/100\n",
            "12896/12896 [==============================] - 11s 827us/step - loss: 0.0700 - out_caps_loss: 0.0563 - out_recon_loss: 27.4141 - out_caps_acc: 0.9204 - val_loss: 0.0811 - val_out_caps_loss: 0.0674 - val_out_recon_loss: 27.2585 - val_out_caps_acc: 0.9011\n",
            "Epoch 17/100\n",
            "12896/12896 [==============================] - 11s 824us/step - loss: 0.0680 - out_caps_loss: 0.0543 - out_recon_loss: 27.4142 - out_caps_acc: 0.9242 - val_loss: 0.0790 - val_out_caps_loss: 0.0653 - val_out_recon_loss: 27.2586 - val_out_caps_acc: 0.9051\n",
            "Epoch 18/100\n",
            "12896/12896 [==============================] - 11s 828us/step - loss: 0.0670 - out_caps_loss: 0.0533 - out_recon_loss: 27.4135 - out_caps_acc: 0.9255 - val_loss: 0.0865 - val_out_caps_loss: 0.0729 - val_out_recon_loss: 27.2580 - val_out_caps_acc: 0.8942\n",
            "Epoch 19/100\n",
            "12896/12896 [==============================] - 12s 916us/step - loss: 0.0665 - out_caps_loss: 0.0528 - out_recon_loss: 27.4140 - out_caps_acc: 0.9274 - val_loss: 0.0817 - val_out_caps_loss: 0.0681 - val_out_recon_loss: 27.2585 - val_out_caps_acc: 0.8970\n",
            "Epoch 20/100\n",
            "12896/12896 [==============================] - 11s 832us/step - loss: 0.0642 - out_caps_loss: 0.0505 - out_recon_loss: 27.4134 - out_caps_acc: 0.9294 - val_loss: 0.0766 - val_out_caps_loss: 0.0630 - val_out_recon_loss: 27.2583 - val_out_caps_acc: 0.9048\n",
            "Epoch 21/100\n",
            "12896/12896 [==============================] - 11s 829us/step - loss: 0.0638 - out_caps_loss: 0.0501 - out_recon_loss: 27.4134 - out_caps_acc: 0.9310 - val_loss: 0.0741 - val_out_caps_loss: 0.0604 - val_out_recon_loss: 27.2585 - val_out_caps_acc: 0.9119\n",
            "Epoch 22/100\n",
            "12896/12896 [==============================] - 11s 829us/step - loss: 0.0624 - out_caps_loss: 0.0487 - out_recon_loss: 27.4131 - out_caps_acc: 0.9332 - val_loss: 0.0767 - val_out_caps_loss: 0.0631 - val_out_recon_loss: 27.2586 - val_out_caps_acc: 0.9085\n",
            "Epoch 23/100\n",
            "12896/12896 [==============================] - 11s 830us/step - loss: 0.0613 - out_caps_loss: 0.0476 - out_recon_loss: 27.4129 - out_caps_acc: 0.9352 - val_loss: 0.0718 - val_out_caps_loss: 0.0582 - val_out_recon_loss: 27.2570 - val_out_caps_acc: 0.9153\n",
            "Epoch 24/100\n",
            "12896/12896 [==============================] - 11s 827us/step - loss: 0.0616 - out_caps_loss: 0.0479 - out_recon_loss: 27.4127 - out_caps_acc: 0.9346 - val_loss: 0.0729 - val_out_caps_loss: 0.0592 - val_out_recon_loss: 27.2583 - val_out_caps_acc: 0.9125\n",
            "Epoch 25/100\n",
            "12896/12896 [==============================] - 11s 825us/step - loss: 0.0604 - out_caps_loss: 0.0467 - out_recon_loss: 27.4127 - out_caps_acc: 0.9344 - val_loss: 0.0709 - val_out_caps_loss: 0.0573 - val_out_recon_loss: 27.2568 - val_out_caps_acc: 0.9166\n",
            "Epoch 26/100\n",
            "12896/12896 [==============================] - 11s 870us/step - loss: 0.0587 - out_caps_loss: 0.0450 - out_recon_loss: 27.4119 - out_caps_acc: 0.9392 - val_loss: 0.0775 - val_out_caps_loss: 0.0639 - val_out_recon_loss: 27.2581 - val_out_caps_acc: 0.9128\n",
            "Epoch 27/100\n",
            "12896/12896 [==============================] - 11s 877us/step - loss: 0.0580 - out_caps_loss: 0.0443 - out_recon_loss: 27.4118 - out_caps_acc: 0.9404 - val_loss: 0.0697 - val_out_caps_loss: 0.0561 - val_out_recon_loss: 27.2576 - val_out_caps_acc: 0.9305\n",
            "Epoch 28/100\n",
            "12896/12896 [==============================] - 11s 825us/step - loss: 0.0587 - out_caps_loss: 0.0450 - out_recon_loss: 27.4120 - out_caps_acc: 0.9397 - val_loss: 0.0723 - val_out_caps_loss: 0.0587 - val_out_recon_loss: 27.2567 - val_out_caps_acc: 0.9187\n",
            "Epoch 29/100\n",
            "12896/12896 [==============================] - 11s 852us/step - loss: 0.0574 - out_caps_loss: 0.0437 - out_recon_loss: 27.4116 - out_caps_acc: 0.9387 - val_loss: 0.0746 - val_out_caps_loss: 0.0610 - val_out_recon_loss: 27.2565 - val_out_caps_acc: 0.9085\n",
            "Epoch 30/100\n",
            "12896/12896 [==============================] - 11s 854us/step - loss: 0.0559 - out_caps_loss: 0.0422 - out_recon_loss: 27.4114 - out_caps_acc: 0.9442 - val_loss: 0.0744 - val_out_caps_loss: 0.0608 - val_out_recon_loss: 27.2563 - val_out_caps_acc: 0.9110\n",
            "Epoch 31/100\n",
            "12896/12896 [==============================] - 11s 826us/step - loss: 0.0562 - out_caps_loss: 0.0425 - out_recon_loss: 27.4115 - out_caps_acc: 0.9440 - val_loss: 0.0842 - val_out_caps_loss: 0.0705 - val_out_recon_loss: 27.2562 - val_out_caps_acc: 0.8955\n",
            "Epoch 32/100\n",
            "12896/12896 [==============================] - 11s 819us/step - loss: 0.0535 - out_caps_loss: 0.0398 - out_recon_loss: 27.4111 - out_caps_acc: 0.9465 - val_loss: 0.0714 - val_out_caps_loss: 0.0577 - val_out_recon_loss: 27.2559 - val_out_caps_acc: 0.9184\n",
            "Epoch 33/100\n",
            "12896/12896 [==============================] - 11s 820us/step - loss: 0.0535 - out_caps_loss: 0.0398 - out_recon_loss: 27.4111 - out_caps_acc: 0.9467 - val_loss: 0.0702 - val_out_caps_loss: 0.0565 - val_out_recon_loss: 27.2559 - val_out_caps_acc: 0.9231\n",
            "Epoch 34/100\n",
            "12896/12896 [==============================] - 12s 916us/step - loss: 0.0525 - out_caps_loss: 0.0388 - out_recon_loss: 27.4111 - out_caps_acc: 0.9487 - val_loss: 0.0724 - val_out_caps_loss: 0.0587 - val_out_recon_loss: 27.2567 - val_out_caps_acc: 0.9206\n",
            "Epoch 35/100\n",
            "12896/12896 [==============================] - 11s 825us/step - loss: 0.0519 - out_caps_loss: 0.0382 - out_recon_loss: 27.4107 - out_caps_acc: 0.9490 - val_loss: 0.0728 - val_out_caps_loss: 0.0592 - val_out_recon_loss: 27.2566 - val_out_caps_acc: 0.9163\n",
            "Epoch 36/100\n",
            "12896/12896 [==============================] - 11s 826us/step - loss: 0.0512 - out_caps_loss: 0.0375 - out_recon_loss: 27.4113 - out_caps_acc: 0.9524 - val_loss: 0.0736 - val_out_caps_loss: 0.0600 - val_out_recon_loss: 27.2558 - val_out_caps_acc: 0.9122\n",
            "Epoch 37/100\n",
            "12896/12896 [==============================] - 11s 885us/step - loss: 0.0516 - out_caps_loss: 0.0379 - out_recon_loss: 27.4109 - out_caps_acc: 0.9499 - val_loss: 0.0714 - val_out_caps_loss: 0.0577 - val_out_recon_loss: 27.2560 - val_out_caps_acc: 0.9206\n",
            "Epoch 38/100\n",
            "12896/12896 [==============================] - 11s 881us/step - loss: 0.0508 - out_caps_loss: 0.0371 - out_recon_loss: 27.4104 - out_caps_acc: 0.9523 - val_loss: 0.0764 - val_out_caps_loss: 0.0628 - val_out_recon_loss: 27.2566 - val_out_caps_acc: 0.9097\n",
            "Epoch 39/100\n",
            "12896/12896 [==============================] - 11s 818us/step - loss: 0.0517 - out_caps_loss: 0.0380 - out_recon_loss: 27.4107 - out_caps_acc: 0.9498 - val_loss: 0.0693 - val_out_caps_loss: 0.0557 - val_out_recon_loss: 27.2551 - val_out_caps_acc: 0.9212\n",
            "Epoch 40/100\n",
            "12896/12896 [==============================] - 11s 826us/step - loss: 0.0489 - out_caps_loss: 0.0352 - out_recon_loss: 27.4107 - out_caps_acc: 0.9541 - val_loss: 0.0627 - val_out_caps_loss: 0.0491 - val_out_recon_loss: 27.2558 - val_out_caps_acc: 0.9321\n",
            "Epoch 41/100\n",
            "12896/12896 [==============================] - 12s 907us/step - loss: 0.0489 - out_caps_loss: 0.0352 - out_recon_loss: 27.4105 - out_caps_acc: 0.9541 - val_loss: 0.0632 - val_out_caps_loss: 0.0496 - val_out_recon_loss: 27.2565 - val_out_caps_acc: 0.9355\n",
            "Epoch 42/100\n",
            "12896/12896 [==============================] - 11s 845us/step - loss: 0.0483 - out_caps_loss: 0.0346 - out_recon_loss: 27.4108 - out_caps_acc: 0.9554 - val_loss: 0.0638 - val_out_caps_loss: 0.0502 - val_out_recon_loss: 27.2564 - val_out_caps_acc: 0.9308\n",
            "Epoch 43/100\n",
            "12896/12896 [==============================] - 11s 832us/step - loss: 0.0484 - out_caps_loss: 0.0347 - out_recon_loss: 27.4105 - out_caps_acc: 0.9544 - val_loss: 0.0800 - val_out_caps_loss: 0.0664 - val_out_recon_loss: 27.2565 - val_out_caps_acc: 0.9011\n",
            "Epoch 44/100\n",
            "12896/12896 [==============================] - 11s 834us/step - loss: 0.0476 - out_caps_loss: 0.0339 - out_recon_loss: 27.4103 - out_caps_acc: 0.9566 - val_loss: 0.0660 - val_out_caps_loss: 0.0524 - val_out_recon_loss: 27.2563 - val_out_caps_acc: 0.9262\n",
            "Epoch 45/100\n",
            "12896/12896 [==============================] - 11s 831us/step - loss: 0.0468 - out_caps_loss: 0.0331 - out_recon_loss: 27.4104 - out_caps_acc: 0.9579 - val_loss: 0.0799 - val_out_caps_loss: 0.0663 - val_out_recon_loss: 27.2565 - val_out_caps_acc: 0.9073\n",
            "Epoch 46/100\n",
            "12896/12896 [==============================] - 11s 836us/step - loss: 0.0472 - out_caps_loss: 0.0335 - out_recon_loss: 27.4104 - out_caps_acc: 0.9564 - val_loss: 0.0648 - val_out_caps_loss: 0.0511 - val_out_recon_loss: 27.2574 - val_out_caps_acc: 0.9271\n",
            "Epoch 47/100\n",
            "12896/12896 [==============================] - 11s 828us/step - loss: 0.0454 - out_caps_loss: 0.0317 - out_recon_loss: 27.4104 - out_caps_acc: 0.9608 - val_loss: 0.0659 - val_out_caps_loss: 0.0523 - val_out_recon_loss: 27.2551 - val_out_caps_acc: 0.9283\n",
            "Epoch 48/100\n",
            "12896/12896 [==============================] - 11s 856us/step - loss: 0.0464 - out_caps_loss: 0.0327 - out_recon_loss: 27.4100 - out_caps_acc: 0.9578 - val_loss: 0.0727 - val_out_caps_loss: 0.0590 - val_out_recon_loss: 27.2561 - val_out_caps_acc: 0.9153\n",
            "Epoch 49/100\n",
            "12896/12896 [==============================] - 11s 884us/step - loss: 0.0454 - out_caps_loss: 0.0317 - out_recon_loss: 27.4102 - out_caps_acc: 0.9599 - val_loss: 0.0695 - val_out_caps_loss: 0.0559 - val_out_recon_loss: 27.2554 - val_out_caps_acc: 0.9212\n",
            "Epoch 50/100\n",
            "12896/12896 [==============================] - 11s 826us/step - loss: 0.0448 - out_caps_loss: 0.0311 - out_recon_loss: 27.4104 - out_caps_acc: 0.9594 - val_loss: 0.0720 - val_out_caps_loss: 0.0584 - val_out_recon_loss: 27.2556 - val_out_caps_acc: 0.9147\n",
            "Epoch 51/100\n",
            "12896/12896 [==============================] - 11s 826us/step - loss: 0.0450 - out_caps_loss: 0.0313 - out_recon_loss: 27.4103 - out_caps_acc: 0.9602 - val_loss: 0.0743 - val_out_caps_loss: 0.0606 - val_out_recon_loss: 27.2549 - val_out_caps_acc: 0.9166\n",
            "Epoch 52/100\n",
            "12896/12896 [==============================] - 11s 825us/step - loss: 0.0447 - out_caps_loss: 0.0310 - out_recon_loss: 27.4101 - out_caps_acc: 0.9607 - val_loss: 0.0573 - val_out_caps_loss: 0.0437 - val_out_recon_loss: 27.2556 - val_out_caps_acc: 0.9395\n",
            "Epoch 53/100\n",
            "12896/12896 [==============================] - 11s 827us/step - loss: 0.0449 - out_caps_loss: 0.0312 - out_recon_loss: 27.4103 - out_caps_acc: 0.9605 - val_loss: 0.0610 - val_out_caps_loss: 0.0474 - val_out_recon_loss: 27.2560 - val_out_caps_acc: 0.9336\n",
            "Epoch 54/100\n",
            "12896/12896 [==============================] - 11s 821us/step - loss: 0.0419 - out_caps_loss: 0.0282 - out_recon_loss: 27.4103 - out_caps_acc: 0.9653 - val_loss: 0.0605 - val_out_caps_loss: 0.0469 - val_out_recon_loss: 27.2557 - val_out_caps_acc: 0.9339\n",
            "Epoch 55/100\n",
            "12896/12896 [==============================] - 11s 827us/step - loss: 0.0417 - out_caps_loss: 0.0280 - out_recon_loss: 27.4102 - out_caps_acc: 0.9649 - val_loss: 0.0613 - val_out_caps_loss: 0.0477 - val_out_recon_loss: 27.2555 - val_out_caps_acc: 0.9352\n",
            "Epoch 56/100\n",
            "12896/12896 [==============================] - 12s 914us/step - loss: 0.0415 - out_caps_loss: 0.0278 - out_recon_loss: 27.4101 - out_caps_acc: 0.9664 - val_loss: 0.0602 - val_out_caps_loss: 0.0466 - val_out_recon_loss: 27.2557 - val_out_caps_acc: 0.9358\n",
            "Epoch 57/100\n",
            "12896/12896 [==============================] - 11s 846us/step - loss: 0.0416 - out_caps_loss: 0.0279 - out_recon_loss: 27.4103 - out_caps_acc: 0.9651 - val_loss: 0.0623 - val_out_caps_loss: 0.0487 - val_out_recon_loss: 27.2557 - val_out_caps_acc: 0.9358\n",
            "Epoch 58/100\n",
            "12896/12896 [==============================] - 11s 856us/step - loss: 0.0411 - out_caps_loss: 0.0274 - out_recon_loss: 27.4103 - out_caps_acc: 0.9663 - val_loss: 0.0625 - val_out_caps_loss: 0.0489 - val_out_recon_loss: 27.2557 - val_out_caps_acc: 0.9352\n",
            "Epoch 59/100\n",
            "12896/12896 [==============================] - 11s 826us/step - loss: 0.0410 - out_caps_loss: 0.0273 - out_recon_loss: 27.4108 - out_caps_acc: 0.9672 - val_loss: 0.0615 - val_out_caps_loss: 0.0479 - val_out_recon_loss: 27.2559 - val_out_caps_acc: 0.9389\n",
            "Epoch 60/100\n",
            "12896/12896 [==============================] - 11s 827us/step - loss: 0.0416 - out_caps_loss: 0.0279 - out_recon_loss: 27.4107 - out_caps_acc: 0.9629 - val_loss: 0.0629 - val_out_caps_loss: 0.0493 - val_out_recon_loss: 27.2567 - val_out_caps_acc: 0.9358\n",
            "Epoch 61/100\n",
            "12896/12896 [==============================] - 11s 824us/step - loss: 0.0412 - out_caps_loss: 0.0275 - out_recon_loss: 27.4103 - out_caps_acc: 0.9677 - val_loss: 0.0625 - val_out_caps_loss: 0.0489 - val_out_recon_loss: 27.2550 - val_out_caps_acc: 0.9293\n",
            "Epoch 62/100\n",
            "12896/12896 [==============================] - 11s 826us/step - loss: 0.0409 - out_caps_loss: 0.0272 - out_recon_loss: 27.4101 - out_caps_acc: 0.9658 - val_loss: 0.0569 - val_out_caps_loss: 0.0433 - val_out_recon_loss: 27.2574 - val_out_caps_acc: 0.9470\n",
            "Epoch 63/100\n",
            "12896/12896 [==============================] - 11s 868us/step - loss: 0.0394 - out_caps_loss: 0.0257 - out_recon_loss: 27.4100 - out_caps_acc: 0.9698 - val_loss: 0.0600 - val_out_caps_loss: 0.0464 - val_out_recon_loss: 27.2560 - val_out_caps_acc: 0.9364\n",
            "Epoch 64/100\n",
            "12896/12896 [==============================] - 11s 863us/step - loss: 0.0391 - out_caps_loss: 0.0254 - out_recon_loss: 27.4104 - out_caps_acc: 0.9677 - val_loss: 0.0651 - val_out_caps_loss: 0.0515 - val_out_recon_loss: 27.2574 - val_out_caps_acc: 0.9293\n",
            "Epoch 65/100\n",
            "12896/12896 [==============================] - 11s 818us/step - loss: 0.0404 - out_caps_loss: 0.0267 - out_recon_loss: 27.4099 - out_caps_acc: 0.9674 - val_loss: 0.0648 - val_out_caps_loss: 0.0512 - val_out_recon_loss: 27.2554 - val_out_caps_acc: 0.9296\n",
            "Epoch 66/100\n",
            "12896/12896 [==============================] - 12s 939us/step - loss: 0.0375 - out_caps_loss: 0.0238 - out_recon_loss: 27.4100 - out_caps_acc: 0.9712 - val_loss: 0.0729 - val_out_caps_loss: 0.0593 - val_out_recon_loss: 27.2558 - val_out_caps_acc: 0.9231\n",
            "Epoch 67/100\n",
            "12896/12896 [==============================] - 11s 819us/step - loss: 0.0392 - out_caps_loss: 0.0255 - out_recon_loss: 27.4102 - out_caps_acc: 0.9691 - val_loss: 0.0560 - val_out_caps_loss: 0.0423 - val_out_recon_loss: 27.2551 - val_out_caps_acc: 0.9420\n",
            "Epoch 68/100\n",
            "12896/12896 [==============================] - 11s 815us/step - loss: 0.0371 - out_caps_loss: 0.0234 - out_recon_loss: 27.4102 - out_caps_acc: 0.9715 - val_loss: 0.0640 - val_out_caps_loss: 0.0504 - val_out_recon_loss: 27.2559 - val_out_caps_acc: 0.9308\n",
            "Epoch 69/100\n",
            "12896/12896 [==============================] - 11s 822us/step - loss: 0.0394 - out_caps_loss: 0.0257 - out_recon_loss: 27.4104 - out_caps_acc: 0.9699 - val_loss: 0.0575 - val_out_caps_loss: 0.0439 - val_out_recon_loss: 27.2560 - val_out_caps_acc: 0.9398\n",
            "Epoch 70/100\n",
            "12896/12896 [==============================] - 11s 845us/step - loss: 0.0370 - out_caps_loss: 0.0233 - out_recon_loss: 27.4103 - out_caps_acc: 0.9729 - val_loss: 0.0560 - val_out_caps_loss: 0.0423 - val_out_recon_loss: 27.2560 - val_out_caps_acc: 0.9454\n",
            "Epoch 71/100\n",
            "12896/12896 [==============================] - 12s 905us/step - loss: 0.0382 - out_caps_loss: 0.0245 - out_recon_loss: 27.4107 - out_caps_acc: 0.9693 - val_loss: 0.0650 - val_out_caps_loss: 0.0514 - val_out_recon_loss: 27.2549 - val_out_caps_acc: 0.9265\n",
            "Epoch 72/100\n",
            "12896/12896 [==============================] - 11s 824us/step - loss: 0.0382 - out_caps_loss: 0.0245 - out_recon_loss: 27.4106 - out_caps_acc: 0.9705 - val_loss: 0.0631 - val_out_caps_loss: 0.0494 - val_out_recon_loss: 27.2556 - val_out_caps_acc: 0.9355\n",
            "Epoch 73/100\n",
            "12896/12896 [==============================] - 11s 819us/step - loss: 0.0367 - out_caps_loss: 0.0230 - out_recon_loss: 27.4099 - out_caps_acc: 0.9724 - val_loss: 0.0548 - val_out_caps_loss: 0.0412 - val_out_recon_loss: 27.2557 - val_out_caps_acc: 0.9482\n",
            "Epoch 74/100\n",
            "12896/12896 [==============================] - 11s 821us/step - loss: 0.0367 - out_caps_loss: 0.0230 - out_recon_loss: 27.4102 - out_caps_acc: 0.9719 - val_loss: 0.0616 - val_out_caps_loss: 0.0479 - val_out_recon_loss: 27.2554 - val_out_caps_acc: 0.9308\n",
            "Epoch 75/100\n",
            "12896/12896 [==============================] - 11s 817us/step - loss: 0.0394 - out_caps_loss: 0.0257 - out_recon_loss: 27.4100 - out_caps_acc: 0.9694 - val_loss: 0.0575 - val_out_caps_loss: 0.0438 - val_out_recon_loss: 27.2551 - val_out_caps_acc: 0.9380\n",
            "Epoch 76/100\n",
            "12896/12896 [==============================] - 10s 813us/step - loss: 0.0359 - out_caps_loss: 0.0222 - out_recon_loss: 27.4105 - out_caps_acc: 0.9732 - val_loss: 0.0582 - val_out_caps_loss: 0.0445 - val_out_recon_loss: 27.2550 - val_out_caps_acc: 0.9398\n",
            "Epoch 77/100\n",
            "12896/12896 [==============================] - 11s 820us/step - loss: 0.0367 - out_caps_loss: 0.0230 - out_recon_loss: 27.4100 - out_caps_acc: 0.9721 - val_loss: 0.0587 - val_out_caps_loss: 0.0450 - val_out_recon_loss: 27.2555 - val_out_caps_acc: 0.9373\n",
            "Epoch 78/100\n",
            "12896/12896 [==============================] - 12s 894us/step - loss: 0.0361 - out_caps_loss: 0.0224 - out_recon_loss: 27.4100 - out_caps_acc: 0.9730 - val_loss: 0.0612 - val_out_caps_loss: 0.0476 - val_out_recon_loss: 27.2554 - val_out_caps_acc: 0.9373\n",
            "Epoch 79/100\n",
            "12896/12896 [==============================] - 11s 851us/step - loss: 0.0337 - out_caps_loss: 0.0200 - out_recon_loss: 27.4098 - out_caps_acc: 0.9775 - val_loss: 0.0736 - val_out_caps_loss: 0.0600 - val_out_recon_loss: 27.2558 - val_out_caps_acc: 0.9194\n",
            "Epoch 80/100\n",
            "12896/12896 [==============================] - 11s 826us/step - loss: 0.0367 - out_caps_loss: 0.0230 - out_recon_loss: 27.4099 - out_caps_acc: 0.9732 - val_loss: 0.0578 - val_out_caps_loss: 0.0442 - val_out_recon_loss: 27.2553 - val_out_caps_acc: 0.9398\n",
            "Epoch 81/100\n",
            "12896/12896 [==============================] - 11s 828us/step - loss: 0.0361 - out_caps_loss: 0.0224 - out_recon_loss: 27.4101 - out_caps_acc: 0.9730 - val_loss: 0.0550 - val_out_caps_loss: 0.0414 - val_out_recon_loss: 27.2553 - val_out_caps_acc: 0.9476\n",
            "Epoch 82/100\n",
            "12896/12896 [==============================] - 11s 832us/step - loss: 0.0351 - out_caps_loss: 0.0214 - out_recon_loss: 27.4101 - out_caps_acc: 0.9752 - val_loss: 0.0682 - val_out_caps_loss: 0.0545 - val_out_recon_loss: 27.2557 - val_out_caps_acc: 0.9237\n",
            "Epoch 83/100\n",
            "12896/12896 [==============================] - 11s 838us/step - loss: 0.0360 - out_caps_loss: 0.0223 - out_recon_loss: 27.4104 - out_caps_acc: 0.9736 - val_loss: 0.0555 - val_out_caps_loss: 0.0419 - val_out_recon_loss: 27.2564 - val_out_caps_acc: 0.9411\n",
            "Epoch 84/100\n",
            "12896/12896 [==============================] - 11s 830us/step - loss: 0.0336 - out_caps_loss: 0.0199 - out_recon_loss: 27.4100 - out_caps_acc: 0.9765 - val_loss: 0.0556 - val_out_caps_loss: 0.0420 - val_out_recon_loss: 27.2553 - val_out_caps_acc: 0.9470\n",
            "Epoch 85/100\n",
            "12896/12896 [==============================] - 11s 858us/step - loss: 0.0343 - out_caps_loss: 0.0206 - out_recon_loss: 27.4099 - out_caps_acc: 0.9769 - val_loss: 0.0654 - val_out_caps_loss: 0.0517 - val_out_recon_loss: 27.2554 - val_out_caps_acc: 0.9308\n",
            "Epoch 86/100\n",
            "12896/12896 [==============================] - 12s 898us/step - loss: 0.0334 - out_caps_loss: 0.0197 - out_recon_loss: 27.4101 - out_caps_acc: 0.9771 - val_loss: 0.0570 - val_out_caps_loss: 0.0434 - val_out_recon_loss: 27.2561 - val_out_caps_acc: 0.9423\n",
            "Epoch 87/100\n",
            "12896/12896 [==============================] - 11s 825us/step - loss: 0.0343 - out_caps_loss: 0.0206 - out_recon_loss: 27.4098 - out_caps_acc: 0.9770 - val_loss: 0.0643 - val_out_caps_loss: 0.0506 - val_out_recon_loss: 27.2596 - val_out_caps_acc: 0.9315\n",
            "Epoch 88/100\n",
            "12896/12896 [==============================] - 11s 821us/step - loss: 0.0329 - out_caps_loss: 0.0192 - out_recon_loss: 27.4098 - out_caps_acc: 0.9772 - val_loss: 0.0524 - val_out_caps_loss: 0.0388 - val_out_recon_loss: 27.2548 - val_out_caps_acc: 0.9510\n",
            "Epoch 89/100\n",
            "12896/12896 [==============================] - 11s 829us/step - loss: 0.0348 - out_caps_loss: 0.0211 - out_recon_loss: 27.4101 - out_caps_acc: 0.9753 - val_loss: 0.0645 - val_out_caps_loss: 0.0508 - val_out_recon_loss: 27.2550 - val_out_caps_acc: 0.9318\n",
            "Epoch 90/100\n",
            "12896/12896 [==============================] - 11s 828us/step - loss: 0.0335 - out_caps_loss: 0.0198 - out_recon_loss: 27.4100 - out_caps_acc: 0.9770 - val_loss: 0.0598 - val_out_caps_loss: 0.0461 - val_out_recon_loss: 27.2550 - val_out_caps_acc: 0.9370\n",
            "Epoch 91/100\n",
            "12896/12896 [==============================] - 11s 827us/step - loss: 0.0320 - out_caps_loss: 0.0183 - out_recon_loss: 27.4101 - out_caps_acc: 0.9787 - val_loss: 0.0548 - val_out_caps_loss: 0.0411 - val_out_recon_loss: 27.2554 - val_out_caps_acc: 0.9457\n",
            "Epoch 92/100\n",
            "12896/12896 [==============================] - 11s 829us/step - loss: 0.0327 - out_caps_loss: 0.0190 - out_recon_loss: 27.4095 - out_caps_acc: 0.9784 - val_loss: 0.0584 - val_out_caps_loss: 0.0448 - val_out_recon_loss: 27.2549 - val_out_caps_acc: 0.9395\n",
            "Epoch 93/100\n",
            "12896/12896 [==============================] - 12s 914us/step - loss: 0.0328 - out_caps_loss: 0.0191 - out_recon_loss: 27.4098 - out_caps_acc: 0.9783 - val_loss: 0.0543 - val_out_caps_loss: 0.0407 - val_out_recon_loss: 27.2549 - val_out_caps_acc: 0.9454\n",
            "Epoch 94/100\n",
            "12896/12896 [==============================] - 11s 888us/step - loss: 0.0322 - out_caps_loss: 0.0185 - out_recon_loss: 27.4099 - out_caps_acc: 0.9790 - val_loss: 0.0556 - val_out_caps_loss: 0.0420 - val_out_recon_loss: 27.2546 - val_out_caps_acc: 0.9423\n",
            "Epoch 95/100\n",
            "12896/12896 [==============================] - 12s 893us/step - loss: 0.0320 - out_caps_loss: 0.0183 - out_recon_loss: 27.4099 - out_caps_acc: 0.9793 - val_loss: 0.0639 - val_out_caps_loss: 0.0503 - val_out_recon_loss: 27.2549 - val_out_caps_acc: 0.9330\n",
            "Epoch 96/100\n",
            "12896/12896 [==============================] - 11s 824us/step - loss: 0.0322 - out_caps_loss: 0.0185 - out_recon_loss: 27.4097 - out_caps_acc: 0.9789 - val_loss: 0.0600 - val_out_caps_loss: 0.0464 - val_out_recon_loss: 27.2546 - val_out_caps_acc: 0.9404\n",
            "Epoch 97/100\n",
            "12896/12896 [==============================] - 11s 828us/step - loss: 0.0315 - out_caps_loss: 0.0178 - out_recon_loss: 27.4097 - out_caps_acc: 0.9800 - val_loss: 0.0598 - val_out_caps_loss: 0.0462 - val_out_recon_loss: 27.2548 - val_out_caps_acc: 0.9395\n",
            "Epoch 98/100\n",
            "12896/12896 [==============================] - 11s 826us/step - loss: 0.0315 - out_caps_loss: 0.0178 - out_recon_loss: 27.4097 - out_caps_acc: 0.9785 - val_loss: 0.0517 - val_out_caps_loss: 0.0381 - val_out_recon_loss: 27.2548 - val_out_caps_acc: 0.9494\n",
            "Epoch 99/100\n",
            "12896/12896 [==============================] - 11s 820us/step - loss: 0.0315 - out_caps_loss: 0.0178 - out_recon_loss: 27.4095 - out_caps_acc: 0.9812 - val_loss: 0.0584 - val_out_caps_loss: 0.0448 - val_out_recon_loss: 27.2572 - val_out_caps_acc: 0.9389\n",
            "Epoch 100/100\n",
            "12896/12896 [==============================] - 11s 877us/step - loss: 0.0309 - out_caps_loss: 0.0172 - out_recon_loss: 27.4099 - out_caps_acc: 0.9804 - val_loss: 0.0592 - val_out_caps_loss: 0.0456 - val_out_recon_loss: 27.2559 - val_out_caps_acc: 0.9389\n",
            "Trained model saved to 'trained_model.h5'\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.engine.training.Model at 0x7f3e504fde10>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 47
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DlDlI-OOMWmK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "layer_name = 'out_caps'\n",
        "intermediate_layer_model4 = models.Model(inputs=model4.input[0],\n",
        "                                 outputs=model4.get_layer(layer_name).output)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4L88lhj6QjkN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "y_pred4 = intermediate_layer_model4.predict(x_test_reshape4)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pQBYhgOTQuBa",
        "colab_type": "code",
        "outputId": "db49c644-5c7b-4c62-a3e1-cbd11bc4c056",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "cm4 = confusion_matrix(y_test4, np.argmax(y_pred4, axis=1))\n",
        "cm4"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[1842,  173],\n",
              "       [  64, 1951]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 50
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T2N_H0chQ1qW",
        "colab_type": "text"
      },
      "source": [
        "# Training and Testing Set 5"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PUMFQXGGQwyR",
        "colab_type": "code",
        "outputId": "42f01bcc-78ad-4e4c-cb6f-b6cfd28e9242",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "model5 = CapsNet(input_shape=[20, 1],\n",
        "                n_class=2,\n",
        "                num_routing=3)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[20, 1] (?, 20, 1)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7TC5k3e5Q-hC",
        "colab_type": "code",
        "outputId": "1637b34a-bf89-4aab-e117-ef6539b66da1",
        "colab": {
          "resources": {
            "http://localhost:8080/nbextensions/google.colab/files.js": {
              "data": "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",
              "ok": true,
              "headers": [
                [
                  "content-type",
                  "application/javascript"
                ]
              ],
              "status": 200,
              "status_text": ""
            }
          },
          "base_uri": "https://localhost:8080/",
          "height": 74
        }
      },
      "source": [
        "uploaded = files.upload()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-b0e500a9-f98e-45cf-88bf-ed8f9a17330f\" name=\"files[]\" multiple disabled />\n",
              "     <output id=\"result-b0e500a9-f98e-45cf-88bf-ed8f9a17330f\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script src=\"/nbextensions/google.colab/files.js\"></script> "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Saving AComp5.csv to AComp5.csv\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GH5T_MJYRFhV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df8 = pd.read_csv(io.BytesIO(uploaded['AComp5.csv']))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-3CKSg2ZROHG",
        "colab_type": "code",
        "outputId": "30753d5b-b003-4444-f5b4-73cc9d8d1dd4",
        "colab": {
          "resources": {
            "http://localhost:8080/nbextensions/google.colab/files.js": {
              "data": "Ly8gQ29weXJpZ2h0IDIwMTcgR29vZ2xlIExMQwovLwovLyBMaWNlbnNlZCB1bmRlciB0aGUgQXBhY2hlIExpY2Vuc2UsIFZlcnNpb24gMi4wICh0aGUgIkxpY2Vuc2UiKTsKLy8geW91IG1heSBub3QgdXNlIHRoaXMgZmlsZSBleGNlcHQgaW4gY29tcGxpYW5jZSB3aXRoIHRoZSBMaWNlbnNlLgovLyBZb3UgbWF5IG9idGFpbiBhIGNvcHkgb2YgdGhlIExpY2Vuc2UgYXQKLy8KLy8gICAgICBodHRwOi8vd3d3LmFwYWNoZS5vcmcvbGljZW5zZXMvTElDRU5TRS0yLjAKLy8KLy8gVW5sZXNzIHJlcXVpcmVkIGJ5IGFwcGxpY2FibGUgbGF3IG9yIGFncmVlZCB0byBpbiB3cml0aW5nLCBzb2Z0d2FyZQovLyBkaXN0cmlidXRlZCB1bmRlciB0aGUgTGljZW5zZSBpcyBkaXN0cmlidXRlZCBvbiBhbiAiQVMgSVMiIEJBU0lTLAovLyBXSVRIT1VUIFdBUlJBTlRJRVMgT1IgQ09ORElUSU9OUyBPRiBBTlkgS0lORCwgZWl0aGVyIGV4cHJlc3Mgb3IgaW1wbGllZC4KLy8gU2VlIHRoZSBMaWNlbnNlIGZvciB0aGUgc3BlY2lmaWMgbGFuZ3VhZ2UgZ292ZXJuaW5nIHBlcm1pc3Npb25zIGFuZAovLyBsaW1pdGF0aW9ucyB1bmRlciB0aGUgTGljZW5zZS4KCi8qKgogKiBAZmlsZW92ZXJ2aWV3IEhlbHBlcnMgZm9yIGdvb2dsZS5jb2xhYiBQeXRob24gbW9kdWxlLgogKi8KKGZ1bmN0aW9uKHNjb3BlKSB7CmZ1bmN0aW9uIHNwYW4odGV4dCwgc3R5bGVBdHRyaWJ1dGVzID0ge30pIHsKICBjb25zdCBlbGVtZW50ID0gZG9jdW1lbnQuY3JlYXRlRWxlbWVudCgnc3BhbicpOwogIGVsZW1lbnQudGV4dENvbnRlbnQgPSB0ZXh0OwogIGZvciAoY29uc3Qga2V5IG9mIE9iamVjdC5rZXlzKHN0eWxlQXR0cmlidXRlcykpIHsKICAgIGVsZW1lbnQuc3R5bGVba2V5XSA9IHN0eWxlQXR0cmlidXRlc1trZXldOwogIH0KICByZXR1cm4gZWxlbWVudDsKfQoKLy8gTWF4IG51bWJlciBvZiBieXRlcyB3aGljaCB3aWxsIGJlIHVwbG9hZGVkIGF0IGEgdGltZS4KY29uc3QgTUFYX1BBWUxPQURfU0laRSA9IDEwMCAqIDEwMjQ7Ci8vIE1heCBhbW91bnQgb2YgdGltZSB0byBibG9jayB3YWl0aW5nIGZvciB0aGUgdXNlci4KY29uc3QgRklMRV9DSEFOR0VfVElNRU9VVF9NUyA9IDMwICogMTAwMDsKCmZ1bmN0aW9uIF91cGxvYWRGaWxlcyhpbnB1dElkLCBvdXRwdXRJZCkgewogIGNvbnN0IHN0ZXBzID0gdXBsb2FkRmlsZXNTdGVwKGlucHV0SWQsIG91dHB1dElkKTsKICBjb25zdCBvdXRwdXRFbGVtZW50ID0gZG9jdW1lbnQuZ2V0RWxlbWVudEJ5SWQob3V0cHV0SWQpOwogIC8vIENhY2hlIHN0ZXBzIG9uIHRoZSBvdXRwdXRFbGVtZW50IHRvIG1ha2UgaXQgYXZhaWxhYmxlIGZvciB0aGUgbmV4dCBjYWxsCiAgLy8gdG8gdXBsb2FkRmlsZXNDb250aW51ZSBmcm9tIFB5dGhvbi4KICBvdXRwdXRFbGVtZW50LnN0ZXBzID0gc3RlcHM7CgogIHJldHVybiBfdXBsb2FkRmlsZXNDb250aW51ZShvdXRwdXRJZCk7Cn0KCi8vIFRoaXMgaXMgcm91Z2hseSBhbiBhc3luYyBnZW5lcmF0b3IgKG5vdCBzdXBwb3J0ZWQgaW4gdGhlIGJyb3dzZXIgeWV0KSwKLy8gd2hlcmUgdGhlcmUgYXJlIG11bHRpcGxlIGFzeW5jaHJvbm91cyBzdGVwcyBhbmQgdGhlIFB5dGhvbiBzaWRlIGlzIGdvaW5nCi8vIHRvIHBvbGwgZm9yIGNvbXBsZXRpb24gb2YgZWFjaCBzdGVwLgovLyBUaGlzIHVzZXMgYSBQcm9taXNlIHRvIGJsb2NrIHRoZSBweXRob24gc2lkZSBvbiBjb21wbGV0aW9uIG9mIGVhY2ggc3RlcCwKLy8gdGhlbiBwYXNzZXMgdGhlIHJlc3VsdCBvZiB0aGUgcHJldmlvdXMgc3RlcCBhcyB0aGUgaW5wdXQgdG8gdGhlIG5leHQgc3RlcC4KZnVuY3Rpb24gX3VwbG9hZEZpbGVzQ29udGludWUob3V0cHV0SWQpIHsKICBjb25zdCBvdXRwdXRFbGVtZW50ID0gZG9jdW1lbnQuZ2V0RWxlbWVudEJ5SWQob3V0cHV0SWQpOwogIGNvbnN0IHN0ZXBzID0gb3V0cHV0RWxlbWVudC5zdGVwczsKCiAgY29uc3QgbmV4dCA9IHN0ZXBzLm5leHQob3V0cHV0RWxlbWVudC5sYXN0UHJvbWlzZVZhbHVlKTsKICByZXR1cm4gUHJvbWlzZS5yZXNvbHZlKG5leHQudmFsdWUucHJvbWlzZSkudGhlbigodmFsdWUpID0+IHsKICAgIC8vIENhY2hlIHRoZSBsYXN0IHByb21pc2UgdmFsdWUgdG8gbWFrZSBpdCBhdmFpbGFibGUgdG8gdGhlIG5leHQKICAgIC8vIHN0ZXAgb2YgdGhlIGdlbmVyYXRvci4KICAgIG91dHB1dEVsZW1lbnQubGFzdFByb21pc2VWYWx1ZSA9IHZhbHVlOwogICAgcmV0dXJuIG5leHQudmFsdWUucmVzcG9uc2U7CiAgfSk7Cn0KCi8qKgogKiBHZW5lcmF0b3IgZnVuY3Rpb24gd2hpY2ggaXMgY2FsbGVkIGJldHdlZW4gZWFjaCBhc3luYyBzdGVwIG9mIHRoZSB1cGxvYWQKICogcHJvY2Vzcy4KICogQHBhcmFtIHtzdHJpbmd9IGlucHV0SWQgRWxlbWVudCBJRCBvZiB0aGUgaW5wdXQgZmlsZSBwaWNrZXIgZWxlbWVudC4KICogQHBhcmFtIHtzdHJpbmd9IG91dHB1dElkIEVsZW1lbnQgSUQgb2YgdGhlIG91dHB1dCBkaXNwbGF5LgogKiBAcmV0dXJuIHshSXRlcmFibGU8IU9iamVjdD59IEl0ZXJhYmxlIG9mIG5leHQgc3RlcHMuCiAqLwpmdW5jdGlvbiogdXBsb2FkRmlsZXNTdGVwKGlucHV0SWQsIG91dHB1dElkKSB7CiAgY29uc3QgaW5wdXRFbGVtZW50ID0gZG9jdW1lbnQuZ2V0RWxlbWVudEJ5SWQoaW5wdXRJZCk7CiAgaW5wdXRFbGVtZW50LmRpc2FibGVkID0gZmFsc2U7CgogIGNvbnN0IG91dHB1dEVsZW1lbnQgPSBkb2N1bWVudC5nZXRFbGVtZW50QnlJZChvdXRwdXRJZCk7CiAgb3V0cHV0RWxlbWVudC5pbm5lckhUTUwgPSAnJzsKCiAgY29uc3QgcGlja2VkUHJvbWlzZSA9IG5ldyBQcm9taXNlKChyZXNvbHZlKSA9PiB7CiAgICBpbnB1dEVsZW1lbnQuYWRkRXZlbnRMaXN0ZW5lcignY2hhbmdlJywgKGUpID0+IHsKICAgICAgcmVzb2x2ZShlLnRhcmdldC5maWxlcyk7CiAgICB9KTsKICB9KTsKCiAgY29uc3QgY2FuY2VsID0gZG9jdW1lbnQuY3JlYXRlRWxlbWVudCgnYnV0dG9uJyk7CiAgaW5wdXRFbGVtZW50LnBhcmVudEVsZW1lbnQuYXBwZW5kQ2hpbGQoY2FuY2VsKTsKICBjYW5jZWwudGV4dENvbnRlbnQgPSAnQ2FuY2VsIHVwbG9hZCc7CiAgY29uc3QgY2FuY2VsUHJvbWlzZSA9IG5ldyBQcm9taXNlKChyZXNvbHZlKSA9PiB7CiAgICBjYW5jZWwub25jbGljayA9ICgpID0+IHsKICAgICAgcmVzb2x2ZShudWxsKTsKICAgIH07CiAgfSk7CgogIC8vIENhbmNlbCB1cGxvYWQgaWYgdXNlciBoYXNuJ3QgcGlja2VkIGFueXRoaW5nIGluIHRpbWVvdXQuCiAgY29uc3QgdGltZW91dFByb21pc2UgPSBuZXcgUHJvbWlzZSgocmVzb2x2ZSkgPT4gewogICAgc2V0VGltZW91dCgoKSA9PiB7CiAgICAgIHJlc29sdmUobnVsbCk7CiAgICB9LCBGSUxFX0NIQU5HRV9USU1FT1VUX01TKTsKICB9KTsKCiAgLy8gV2FpdCBmb3IgdGhlIHVzZXIgdG8gcGljayB0aGUgZmlsZXMuCiAgY29uc3QgZmlsZXMgPSB5aWVsZCB7CiAgICBwcm9taXNlOiBQcm9taXNlLnJhY2UoW3BpY2tlZFByb21pc2UsIHRpbWVvdXRQcm9taXNlLCBjYW5jZWxQcm9taXNlXSksCiAgICByZXNwb25zZTogewogICAgICBhY3Rpb246ICdzdGFydGluZycsCiAgICB9CiAgfTsKCiAgaWYgKCFmaWxlcykgewogICAgcmV0dXJuIHsKICAgICAgcmVzcG9uc2U6IHsKICAgICAgICBhY3Rpb246ICdjb21wbGV0ZScsCiAgICAgIH0KICAgIH07CiAgfQoKICBjYW5jZWwucmVtb3ZlKCk7CgogIC8vIERpc2FibGUgdGhlIGlucHV0IGVsZW1lbnQgc2luY2UgZnVydGhlciBwaWNrcyBhcmUgbm90IGFsbG93ZWQuCiAgaW5wdXRFbGVtZW50LmRpc2FibGVkID0gdHJ1ZTsKCiAgZm9yIChjb25zdCBmaWxlIG9mIGZpbGVzKSB7CiAgICBjb25zdCBsaSA9IGRvY3VtZW50LmNyZWF0ZUVsZW1lbnQoJ2xpJyk7CiAgICBsaS5hcHBlbmQoc3BhbihmaWxlLm5hbWUsIHtmb250V2VpZ2h0OiAnYm9sZCd9KSk7CiAgICBsaS5hcHBlbmQoc3BhbigKICAgICAgICBgKCR7ZmlsZS50eXBlIHx8ICduL2EnfSkgLSAke2ZpbGUuc2l6ZX0gYnl0ZXMsIGAgKwogICAgICAgIGBsYXN0IG1vZGlmaWVkOiAkewogICAgICAgICAgICBmaWxlLmxhc3RNb2RpZmllZERhdGUgPyBmaWxlLmxhc3RNb2RpZmllZERhdGUudG9Mb2NhbGVEYXRlU3RyaW5nKCkgOgogICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAnbi9hJ30gLSBgKSk7CiAgICBjb25zdCBwZXJjZW50ID0gc3BhbignMCUgZG9uZScpOwogICAgbGkuYXBwZW5kQ2hpbGQocGVyY2VudCk7CgogICAgb3V0cHV0RWxlbWVudC5hcHBlbmRDaGlsZChsaSk7CgogICAgY29uc3QgZmlsZURhdGFQcm9taXNlID0gbmV3IFByb21pc2UoKHJlc29sdmUpID0+IHsKICAgICAgY29uc3QgcmVhZGVyID0gbmV3IEZpbGVSZWFkZXIoKTsKICAgICAgcmVhZGVyLm9ubG9hZCA9IChlKSA9PiB7CiAgICAgICAgcmVzb2x2ZShlLnRhcmdldC5yZXN1bHQpOwogICAgICB9OwogICAgICByZWFkZXIucmVhZEFzQXJyYXlCdWZmZXIoZmlsZSk7CiAgICB9KTsKICAgIC8vIFdhaXQgZm9yIHRoZSBkYXRhIHRvIGJlIHJlYWR5LgogICAgbGV0IGZpbGVEYXRhID0geWllbGQgewogICAgICBwcm9taXNlOiBmaWxlRGF0YVByb21pc2UsCiAgICAgIHJlc3BvbnNlOiB7CiAgICAgICAgYWN0aW9uOiAnY29udGludWUnLAogICAgICB9CiAgICB9OwoKICAgIC8vIFVzZSBhIGNodW5rZWQgc2VuZGluZyB0byBhdm9pZCBtZXNzYWdlIHNpemUgbGltaXRzLiBTZWUgYi82MjExNTY2MC4KICAgIGxldCBwb3NpdGlvbiA9IDA7CiAgICB3aGlsZSAocG9zaXRpb24gPCBmaWxlRGF0YS5ieXRlTGVuZ3RoKSB7CiAgICAgIGNvbnN0IGxlbmd0aCA9IE1hdGgubWluKGZpbGVEYXRhLmJ5dGVMZW5ndGggLSBwb3NpdGlvbiwgTUFYX1BBWUxPQURfU0laRSk7CiAgICAgIGNvbnN0IGNodW5rID0gbmV3IFVpbnQ4QXJyYXkoZmlsZURhdGEsIHBvc2l0aW9uLCBsZW5ndGgpOwogICAgICBwb3NpdGlvbiArPSBsZW5ndGg7CgogICAgICBjb25zdCBiYXNlNjQgPSBidG9hKFN0cmluZy5mcm9tQ2hhckNvZGUuYXBwbHkobnVsbCwgY2h1bmspKTsKICAgICAgeWllbGQgewogICAgICAgIHJlc3BvbnNlOiB7CiAgICAgICAgICBhY3Rpb246ICdhcHBlbmQnLAogICAgICAgICAgZmlsZTogZmlsZS5uYW1lLAogICAgICAgICAgZGF0YTogYmFzZTY0LAogICAgICAgIH0sCiAgICAgIH07CiAgICAgIHBlcmNlbnQudGV4dENvbnRlbnQgPQogICAgICAgICAgYCR7TWF0aC5yb3VuZCgocG9zaXRpb24gLyBmaWxlRGF0YS5ieXRlTGVuZ3RoKSAqIDEwMCl9JSBkb25lYDsKICAgIH0KICB9CgogIC8vIEFsbCBkb25lLgogIHlpZWxkIHsKICAgIHJlc3BvbnNlOiB7CiAgICAgIGFjdGlvbjogJ2NvbXBsZXRlJywKICAgIH0KICB9Owp9CgpzY29wZS5nb29nbGUgPSBzY29wZS5nb29nbGUgfHwge307CnNjb3BlLmdvb2dsZS5jb2xhYiA9IHNjb3BlLmdvb2dsZS5jb2xhYiB8fCB7fTsKc2NvcGUuZ29vZ2xlLmNvbGFiLl9maWxlcyA9IHsKICBfdXBsb2FkRmlsZXMsCiAgX3VwbG9hZEZpbGVzQ29udGludWUsCn07Cn0pKHNlbGYpOwo=",
              "ok": true,
              "headers": [
                [
                  "content-type",
                  "application/javascript"
                ]
              ],
              "status": 200,
              "status_text": ""
            }
          },
          "base_uri": "https://localhost:8080/",
          "height": 74
        }
      },
      "source": [
        "uploaded = files.upload()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-7f1ca3a6-3208-4e84-9f98-f9b099459210\" name=\"files[]\" multiple disabled />\n",
              "     <output id=\"result-7f1ca3a6-3208-4e84-9f98-f9b099459210\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script src=\"/nbextensions/google.colab/files.js\"></script> "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Saving test_AComp5.csv to test_AComp5.csv\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zHUiuM1BRRlD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df9 = pd.read_csv(io.BytesIO(uploaded['test_AComp5.csv']))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y2ERXSTARV5G",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "x_train5 = df8.iloc[:, 0:20].values\n",
        "y_train5 = df8.iloc[:, 20].values\n",
        "x_test5 = df9.iloc[:, 0:20].values\n",
        "y_test5 = df9.iloc[:, 20].values"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0NJ2_6wGRdGZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Reshaping the train and test set\n",
        "x_train_reshape5 = x_train5.reshape(16120, 20, 1)\n",
        "y_train_reshape5 = y_train5.reshape(16120, 1)\n",
        "x_test_reshape5 = x_test5.reshape(4030, 20, 1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rOkhamcGRhQe",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Reshaping the Labels\n",
        "y_train5_ = tf.keras.utils.to_categorical(y_train_reshape5,num_classes=2)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UwVmstuVRkUY",
        "colab_type": "code",
        "outputId": "74379ce0-bed0-499e-cd36-ec9df2442c7b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 3284
        }
      },
      "source": [
        "train(model=model5, data=((x_train_reshape5, y_train5_)), epoch_size_frac = 0.5)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 12896 samples, validate on 3224 samples\n",
            "Epoch 1/150\n",
            "12896/12896 [==============================] - 13s 1ms/step - loss: 0.1546 - out_caps_loss: 0.1402 - out_recon_loss: 28.6728 - out_caps_acc: 0.8078 - val_loss: 0.1124 - val_out_caps_loss: 0.0987 - val_out_recon_loss: 27.2798 - val_out_caps_acc: 0.8558\n",
            "Epoch 2/150\n",
            "12896/12896 [==============================] - 11s 852us/step - loss: 0.1087 - out_caps_loss: 0.0950 - out_recon_loss: 27.4061 - out_caps_acc: 0.8594 - val_loss: 0.1091 - val_out_caps_loss: 0.0955 - val_out_recon_loss: 27.2678 - val_out_caps_acc: 0.8573\n",
            "Epoch 3/150\n",
            "12896/12896 [==============================] - 11s 853us/step - loss: 0.1013 - out_caps_loss: 0.0876 - out_recon_loss: 27.3944 - out_caps_acc: 0.8693 - val_loss: 0.1119 - val_out_caps_loss: 0.0983 - val_out_recon_loss: 27.2636 - val_out_caps_acc: 0.8520\n",
            "Epoch 4/150\n",
            "12896/12896 [==============================] - 11s 844us/step - loss: 0.0970 - out_caps_loss: 0.0833 - out_recon_loss: 27.3913 - out_caps_acc: 0.8788 - val_loss: 0.1069 - val_out_caps_loss: 0.0933 - val_out_recon_loss: 27.2619 - val_out_caps_acc: 0.8592\n",
            "Epoch 5/150\n",
            "12896/12896 [==============================] - 11s 860us/step - loss: 0.0937 - out_caps_loss: 0.0800 - out_recon_loss: 27.3901 - out_caps_acc: 0.8827 - val_loss: 0.0947 - val_out_caps_loss: 0.0810 - val_out_recon_loss: 27.2606 - val_out_caps_acc: 0.8756\n",
            "Epoch 6/150\n",
            "12896/12896 [==============================] - 12s 915us/step - loss: 0.0895 - out_caps_loss: 0.0758 - out_recon_loss: 27.3891 - out_caps_acc: 0.8909 - val_loss: 0.1039 - val_out_caps_loss: 0.0902 - val_out_recon_loss: 27.2604 - val_out_caps_acc: 0.8654\n",
            "Epoch 7/150\n",
            "12896/12896 [==============================] - 11s 852us/step - loss: 0.0873 - out_caps_loss: 0.0736 - out_recon_loss: 27.3887 - out_caps_acc: 0.8910 - val_loss: 0.0948 - val_out_caps_loss: 0.0811 - val_out_recon_loss: 27.2593 - val_out_caps_acc: 0.8753\n",
            "Epoch 8/150\n",
            "12896/12896 [==============================] - 11s 853us/step - loss: 0.0844 - out_caps_loss: 0.0707 - out_recon_loss: 27.3883 - out_caps_acc: 0.8980 - val_loss: 0.0861 - val_out_caps_loss: 0.0724 - val_out_recon_loss: 27.2599 - val_out_caps_acc: 0.8933\n",
            "Epoch 9/150\n",
            "12896/12896 [==============================] - 11s 855us/step - loss: 0.0822 - out_caps_loss: 0.0685 - out_recon_loss: 27.3881 - out_caps_acc: 0.8998 - val_loss: 0.0880 - val_out_caps_loss: 0.0744 - val_out_recon_loss: 27.2593 - val_out_caps_acc: 0.8886\n",
            "Epoch 10/150\n",
            "12896/12896 [==============================] - 12s 897us/step - loss: 0.0795 - out_caps_loss: 0.0658 - out_recon_loss: 27.3879 - out_caps_acc: 0.9047 - val_loss: 0.0832 - val_out_caps_loss: 0.0696 - val_out_recon_loss: 27.2598 - val_out_caps_acc: 0.8998\n",
            "Epoch 11/150\n",
            "12896/12896 [==============================] - 11s 849us/step - loss: 0.0784 - out_caps_loss: 0.0647 - out_recon_loss: 27.3874 - out_caps_acc: 0.9055 - val_loss: 0.0864 - val_out_caps_loss: 0.0728 - val_out_recon_loss: 27.2589 - val_out_caps_acc: 0.8893\n",
            "Epoch 12/150\n",
            "12896/12896 [==============================] - 11s 852us/step - loss: 0.0760 - out_caps_loss: 0.0623 - out_recon_loss: 27.3875 - out_caps_acc: 0.9083 - val_loss: 0.0813 - val_out_caps_loss: 0.0677 - val_out_recon_loss: 27.2585 - val_out_caps_acc: 0.8955\n",
            "Epoch 13/150\n",
            "12896/12896 [==============================] - 12s 926us/step - loss: 0.0750 - out_caps_loss: 0.0613 - out_recon_loss: 27.3874 - out_caps_acc: 0.9107 - val_loss: 0.0850 - val_out_caps_loss: 0.0714 - val_out_recon_loss: 27.2589 - val_out_caps_acc: 0.8952\n",
            "Epoch 14/150\n",
            "12896/12896 [==============================] - 11s 847us/step - loss: 0.0728 - out_caps_loss: 0.0592 - out_recon_loss: 27.3872 - out_caps_acc: 0.9173 - val_loss: 0.0818 - val_out_caps_loss: 0.0682 - val_out_recon_loss: 27.2584 - val_out_caps_acc: 0.8976\n",
            "Epoch 15/150\n",
            "12896/12896 [==============================] - 11s 844us/step - loss: 0.0708 - out_caps_loss: 0.0571 - out_recon_loss: 27.3868 - out_caps_acc: 0.9188 - val_loss: 0.0799 - val_out_caps_loss: 0.0663 - val_out_recon_loss: 27.2584 - val_out_caps_acc: 0.9017\n",
            "Epoch 16/150\n",
            "12896/12896 [==============================] - 11s 840us/step - loss: 0.0701 - out_caps_loss: 0.0564 - out_recon_loss: 27.3860 - out_caps_acc: 0.9203 - val_loss: 0.0757 - val_out_caps_loss: 0.0621 - val_out_recon_loss: 27.2581 - val_out_caps_acc: 0.9079\n",
            "Epoch 17/150\n",
            "12896/12896 [==============================] - 11s 852us/step - loss: 0.0690 - out_caps_loss: 0.0553 - out_recon_loss: 27.3853 - out_caps_acc: 0.9210 - val_loss: 0.0770 - val_out_caps_loss: 0.0633 - val_out_recon_loss: 27.2575 - val_out_caps_acc: 0.9057\n",
            "Epoch 18/150\n",
            "12896/12896 [==============================] - 11s 852us/step - loss: 0.0669 - out_caps_loss: 0.0532 - out_recon_loss: 27.3851 - out_caps_acc: 0.9228 - val_loss: 0.0824 - val_out_caps_loss: 0.0687 - val_out_recon_loss: 27.2570 - val_out_caps_acc: 0.8989\n",
            "Epoch 19/150\n",
            "12896/12896 [==============================] - 11s 875us/step - loss: 0.0676 - out_caps_loss: 0.0539 - out_recon_loss: 27.3848 - out_caps_acc: 0.9225 - val_loss: 0.0738 - val_out_caps_loss: 0.0602 - val_out_recon_loss: 27.2563 - val_out_caps_acc: 0.9116\n",
            "Epoch 20/150\n",
            "12896/12896 [==============================] - 12s 963us/step - loss: 0.0651 - out_caps_loss: 0.0514 - out_recon_loss: 27.3845 - out_caps_acc: 0.9273 - val_loss: 0.0772 - val_out_caps_loss: 0.0635 - val_out_recon_loss: 27.2571 - val_out_caps_acc: 0.9063\n",
            "Epoch 21/150\n",
            "12896/12896 [==============================] - 11s 849us/step - loss: 0.0638 - out_caps_loss: 0.0501 - out_recon_loss: 27.3846 - out_caps_acc: 0.9300 - val_loss: 0.0856 - val_out_caps_loss: 0.0720 - val_out_recon_loss: 27.2581 - val_out_caps_acc: 0.9001\n",
            "Epoch 22/150\n",
            "12896/12896 [==============================] - 11s 852us/step - loss: 0.0629 - out_caps_loss: 0.0492 - out_recon_loss: 27.3846 - out_caps_acc: 0.9297 - val_loss: 0.0751 - val_out_caps_loss: 0.0615 - val_out_recon_loss: 27.2574 - val_out_caps_acc: 0.9097\n",
            "Epoch 23/150\n",
            "12896/12896 [==============================] - 11s 849us/step - loss: 0.0621 - out_caps_loss: 0.0484 - out_recon_loss: 27.3842 - out_caps_acc: 0.9316 - val_loss: 0.0842 - val_out_caps_loss: 0.0705 - val_out_recon_loss: 27.2569 - val_out_caps_acc: 0.8967\n",
            "Epoch 24/150\n",
            "12896/12896 [==============================] - 11s 840us/step - loss: 0.0613 - out_caps_loss: 0.0476 - out_recon_loss: 27.3843 - out_caps_acc: 0.9311 - val_loss: 0.0705 - val_out_caps_loss: 0.0569 - val_out_recon_loss: 27.2561 - val_out_caps_acc: 0.9156\n",
            "Epoch 25/150\n",
            "12896/12896 [==============================] - 11s 841us/step - loss: 0.0586 - out_caps_loss: 0.0449 - out_recon_loss: 27.3844 - out_caps_acc: 0.9377 - val_loss: 0.0705 - val_out_caps_loss: 0.0569 - val_out_recon_loss: 27.2566 - val_out_caps_acc: 0.9172\n",
            "Epoch 26/150\n",
            "12896/12896 [==============================] - 11s 858us/step - loss: 0.0592 - out_caps_loss: 0.0455 - out_recon_loss: 27.3840 - out_caps_acc: 0.9359 - val_loss: 0.0717 - val_out_caps_loss: 0.0581 - val_out_recon_loss: 27.2560 - val_out_caps_acc: 0.9184\n",
            "Epoch 27/150\n",
            "12896/12896 [==============================] - 12s 895us/step - loss: 0.0577 - out_caps_loss: 0.0440 - out_recon_loss: 27.3841 - out_caps_acc: 0.9385 - val_loss: 0.0809 - val_out_caps_loss: 0.0672 - val_out_recon_loss: 27.2560 - val_out_caps_acc: 0.9014\n",
            "Epoch 28/150\n",
            "12896/12896 [==============================] - 11s 882us/step - loss: 0.0568 - out_caps_loss: 0.0431 - out_recon_loss: 27.3833 - out_caps_acc: 0.9387 - val_loss: 0.0726 - val_out_caps_loss: 0.0590 - val_out_recon_loss: 27.2561 - val_out_caps_acc: 0.9203\n",
            "Epoch 29/150\n",
            "12896/12896 [==============================] - 11s 845us/step - loss: 0.0563 - out_caps_loss: 0.0426 - out_recon_loss: 27.3838 - out_caps_acc: 0.9422 - val_loss: 0.0646 - val_out_caps_loss: 0.0510 - val_out_recon_loss: 27.2556 - val_out_caps_acc: 0.9308\n",
            "Epoch 30/150\n",
            "12896/12896 [==============================] - 11s 845us/step - loss: 0.0537 - out_caps_loss: 0.0400 - out_recon_loss: 27.3836 - out_caps_acc: 0.9435 - val_loss: 0.0687 - val_out_caps_loss: 0.0551 - val_out_recon_loss: 27.2556 - val_out_caps_acc: 0.9190\n",
            "Epoch 31/150\n",
            "12896/12896 [==============================] - 11s 844us/step - loss: 0.0542 - out_caps_loss: 0.0405 - out_recon_loss: 27.3836 - out_caps_acc: 0.9438 - val_loss: 0.0761 - val_out_caps_loss: 0.0625 - val_out_recon_loss: 27.2564 - val_out_caps_acc: 0.9116\n",
            "Epoch 32/150\n",
            "12896/12896 [==============================] - 11s 836us/step - loss: 0.0540 - out_caps_loss: 0.0403 - out_recon_loss: 27.3835 - out_caps_acc: 0.9436 - val_loss: 0.0678 - val_out_caps_loss: 0.0541 - val_out_recon_loss: 27.2560 - val_out_caps_acc: 0.9190\n",
            "Epoch 33/150\n",
            "12896/12896 [==============================] - 11s 842us/step - loss: 0.0516 - out_caps_loss: 0.0379 - out_recon_loss: 27.3841 - out_caps_acc: 0.9464 - val_loss: 0.0692 - val_out_caps_loss: 0.0556 - val_out_recon_loss: 27.2574 - val_out_caps_acc: 0.9265\n",
            "Epoch 34/150\n",
            "12896/12896 [==============================] - 11s 867us/step - loss: 0.0521 - out_caps_loss: 0.0384 - out_recon_loss: 27.3837 - out_caps_acc: 0.9484 - val_loss: 0.0642 - val_out_caps_loss: 0.0505 - val_out_recon_loss: 27.2561 - val_out_caps_acc: 0.9342\n",
            "Epoch 35/150\n",
            "12896/12896 [==============================] - 12s 903us/step - loss: 0.0514 - out_caps_loss: 0.0377 - out_recon_loss: 27.3838 - out_caps_acc: 0.9461 - val_loss: 0.0705 - val_out_caps_loss: 0.0569 - val_out_recon_loss: 27.2562 - val_out_caps_acc: 0.9159\n",
            "Epoch 36/150\n",
            "12896/12896 [==============================] - 11s 850us/step - loss: 0.0505 - out_caps_loss: 0.0368 - out_recon_loss: 27.3835 - out_caps_acc: 0.9491 - val_loss: 0.0647 - val_out_caps_loss: 0.0511 - val_out_recon_loss: 27.2554 - val_out_caps_acc: 0.9315\n",
            "Epoch 37/150\n",
            "12896/12896 [==============================] - 11s 876us/step - loss: 0.0513 - out_caps_loss: 0.0376 - out_recon_loss: 27.3834 - out_caps_acc: 0.9475 - val_loss: 0.0692 - val_out_caps_loss: 0.0556 - val_out_recon_loss: 27.2557 - val_out_caps_acc: 0.9209\n",
            "Epoch 38/150\n",
            "12896/12896 [==============================] - 11s 874us/step - loss: 0.0504 - out_caps_loss: 0.0367 - out_recon_loss: 27.3834 - out_caps_acc: 0.9480 - val_loss: 0.0673 - val_out_caps_loss: 0.0537 - val_out_recon_loss: 27.2571 - val_out_caps_acc: 0.9256\n",
            "Epoch 39/150\n",
            "12896/12896 [==============================] - 11s 850us/step - loss: 0.0503 - out_caps_loss: 0.0366 - out_recon_loss: 27.3832 - out_caps_acc: 0.9485 - val_loss: 0.0707 - val_out_caps_loss: 0.0571 - val_out_recon_loss: 27.2583 - val_out_caps_acc: 0.9200\n",
            "Epoch 40/150\n",
            "12896/12896 [==============================] - 11s 843us/step - loss: 0.0492 - out_caps_loss: 0.0355 - out_recon_loss: 27.3832 - out_caps_acc: 0.9503 - val_loss: 0.0670 - val_out_caps_loss: 0.0534 - val_out_recon_loss: 27.2568 - val_out_caps_acc: 0.9290\n",
            "Epoch 41/150\n",
            "12896/12896 [==============================] - 11s 850us/step - loss: 0.0479 - out_caps_loss: 0.0342 - out_recon_loss: 27.3834 - out_caps_acc: 0.9521 - val_loss: 0.0717 - val_out_caps_loss: 0.0580 - val_out_recon_loss: 27.2554 - val_out_caps_acc: 0.9163\n",
            "Epoch 42/150\n",
            "12896/12896 [==============================] - 12s 928us/step - loss: 0.0490 - out_caps_loss: 0.0353 - out_recon_loss: 27.3839 - out_caps_acc: 0.9507 - val_loss: 0.0694 - val_out_caps_loss: 0.0558 - val_out_recon_loss: 27.2564 - val_out_caps_acc: 0.9203\n",
            "Epoch 43/150\n",
            "12896/12896 [==============================] - 11s 848us/step - loss: 0.0471 - out_caps_loss: 0.0334 - out_recon_loss: 27.3839 - out_caps_acc: 0.9532 - val_loss: 0.0621 - val_out_caps_loss: 0.0485 - val_out_recon_loss: 27.2547 - val_out_caps_acc: 0.9305\n",
            "Epoch 44/150\n",
            "12896/12896 [==============================] - 11s 841us/step - loss: 0.0465 - out_caps_loss: 0.0328 - out_recon_loss: 27.3833 - out_caps_acc: 0.9551 - val_loss: 0.0652 - val_out_caps_loss: 0.0516 - val_out_recon_loss: 27.2560 - val_out_caps_acc: 0.9293\n",
            "Epoch 45/150\n",
            "12896/12896 [==============================] - 11s 842us/step - loss: 0.0465 - out_caps_loss: 0.0328 - out_recon_loss: 27.3834 - out_caps_acc: 0.9557 - val_loss: 0.0690 - val_out_caps_loss: 0.0554 - val_out_recon_loss: 27.2552 - val_out_caps_acc: 0.9215\n",
            "Epoch 46/150\n",
            "12896/12896 [==============================] - 11s 842us/step - loss: 0.0474 - out_caps_loss: 0.0337 - out_recon_loss: 27.3827 - out_caps_acc: 0.9543 - val_loss: 0.0659 - val_out_caps_loss: 0.0523 - val_out_recon_loss: 27.2561 - val_out_caps_acc: 0.9287\n",
            "Epoch 47/150\n",
            "12896/12896 [==============================] - 11s 871us/step - loss: 0.0452 - out_caps_loss: 0.0315 - out_recon_loss: 27.3836 - out_caps_acc: 0.9579 - val_loss: 0.0633 - val_out_caps_loss: 0.0497 - val_out_recon_loss: 27.2560 - val_out_caps_acc: 0.9315\n",
            "Epoch 48/150\n",
            "12896/12896 [==============================] - 12s 912us/step - loss: 0.0449 - out_caps_loss: 0.0312 - out_recon_loss: 27.3838 - out_caps_acc: 0.9563 - val_loss: 0.0642 - val_out_caps_loss: 0.0506 - val_out_recon_loss: 27.2567 - val_out_caps_acc: 0.9299\n",
            "Epoch 49/150\n",
            "12896/12896 [==============================] - 12s 933us/step - loss: 0.0438 - out_caps_loss: 0.0301 - out_recon_loss: 27.3835 - out_caps_acc: 0.9596 - val_loss: 0.0841 - val_out_caps_loss: 0.0705 - val_out_recon_loss: 27.2570 - val_out_caps_acc: 0.9017\n",
            "Epoch 50/150\n",
            "12896/12896 [==============================] - 11s 847us/step - loss: 0.0450 - out_caps_loss: 0.0313 - out_recon_loss: 27.3837 - out_caps_acc: 0.9562 - val_loss: 0.0620 - val_out_caps_loss: 0.0484 - val_out_recon_loss: 27.2562 - val_out_caps_acc: 0.9305\n",
            "Epoch 51/150\n",
            "12896/12896 [==============================] - 11s 850us/step - loss: 0.0429 - out_caps_loss: 0.0292 - out_recon_loss: 27.3830 - out_caps_acc: 0.9603 - val_loss: 0.0679 - val_out_caps_loss: 0.0543 - val_out_recon_loss: 27.2553 - val_out_caps_acc: 0.9252\n",
            "Epoch 52/150\n",
            "12896/12896 [==============================] - 11s 840us/step - loss: 0.0427 - out_caps_loss: 0.0291 - out_recon_loss: 27.3833 - out_caps_acc: 0.9607 - val_loss: 0.0671 - val_out_caps_loss: 0.0535 - val_out_recon_loss: 27.2561 - val_out_caps_acc: 0.9225\n",
            "Epoch 53/150\n",
            "12896/12896 [==============================] - 11s 843us/step - loss: 0.0433 - out_caps_loss: 0.0296 - out_recon_loss: 27.3833 - out_caps_acc: 0.9596 - val_loss: 0.0630 - val_out_caps_loss: 0.0494 - val_out_recon_loss: 27.2556 - val_out_caps_acc: 0.9315\n",
            "Epoch 54/150\n",
            "12896/12896 [==============================] - 11s 836us/step - loss: 0.0429 - out_caps_loss: 0.0292 - out_recon_loss: 27.3833 - out_caps_acc: 0.9594 - val_loss: 0.0743 - val_out_caps_loss: 0.0607 - val_out_recon_loss: 27.2559 - val_out_caps_acc: 0.9135\n",
            "Epoch 55/150\n",
            "12896/12896 [==============================] - 11s 838us/step - loss: 0.0418 - out_caps_loss: 0.0281 - out_recon_loss: 27.3836 - out_caps_acc: 0.9619 - val_loss: 0.0724 - val_out_caps_loss: 0.0588 - val_out_recon_loss: 27.2563 - val_out_caps_acc: 0.9200\n",
            "Epoch 56/150\n",
            "12896/12896 [==============================] - 12s 900us/step - loss: 0.0416 - out_caps_loss: 0.0279 - out_recon_loss: 27.3837 - out_caps_acc: 0.9641 - val_loss: 0.0614 - val_out_caps_loss: 0.0477 - val_out_recon_loss: 27.2567 - val_out_caps_acc: 0.9361\n",
            "Epoch 57/150\n",
            "12896/12896 [==============================] - 11s 871us/step - loss: 0.0417 - out_caps_loss: 0.0280 - out_recon_loss: 27.3835 - out_caps_acc: 0.9629 - val_loss: 0.0612 - val_out_caps_loss: 0.0475 - val_out_recon_loss: 27.2558 - val_out_caps_acc: 0.9370\n",
            "Epoch 58/150\n",
            "12896/12896 [==============================] - 11s 845us/step - loss: 0.0430 - out_caps_loss: 0.0293 - out_recon_loss: 27.3834 - out_caps_acc: 0.9601 - val_loss: 0.0584 - val_out_caps_loss: 0.0448 - val_out_recon_loss: 27.2560 - val_out_caps_acc: 0.9401\n",
            "Epoch 59/150\n",
            "12896/12896 [==============================] - 11s 847us/step - loss: 0.0394 - out_caps_loss: 0.0257 - out_recon_loss: 27.3835 - out_caps_acc: 0.9658 - val_loss: 0.0581 - val_out_caps_loss: 0.0444 - val_out_recon_loss: 27.2561 - val_out_caps_acc: 0.9411\n",
            "Epoch 60/150\n",
            "12896/12896 [==============================] - 11s 848us/step - loss: 0.0399 - out_caps_loss: 0.0262 - out_recon_loss: 27.3834 - out_caps_acc: 0.9646 - val_loss: 0.0710 - val_out_caps_loss: 0.0574 - val_out_recon_loss: 27.2556 - val_out_caps_acc: 0.9225\n",
            "Epoch 61/150\n",
            "12896/12896 [==============================] - 11s 846us/step - loss: 0.0408 - out_caps_loss: 0.0271 - out_recon_loss: 27.3832 - out_caps_acc: 0.9645 - val_loss: 0.0588 - val_out_caps_loss: 0.0452 - val_out_recon_loss: 27.2559 - val_out_caps_acc: 0.9404\n",
            "Epoch 62/150\n",
            "12896/12896 [==============================] - 11s 852us/step - loss: 0.0403 - out_caps_loss: 0.0266 - out_recon_loss: 27.3833 - out_caps_acc: 0.9653 - val_loss: 0.0610 - val_out_caps_loss: 0.0474 - val_out_recon_loss: 27.2565 - val_out_caps_acc: 0.9330\n",
            "Epoch 63/150\n",
            "12896/12896 [==============================] - 11s 870us/step - loss: 0.0398 - out_caps_loss: 0.0261 - out_recon_loss: 27.3834 - out_caps_acc: 0.9663 - val_loss: 0.0588 - val_out_caps_loss: 0.0451 - val_out_recon_loss: 27.2557 - val_out_caps_acc: 0.9411\n",
            "Epoch 64/150\n",
            "12896/12896 [==============================] - 12s 896us/step - loss: 0.0396 - out_caps_loss: 0.0259 - out_recon_loss: 27.3834 - out_caps_acc: 0.9657 - val_loss: 0.0600 - val_out_caps_loss: 0.0464 - val_out_recon_loss: 27.2560 - val_out_caps_acc: 0.9389\n",
            "Epoch 65/150\n",
            "12896/12896 [==============================] - 12s 893us/step - loss: 0.0399 - out_caps_loss: 0.0262 - out_recon_loss: 27.3832 - out_caps_acc: 0.9656 - val_loss: 0.0615 - val_out_caps_loss: 0.0478 - val_out_recon_loss: 27.2562 - val_out_caps_acc: 0.9342\n",
            "Epoch 66/150\n",
            "12896/12896 [==============================] - 11s 838us/step - loss: 0.0379 - out_caps_loss: 0.0242 - out_recon_loss: 27.3829 - out_caps_acc: 0.9684 - val_loss: 0.0588 - val_out_caps_loss: 0.0452 - val_out_recon_loss: 27.2563 - val_out_caps_acc: 0.9386\n",
            "Epoch 67/150\n",
            "12896/12896 [==============================] - 11s 841us/step - loss: 0.0392 - out_caps_loss: 0.0256 - out_recon_loss: 27.3831 - out_caps_acc: 0.9660 - val_loss: 0.0641 - val_out_caps_loss: 0.0504 - val_out_recon_loss: 27.2560 - val_out_caps_acc: 0.9280\n",
            "Epoch 68/150\n",
            "12896/12896 [==============================] - 11s 830us/step - loss: 0.0383 - out_caps_loss: 0.0246 - out_recon_loss: 27.3829 - out_caps_acc: 0.9674 - val_loss: 0.0681 - val_out_caps_loss: 0.0544 - val_out_recon_loss: 27.2572 - val_out_caps_acc: 0.9218\n",
            "Epoch 69/150\n",
            "12896/12896 [==============================] - 11s 834us/step - loss: 0.0379 - out_caps_loss: 0.0242 - out_recon_loss: 27.3832 - out_caps_acc: 0.9674 - val_loss: 0.0578 - val_out_caps_loss: 0.0442 - val_out_recon_loss: 27.2557 - val_out_caps_acc: 0.9404\n",
            "Epoch 70/150\n",
            "12896/12896 [==============================] - 11s 824us/step - loss: 0.0390 - out_caps_loss: 0.0253 - out_recon_loss: 27.3832 - out_caps_acc: 0.9667 - val_loss: 0.0703 - val_out_caps_loss: 0.0566 - val_out_recon_loss: 27.2554 - val_out_caps_acc: 0.9200\n",
            "Epoch 71/150\n",
            "12896/12896 [==============================] - 12s 920us/step - loss: 0.0376 - out_caps_loss: 0.0239 - out_recon_loss: 27.3835 - out_caps_acc: 0.9679 - val_loss: 0.0595 - val_out_caps_loss: 0.0459 - val_out_recon_loss: 27.2573 - val_out_caps_acc: 0.9380\n",
            "Epoch 72/150\n",
            "12896/12896 [==============================] - 11s 827us/step - loss: 0.0368 - out_caps_loss: 0.0231 - out_recon_loss: 27.3830 - out_caps_acc: 0.9700 - val_loss: 0.0643 - val_out_caps_loss: 0.0507 - val_out_recon_loss: 27.2562 - val_out_caps_acc: 0.9330\n",
            "Epoch 73/150\n",
            "12896/12896 [==============================] - 11s 832us/step - loss: 0.0367 - out_caps_loss: 0.0230 - out_recon_loss: 27.3831 - out_caps_acc: 0.9705 - val_loss: 0.0625 - val_out_caps_loss: 0.0488 - val_out_recon_loss: 27.2597 - val_out_caps_acc: 0.9355\n",
            "Epoch 74/150\n",
            "12896/12896 [==============================] - 11s 826us/step - loss: 0.0385 - out_caps_loss: 0.0249 - out_recon_loss: 27.3830 - out_caps_acc: 0.9674 - val_loss: 0.0568 - val_out_caps_loss: 0.0431 - val_out_recon_loss: 27.2557 - val_out_caps_acc: 0.9454\n",
            "Epoch 75/150\n",
            "12896/12896 [==============================] - 11s 853us/step - loss: 0.0356 - out_caps_loss: 0.0219 - out_recon_loss: 27.3835 - out_caps_acc: 0.9705 - val_loss: 0.0631 - val_out_caps_loss: 0.0494 - val_out_recon_loss: 27.2559 - val_out_caps_acc: 0.9324\n",
            "Epoch 76/150\n",
            "12896/12896 [==============================] - 12s 915us/step - loss: 0.0374 - out_caps_loss: 0.0237 - out_recon_loss: 27.3832 - out_caps_acc: 0.9683 - val_loss: 0.0682 - val_out_caps_loss: 0.0546 - val_out_recon_loss: 27.2563 - val_out_caps_acc: 0.9293\n",
            "Epoch 77/150\n",
            "12896/12896 [==============================] - 11s 818us/step - loss: 0.0368 - out_caps_loss: 0.0231 - out_recon_loss: 27.3827 - out_caps_acc: 0.9687 - val_loss: 0.0590 - val_out_caps_loss: 0.0454 - val_out_recon_loss: 27.2551 - val_out_caps_acc: 0.9367\n",
            "Epoch 78/150\n",
            "12896/12896 [==============================] - 12s 897us/step - loss: 0.0363 - out_caps_loss: 0.0226 - out_recon_loss: 27.3829 - out_caps_acc: 0.9703 - val_loss: 0.0582 - val_out_caps_loss: 0.0446 - val_out_recon_loss: 27.2576 - val_out_caps_acc: 0.9401\n",
            "Epoch 79/150\n",
            "12896/12896 [==============================] - 11s 832us/step - loss: 0.0359 - out_caps_loss: 0.0222 - out_recon_loss: 27.3830 - out_caps_acc: 0.9704 - val_loss: 0.0548 - val_out_caps_loss: 0.0411 - val_out_recon_loss: 27.2558 - val_out_caps_acc: 0.9442\n",
            "Epoch 80/150\n",
            "12896/12896 [==============================] - 11s 821us/step - loss: 0.0370 - out_caps_loss: 0.0233 - out_recon_loss: 27.3833 - out_caps_acc: 0.9684 - val_loss: 0.0672 - val_out_caps_loss: 0.0536 - val_out_recon_loss: 27.2571 - val_out_caps_acc: 0.9274\n",
            "Epoch 81/150\n",
            "12896/12896 [==============================] - 11s 827us/step - loss: 0.0363 - out_caps_loss: 0.0226 - out_recon_loss: 27.3834 - out_caps_acc: 0.9705 - val_loss: 0.0576 - val_out_caps_loss: 0.0440 - val_out_recon_loss: 27.2561 - val_out_caps_acc: 0.9408\n",
            "Epoch 82/150\n",
            "12896/12896 [==============================] - 11s 820us/step - loss: 0.0350 - out_caps_loss: 0.0213 - out_recon_loss: 27.3829 - out_caps_acc: 0.9724 - val_loss: 0.0670 - val_out_caps_loss: 0.0534 - val_out_recon_loss: 27.2575 - val_out_caps_acc: 0.9305\n",
            "Epoch 83/150\n",
            "12896/12896 [==============================] - 11s 822us/step - loss: 0.0343 - out_caps_loss: 0.0206 - out_recon_loss: 27.3835 - out_caps_acc: 0.9739 - val_loss: 0.0590 - val_out_caps_loss: 0.0454 - val_out_recon_loss: 27.2567 - val_out_caps_acc: 0.9398\n",
            "Epoch 84/150\n",
            "12896/12896 [==============================] - 11s 825us/step - loss: 0.0367 - out_caps_loss: 0.0230 - out_recon_loss: 27.3825 - out_caps_acc: 0.9693 - val_loss: 0.0567 - val_out_caps_loss: 0.0431 - val_out_recon_loss: 27.2559 - val_out_caps_acc: 0.9408\n",
            "Epoch 85/150\n",
            "12896/12896 [==============================] - 11s 840us/step - loss: 0.0346 - out_caps_loss: 0.0209 - out_recon_loss: 27.3832 - out_caps_acc: 0.9741 - val_loss: 0.0575 - val_out_caps_loss: 0.0439 - val_out_recon_loss: 27.2558 - val_out_caps_acc: 0.9398\n",
            "Epoch 86/150\n",
            "12896/12896 [==============================] - 12s 898us/step - loss: 0.0345 - out_caps_loss: 0.0208 - out_recon_loss: 27.3830 - out_caps_acc: 0.9722 - val_loss: 0.0727 - val_out_caps_loss: 0.0591 - val_out_recon_loss: 27.2557 - val_out_caps_acc: 0.9221\n",
            "Epoch 87/150\n",
            "12896/12896 [==============================] - 11s 822us/step - loss: 0.0348 - out_caps_loss: 0.0212 - out_recon_loss: 27.3830 - out_caps_acc: 0.9723 - val_loss: 0.0564 - val_out_caps_loss: 0.0428 - val_out_recon_loss: 27.2559 - val_out_caps_acc: 0.9454\n",
            "Epoch 88/150\n",
            "12896/12896 [==============================] - 11s 822us/step - loss: 0.0357 - out_caps_loss: 0.0220 - out_recon_loss: 27.3828 - out_caps_acc: 0.9715 - val_loss: 0.0595 - val_out_caps_loss: 0.0458 - val_out_recon_loss: 27.2558 - val_out_caps_acc: 0.9398\n",
            "Epoch 89/150\n",
            "12896/12896 [==============================] - 11s 817us/step - loss: 0.0330 - out_caps_loss: 0.0193 - out_recon_loss: 27.3830 - out_caps_acc: 0.9747 - val_loss: 0.0617 - val_out_caps_loss: 0.0481 - val_out_recon_loss: 27.2557 - val_out_caps_acc: 0.9315\n",
            "Epoch 90/150\n",
            "12896/12896 [==============================] - 11s 830us/step - loss: 0.0340 - out_caps_loss: 0.0203 - out_recon_loss: 27.3831 - out_caps_acc: 0.9727 - val_loss: 0.0573 - val_out_caps_loss: 0.0437 - val_out_recon_loss: 27.2564 - val_out_caps_acc: 0.9411\n",
            "Epoch 91/150\n",
            "12896/12896 [==============================] - 11s 831us/step - loss: 0.0337 - out_caps_loss: 0.0200 - out_recon_loss: 27.3828 - out_caps_acc: 0.9738 - val_loss: 0.0713 - val_out_caps_loss: 0.0576 - val_out_recon_loss: 27.2561 - val_out_caps_acc: 0.9197\n",
            "Epoch 92/150\n",
            "12896/12896 [==============================] - 11s 832us/step - loss: 0.0340 - out_caps_loss: 0.0203 - out_recon_loss: 27.3832 - out_caps_acc: 0.9736 - val_loss: 0.0608 - val_out_caps_loss: 0.0472 - val_out_recon_loss: 27.2561 - val_out_caps_acc: 0.9364\n",
            "Epoch 93/150\n",
            "12896/12896 [==============================] - 12s 928us/step - loss: 0.0341 - out_caps_loss: 0.0204 - out_recon_loss: 27.3827 - out_caps_acc: 0.9743 - val_loss: 0.0639 - val_out_caps_loss: 0.0502 - val_out_recon_loss: 27.2551 - val_out_caps_acc: 0.9311\n",
            "Epoch 94/150\n",
            "12896/12896 [==============================] - 11s 832us/step - loss: 0.0328 - out_caps_loss: 0.0191 - out_recon_loss: 27.3826 - out_caps_acc: 0.9746 - val_loss: 0.0603 - val_out_caps_loss: 0.0466 - val_out_recon_loss: 27.2558 - val_out_caps_acc: 0.9383\n",
            "Trained model saved to 'trained_model.h5'\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.engine.training.Model at 0x7f3e195affd0>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 62
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fg9RAx4JRoeG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "layer_name = 'out_caps'\n",
        "intermediate_layer_model5 = models.Model(inputs=model5.input[0],\n",
        "                                 outputs=model5.get_layer(layer_name).output)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6esNVtP8YgfK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "y_pred5 = intermediate_layer_model5.predict(x_test_reshape5)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "txJCoZf9Ylr5",
        "colab_type": "code",
        "outputId": "7f23cf1e-78ca-4519-95a0-828a035f67f1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "cm5 = confusion_matrix(y_test5, np.argmax(y_pred5, axis=1))\n",
        "cm5"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[1920,   95],\n",
              "       [ 128, 1887]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 67
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_zniHofB1K_p",
        "colab_type": "text"
      },
      "source": [
        "# Using Combined Data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LwlxnW0XYpBU",
        "colab_type": "code",
        "outputId": "ca1b8410-a3a3-4b65-b3fd-498045a95cc6",
        "colab": {
          "resources": {
            "http://localhost:8080/nbextensions/google.colab/files.js": {
              "data": "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",
              "ok": true,
              "headers": [
                [
                  "content-type",
                  "application/javascript"
                ]
              ],
              "status": 200,
              "status_text": ""
            }
          },
          "base_uri": "https://localhost:8080/",
          "height": 74
        }
      },
      "source": [
        "upload = files.upload()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-9270b3b8-7ef2-4a29-be4c-addf99cef862\" name=\"files[]\" multiple disabled />\n",
              "     <output id=\"result-9270b3b8-7ef2-4a29-be4c-addf99cef862\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script src=\"/nbextensions/google.colab/files.js\"></script> "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Saving Complete_Data_Allergens.csv to Complete_Data_Allergens.csv\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6msMq_im_rYl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pandas as pd\n",
        "import io\n",
        "df = pd.read_csv(io.BytesIO(upload['Complete_Data_Allergens.csv']))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uJkCVn6aBMqO",
        "colab_type": "code",
        "outputId": "3088c407-95f0-4702-e40c-5cae8eee95ce",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 224
        }
      },
      "source": [
        "df.head()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>A</th>\n",
              "      <th>C</th>\n",
              "      <th>D</th>\n",
              "      <th>E</th>\n",
              "      <th>F</th>\n",
              "      <th>G</th>\n",
              "      <th>H</th>\n",
              "      <th>I</th>\n",
              "      <th>K</th>\n",
              "      <th>L</th>\n",
              "      <th>M</th>\n",
              "      <th>N</th>\n",
              "      <th>P</th>\n",
              "      <th>Q</th>\n",
              "      <th>R</th>\n",
              "      <th>S</th>\n",
              "      <th>T</th>\n",
              "      <th>V</th>\n",
              "      <th>W</th>\n",
              "      <th>Y</th>\n",
              "      <th>Labels</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>9.88</td>\n",
              "      <td>3.70</td>\n",
              "      <td>8.02</td>\n",
              "      <td>9.26</td>\n",
              "      <td>3.09</td>\n",
              "      <td>4.32</td>\n",
              "      <td>0.62</td>\n",
              "      <td>3.09</td>\n",
              "      <td>5.56</td>\n",
              "      <td>6.17</td>\n",
              "      <td>1.23</td>\n",
              "      <td>6.17</td>\n",
              "      <td>9.26</td>\n",
              "      <td>4.32</td>\n",
              "      <td>3.09</td>\n",
              "      <td>12.35</td>\n",
              "      <td>6.17</td>\n",
              "      <td>2.47</td>\n",
              "      <td>0.62</td>\n",
              "      <td>0.62</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>6.24</td>\n",
              "      <td>2.72</td>\n",
              "      <td>6.24</td>\n",
              "      <td>4.64</td>\n",
              "      <td>4.64</td>\n",
              "      <td>7.68</td>\n",
              "      <td>2.24</td>\n",
              "      <td>4.32</td>\n",
              "      <td>6.72</td>\n",
              "      <td>11.20</td>\n",
              "      <td>2.24</td>\n",
              "      <td>3.52</td>\n",
              "      <td>5.28</td>\n",
              "      <td>2.72</td>\n",
              "      <td>5.60</td>\n",
              "      <td>7.36</td>\n",
              "      <td>5.92</td>\n",
              "      <td>8.00</td>\n",
              "      <td>1.28</td>\n",
              "      <td>1.44</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>9.94</td>\n",
              "      <td>0.00</td>\n",
              "      <td>3.44</td>\n",
              "      <td>3.63</td>\n",
              "      <td>3.06</td>\n",
              "      <td>7.65</td>\n",
              "      <td>3.06</td>\n",
              "      <td>3.06</td>\n",
              "      <td>3.06</td>\n",
              "      <td>3.44</td>\n",
              "      <td>1.53</td>\n",
              "      <td>7.07</td>\n",
              "      <td>10.13</td>\n",
              "      <td>8.03</td>\n",
              "      <td>5.93</td>\n",
              "      <td>14.15</td>\n",
              "      <td>4.21</td>\n",
              "      <td>5.74</td>\n",
              "      <td>1.15</td>\n",
              "      <td>1.72</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>6.57</td>\n",
              "      <td>2.35</td>\n",
              "      <td>7.04</td>\n",
              "      <td>5.16</td>\n",
              "      <td>2.35</td>\n",
              "      <td>5.16</td>\n",
              "      <td>3.76</td>\n",
              "      <td>4.69</td>\n",
              "      <td>1.88</td>\n",
              "      <td>11.74</td>\n",
              "      <td>2.82</td>\n",
              "      <td>4.23</td>\n",
              "      <td>1.41</td>\n",
              "      <td>7.51</td>\n",
              "      <td>10.80</td>\n",
              "      <td>5.16</td>\n",
              "      <td>5.16</td>\n",
              "      <td>7.98</td>\n",
              "      <td>1.41</td>\n",
              "      <td>2.82</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>5.35</td>\n",
              "      <td>3.38</td>\n",
              "      <td>4.51</td>\n",
              "      <td>5.07</td>\n",
              "      <td>1.41</td>\n",
              "      <td>5.35</td>\n",
              "      <td>2.82</td>\n",
              "      <td>5.35</td>\n",
              "      <td>5.07</td>\n",
              "      <td>8.73</td>\n",
              "      <td>1.41</td>\n",
              "      <td>4.51</td>\n",
              "      <td>5.63</td>\n",
              "      <td>4.79</td>\n",
              "      <td>3.66</td>\n",
              "      <td>8.17</td>\n",
              "      <td>10.42</td>\n",
              "      <td>9.30</td>\n",
              "      <td>2.25</td>\n",
              "      <td>2.82</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "      A     C     D     E     F     G  ...      S      T     V     W     Y  Labels\n",
              "0  9.88  3.70  8.02  9.26  3.09  4.32  ...  12.35   6.17  2.47  0.62  0.62       0\n",
              "1  6.24  2.72  6.24  4.64  4.64  7.68  ...   7.36   5.92  8.00  1.28  1.44       0\n",
              "2  9.94  0.00  3.44  3.63  3.06  7.65  ...  14.15   4.21  5.74  1.15  1.72       0\n",
              "3  6.57  2.35  7.04  5.16  2.35  5.16  ...   5.16   5.16  7.98  1.41  2.82       0\n",
              "4  5.35  3.38  4.51  5.07  1.41  5.35  ...   8.17  10.42  9.30  2.25  2.82       0\n",
              "\n",
              "[5 rows x 21 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 92
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jffcNqopBeuJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "X = df.iloc[:, 0:20].values\n",
        "Y = df.iloc[:, 20].values"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5v-J8YKdB0LN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "x_train, x_test, y_train, y_test = train_test_split(X, Y, test_size = 0.3, stratify = Y, random_state = 42)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Du6tjIcMCVUr",
        "colab_type": "code",
        "outputId": "b2f167fd-994f-4b1a-e59b-a52d18ec0f25",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "import numpy as np\n",
        "unique, counts = np.unique(y_train, return_counts=True)\n",
        "dict(zip(unique, counts))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{0: 7052, 1: 7053}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 100
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F69lAygVCxAK",
        "colab_type": "code",
        "outputId": "24dc8898-7793-4bf3-80e3-56bd9d475c50",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "x_train.shape, x_test.shape"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((14105, 20), (6045, 20))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 101
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ogH1jVLqCfIU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Reshaping the train and test set\n",
        "x_train_reshape = x_train.reshape(14105, 20, 1)\n",
        "y_train_reshape = y_train.reshape(14105, 1)\n",
        "x_test_reshape = x_test.reshape(6045, 20, 1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G6C6Ulb5C9GD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Reshaping the Labels\n",
        "y_train_ = tf.keras.utils.to_categorical(y_train_reshape,num_classes=2)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2dFBFKflDA1L",
        "colab_type": "code",
        "outputId": "af644012-b5e5-4188-86a6-980be2655438",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "model = CapsNet(input_shape=[20, 1],\n",
        "                n_class=2,\n",
        "                num_routing=4)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[20, 1] (?, 20, 1)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kE76sRJ1DFYV",
        "colab_type": "code",
        "outputId": "e28872df-2013-4e76-a77a-98f55d75edb8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 4916
        }
      },
      "source": [
        "train(model=model, data=((x_train_reshape, y_train_)), epoch_size_frac = 0.5)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 11284 samples, validate on 2821 samples\n",
            "Epoch 1/150\n",
            "11284/11284 [==============================] - 13s 1ms/step - loss: 0.1608 - out_caps_loss: 0.1464 - out_recon_loss: 28.7864 - out_caps_acc: 0.7908 - val_loss: 0.1219 - val_out_caps_loss: 0.1082 - val_out_recon_loss: 27.3844 - val_out_caps_acc: 0.8462\n",
            "Epoch 2/150\n",
            "11284/11284 [==============================] - 10s 862us/step - loss: 0.1124 - out_caps_loss: 0.0988 - out_recon_loss: 27.3286 - out_caps_acc: 0.8557 - val_loss: 0.1098 - val_out_caps_loss: 0.0961 - val_out_recon_loss: 27.3699 - val_out_caps_acc: 0.8561\n",
            "Epoch 3/150\n",
            "11284/11284 [==============================] - 11s 949us/step - loss: 0.1042 - out_caps_loss: 0.0905 - out_recon_loss: 27.3105 - out_caps_acc: 0.8704 - val_loss: 0.1109 - val_out_caps_loss: 0.0972 - val_out_recon_loss: 27.3635 - val_out_caps_acc: 0.8547\n",
            "Epoch 4/150\n",
            "11284/11284 [==============================] - 10s 870us/step - loss: 0.0998 - out_caps_loss: 0.0861 - out_recon_loss: 27.3063 - out_caps_acc: 0.8702 - val_loss: 0.1024 - val_out_caps_loss: 0.0887 - val_out_recon_loss: 27.3607 - val_out_caps_acc: 0.8657\n",
            "Epoch 5/150\n",
            "11284/11284 [==============================] - 10s 852us/step - loss: 0.0946 - out_caps_loss: 0.0809 - out_recon_loss: 27.3046 - out_caps_acc: 0.8822 - val_loss: 0.0972 - val_out_caps_loss: 0.0836 - val_out_recon_loss: 27.3594 - val_out_caps_acc: 0.8756\n",
            "Epoch 6/150\n",
            "11284/11284 [==============================] - 10s 847us/step - loss: 0.0919 - out_caps_loss: 0.0783 - out_recon_loss: 27.3037 - out_caps_acc: 0.8847 - val_loss: 0.1010 - val_out_caps_loss: 0.0874 - val_out_recon_loss: 27.3604 - val_out_caps_acc: 0.8703\n",
            "Epoch 7/150\n",
            "11284/11284 [==============================] - 10s 847us/step - loss: 0.0893 - out_caps_loss: 0.0756 - out_recon_loss: 27.3028 - out_caps_acc: 0.8907 - val_loss: 0.0994 - val_out_caps_loss: 0.0857 - val_out_recon_loss: 27.3586 - val_out_caps_acc: 0.8773\n",
            "Epoch 8/150\n",
            "11284/11284 [==============================] - 10s 847us/step - loss: 0.0864 - out_caps_loss: 0.0727 - out_recon_loss: 27.3025 - out_caps_acc: 0.8945 - val_loss: 0.0902 - val_out_caps_loss: 0.0766 - val_out_recon_loss: 27.3576 - val_out_caps_acc: 0.8894\n",
            "Epoch 9/150\n",
            "11284/11284 [==============================] - 9s 836us/step - loss: 0.0826 - out_caps_loss: 0.0689 - out_recon_loss: 27.3021 - out_caps_acc: 0.8987 - val_loss: 0.0882 - val_out_caps_loss: 0.0745 - val_out_recon_loss: 27.3581 - val_out_caps_acc: 0.8848\n",
            "Epoch 10/150\n",
            "11284/11284 [==============================] - 10s 846us/step - loss: 0.0809 - out_caps_loss: 0.0672 - out_recon_loss: 27.3018 - out_caps_acc: 0.9021 - val_loss: 0.0871 - val_out_caps_loss: 0.0734 - val_out_recon_loss: 27.3570 - val_out_caps_acc: 0.8933\n",
            "Epoch 11/150\n",
            "11284/11284 [==============================] - 10s 911us/step - loss: 0.0782 - out_caps_loss: 0.0646 - out_recon_loss: 27.3015 - out_caps_acc: 0.9054 - val_loss: 0.0856 - val_out_caps_loss: 0.0719 - val_out_recon_loss: 27.3569 - val_out_caps_acc: 0.8940\n",
            "Epoch 12/150\n",
            "11284/11284 [==============================] - 10s 908us/step - loss: 0.0765 - out_caps_loss: 0.0629 - out_recon_loss: 27.3010 - out_caps_acc: 0.9067 - val_loss: 0.0897 - val_out_caps_loss: 0.0761 - val_out_recon_loss: 27.3573 - val_out_caps_acc: 0.8919\n",
            "Epoch 13/150\n",
            "11284/11284 [==============================] - 9s 840us/step - loss: 0.0747 - out_caps_loss: 0.0611 - out_recon_loss: 27.3007 - out_caps_acc: 0.9089 - val_loss: 0.0830 - val_out_caps_loss: 0.0693 - val_out_recon_loss: 27.3562 - val_out_caps_acc: 0.8922\n",
            "Epoch 14/150\n",
            "11284/11284 [==============================] - 10s 847us/step - loss: 0.0729 - out_caps_loss: 0.0593 - out_recon_loss: 27.3006 - out_caps_acc: 0.9116 - val_loss: 0.0842 - val_out_caps_loss: 0.0705 - val_out_recon_loss: 27.3574 - val_out_caps_acc: 0.8922\n",
            "Epoch 15/150\n",
            "11284/11284 [==============================] - 10s 848us/step - loss: 0.0713 - out_caps_loss: 0.0576 - out_recon_loss: 27.3003 - out_caps_acc: 0.9143 - val_loss: 0.0823 - val_out_caps_loss: 0.0686 - val_out_recon_loss: 27.3563 - val_out_caps_acc: 0.8997\n",
            "Epoch 16/150\n",
            "11284/11284 [==============================] - 11s 970us/step - loss: 0.0703 - out_caps_loss: 0.0566 - out_recon_loss: 27.3002 - out_caps_acc: 0.9146 - val_loss: 0.0775 - val_out_caps_loss: 0.0638 - val_out_recon_loss: 27.3558 - val_out_caps_acc: 0.9036\n",
            "Epoch 17/150\n",
            "11284/11284 [==============================] - 10s 844us/step - loss: 0.0681 - out_caps_loss: 0.0545 - out_recon_loss: 27.2998 - out_caps_acc: 0.9198 - val_loss: 0.0761 - val_out_caps_loss: 0.0625 - val_out_recon_loss: 27.3564 - val_out_caps_acc: 0.9022\n",
            "Epoch 18/150\n",
            "11284/11284 [==============================] - 10s 843us/step - loss: 0.0656 - out_caps_loss: 0.0519 - out_recon_loss: 27.2997 - out_caps_acc: 0.9222 - val_loss: 0.0831 - val_out_caps_loss: 0.0694 - val_out_recon_loss: 27.3557 - val_out_caps_acc: 0.8976\n",
            "Epoch 19/150\n",
            "11284/11284 [==============================] - 10s 892us/step - loss: 0.0657 - out_caps_loss: 0.0520 - out_recon_loss: 27.2995 - out_caps_acc: 0.9230 - val_loss: 0.0789 - val_out_caps_loss: 0.0652 - val_out_recon_loss: 27.3559 - val_out_caps_acc: 0.9025\n",
            "Epoch 20/150\n",
            "11284/11284 [==============================] - 10s 909us/step - loss: 0.0667 - out_caps_loss: 0.0531 - out_recon_loss: 27.2997 - out_caps_acc: 0.9210 - val_loss: 0.0759 - val_out_caps_loss: 0.0622 - val_out_recon_loss: 27.3556 - val_out_caps_acc: 0.9117\n",
            "Epoch 21/150\n",
            "11284/11284 [==============================] - 9s 834us/step - loss: 0.0640 - out_caps_loss: 0.0504 - out_recon_loss: 27.2994 - out_caps_acc: 0.9267 - val_loss: 0.0739 - val_out_caps_loss: 0.0603 - val_out_recon_loss: 27.3551 - val_out_caps_acc: 0.9142\n",
            "Epoch 22/150\n",
            "11284/11284 [==============================] - 9s 841us/step - loss: 0.0627 - out_caps_loss: 0.0491 - out_recon_loss: 27.2992 - out_caps_acc: 0.9272 - val_loss: 0.0758 - val_out_caps_loss: 0.0622 - val_out_recon_loss: 27.3551 - val_out_caps_acc: 0.9068\n",
            "Epoch 23/150\n",
            "11284/11284 [==============================] - 10s 842us/step - loss: 0.0621 - out_caps_loss: 0.0484 - out_recon_loss: 27.2992 - out_caps_acc: 0.9297 - val_loss: 0.0721 - val_out_caps_loss: 0.0584 - val_out_recon_loss: 27.3556 - val_out_caps_acc: 0.9082\n",
            "Epoch 24/150\n",
            "11284/11284 [==============================] - 9s 833us/step - loss: 0.0606 - out_caps_loss: 0.0469 - out_recon_loss: 27.2987 - out_caps_acc: 0.9316 - val_loss: 0.0777 - val_out_caps_loss: 0.0640 - val_out_recon_loss: 27.3574 - val_out_caps_acc: 0.9043\n",
            "Epoch 25/150\n",
            "11284/11284 [==============================] - 10s 847us/step - loss: 0.0605 - out_caps_loss: 0.0469 - out_recon_loss: 27.2980 - out_caps_acc: 0.9316 - val_loss: 0.0723 - val_out_caps_loss: 0.0587 - val_out_recon_loss: 27.3548 - val_out_caps_acc: 0.9135\n",
            "Epoch 26/150\n",
            "11284/11284 [==============================] - 9s 841us/step - loss: 0.0591 - out_caps_loss: 0.0454 - out_recon_loss: 27.2976 - out_caps_acc: 0.9343 - val_loss: 0.0710 - val_out_caps_loss: 0.0574 - val_out_recon_loss: 27.3531 - val_out_caps_acc: 0.9199\n",
            "Epoch 27/150\n",
            "11284/11284 [==============================] - 10s 849us/step - loss: 0.0586 - out_caps_loss: 0.0449 - out_recon_loss: 27.2975 - out_caps_acc: 0.9357 - val_loss: 0.0932 - val_out_caps_loss: 0.0795 - val_out_recon_loss: 27.3549 - val_out_caps_acc: 0.8837\n",
            "Epoch 28/150\n",
            "11284/11284 [==============================] - 11s 951us/step - loss: 0.0581 - out_caps_loss: 0.0445 - out_recon_loss: 27.2975 - out_caps_acc: 0.9351 - val_loss: 0.0716 - val_out_caps_loss: 0.0580 - val_out_recon_loss: 27.3541 - val_out_caps_acc: 0.9132\n",
            "Epoch 29/150\n",
            "11284/11284 [==============================] - 9s 842us/step - loss: 0.0569 - out_caps_loss: 0.0433 - out_recon_loss: 27.2973 - out_caps_acc: 0.9370 - val_loss: 0.0719 - val_out_caps_loss: 0.0582 - val_out_recon_loss: 27.3546 - val_out_caps_acc: 0.9171\n",
            "Epoch 30/150\n",
            "11284/11284 [==============================] - 9s 837us/step - loss: 0.0552 - out_caps_loss: 0.0416 - out_recon_loss: 27.2972 - out_caps_acc: 0.9428 - val_loss: 0.0695 - val_out_caps_loss: 0.0558 - val_out_recon_loss: 27.3543 - val_out_caps_acc: 0.9195\n",
            "Epoch 31/150\n",
            "11284/11284 [==============================] - 9s 839us/step - loss: 0.0563 - out_caps_loss: 0.0427 - out_recon_loss: 27.2971 - out_caps_acc: 0.9387 - val_loss: 0.0709 - val_out_caps_loss: 0.0573 - val_out_recon_loss: 27.3538 - val_out_caps_acc: 0.9178\n",
            "Epoch 32/150\n",
            "11284/11284 [==============================] - 9s 838us/step - loss: 0.0541 - out_caps_loss: 0.0405 - out_recon_loss: 27.2973 - out_caps_acc: 0.9407 - val_loss: 0.0675 - val_out_caps_loss: 0.0539 - val_out_recon_loss: 27.3538 - val_out_caps_acc: 0.9206\n",
            "Epoch 33/150\n",
            "11284/11284 [==============================] - 9s 839us/step - loss: 0.0545 - out_caps_loss: 0.0409 - out_recon_loss: 27.2970 - out_caps_acc: 0.9411 - val_loss: 0.0664 - val_out_caps_loss: 0.0527 - val_out_recon_loss: 27.3528 - val_out_caps_acc: 0.9280\n",
            "Epoch 34/150\n",
            "11284/11284 [==============================] - 9s 839us/step - loss: 0.0524 - out_caps_loss: 0.0388 - out_recon_loss: 27.2970 - out_caps_acc: 0.9442 - val_loss: 0.0649 - val_out_caps_loss: 0.0512 - val_out_recon_loss: 27.3536 - val_out_caps_acc: 0.9259\n",
            "Epoch 35/150\n",
            "11284/11284 [==============================] - 9s 839us/step - loss: 0.0524 - out_caps_loss: 0.0388 - out_recon_loss: 27.2971 - out_caps_acc: 0.9447 - val_loss: 0.0700 - val_out_caps_loss: 0.0563 - val_out_recon_loss: 27.3538 - val_out_caps_acc: 0.9195\n",
            "Epoch 36/150\n",
            "11284/11284 [==============================] - 10s 927us/step - loss: 0.0513 - out_caps_loss: 0.0377 - out_recon_loss: 27.2968 - out_caps_acc: 0.9460 - val_loss: 0.0663 - val_out_caps_loss: 0.0526 - val_out_recon_loss: 27.3548 - val_out_caps_acc: 0.9280\n",
            "Epoch 37/150\n",
            "11284/11284 [==============================] - 10s 862us/step - loss: 0.0512 - out_caps_loss: 0.0375 - out_recon_loss: 27.2969 - out_caps_acc: 0.9450 - val_loss: 0.0696 - val_out_caps_loss: 0.0559 - val_out_recon_loss: 27.3536 - val_out_caps_acc: 0.9174\n",
            "Epoch 38/150\n",
            "11284/11284 [==============================] - 9s 839us/step - loss: 0.0512 - out_caps_loss: 0.0376 - out_recon_loss: 27.2967 - out_caps_acc: 0.9467 - val_loss: 0.0848 - val_out_caps_loss: 0.0712 - val_out_recon_loss: 27.3531 - val_out_caps_acc: 0.8947\n",
            "Epoch 39/150\n",
            "11284/11284 [==============================] - 9s 834us/step - loss: 0.0505 - out_caps_loss: 0.0369 - out_recon_loss: 27.2968 - out_caps_acc: 0.9482 - val_loss: 0.0668 - val_out_caps_loss: 0.0531 - val_out_recon_loss: 27.3529 - val_out_caps_acc: 0.9266\n",
            "Epoch 40/150\n",
            "11284/11284 [==============================] - 9s 839us/step - loss: 0.0496 - out_caps_loss: 0.0360 - out_recon_loss: 27.2973 - out_caps_acc: 0.9475 - val_loss: 0.0662 - val_out_caps_loss: 0.0525 - val_out_recon_loss: 27.3528 - val_out_caps_acc: 0.9199\n",
            "Epoch 41/150\n",
            "11284/11284 [==============================] - 9s 841us/step - loss: 0.0491 - out_caps_loss: 0.0354 - out_recon_loss: 27.2966 - out_caps_acc: 0.9490 - val_loss: 0.0681 - val_out_caps_loss: 0.0544 - val_out_recon_loss: 27.3531 - val_out_caps_acc: 0.9206\n",
            "Epoch 42/150\n",
            "11284/11284 [==============================] - 9s 837us/step - loss: 0.0492 - out_caps_loss: 0.0355 - out_recon_loss: 27.2969 - out_caps_acc: 0.9498 - val_loss: 0.0686 - val_out_caps_loss: 0.0549 - val_out_recon_loss: 27.3535 - val_out_caps_acc: 0.9153\n",
            "Epoch 43/150\n",
            "11284/11284 [==============================] - 10s 908us/step - loss: 0.0467 - out_caps_loss: 0.0330 - out_recon_loss: 27.2968 - out_caps_acc: 0.9544 - val_loss: 0.0636 - val_out_caps_loss: 0.0499 - val_out_recon_loss: 27.3532 - val_out_caps_acc: 0.9245\n",
            "Epoch 44/150\n",
            "11284/11284 [==============================] - 10s 893us/step - loss: 0.0475 - out_caps_loss: 0.0338 - out_recon_loss: 27.2969 - out_caps_acc: 0.9514 - val_loss: 0.0655 - val_out_caps_loss: 0.0518 - val_out_recon_loss: 27.3534 - val_out_caps_acc: 0.9252\n",
            "Epoch 45/150\n",
            "11284/11284 [==============================] - 10s 898us/step - loss: 0.0465 - out_caps_loss: 0.0329 - out_recon_loss: 27.2972 - out_caps_acc: 0.9545 - val_loss: 0.0642 - val_out_caps_loss: 0.0505 - val_out_recon_loss: 27.3534 - val_out_caps_acc: 0.9309\n",
            "Epoch 46/150\n",
            "11284/11284 [==============================] - 9s 841us/step - loss: 0.0466 - out_caps_loss: 0.0330 - out_recon_loss: 27.2971 - out_caps_acc: 0.9538 - val_loss: 0.0635 - val_out_caps_loss: 0.0498 - val_out_recon_loss: 27.3540 - val_out_caps_acc: 0.9316\n",
            "Epoch 47/150\n",
            "11284/11284 [==============================] - 10s 860us/step - loss: 0.0452 - out_caps_loss: 0.0315 - out_recon_loss: 27.2974 - out_caps_acc: 0.9556 - val_loss: 0.0668 - val_out_caps_loss: 0.0531 - val_out_recon_loss: 27.3539 - val_out_caps_acc: 0.9188\n",
            "Epoch 48/150\n",
            "11284/11284 [==============================] - 11s 963us/step - loss: 0.0465 - out_caps_loss: 0.0329 - out_recon_loss: 27.2969 - out_caps_acc: 0.9528 - val_loss: 0.0692 - val_out_caps_loss: 0.0555 - val_out_recon_loss: 27.3531 - val_out_caps_acc: 0.9231\n",
            "Epoch 49/150\n",
            "11284/11284 [==============================] - 9s 840us/step - loss: 0.0454 - out_caps_loss: 0.0318 - out_recon_loss: 27.2966 - out_caps_acc: 0.9559 - val_loss: 0.0662 - val_out_caps_loss: 0.0525 - val_out_recon_loss: 27.3534 - val_out_caps_acc: 0.9263\n",
            "Epoch 50/150\n",
            "11284/11284 [==============================] - 9s 840us/step - loss: 0.0450 - out_caps_loss: 0.0313 - out_recon_loss: 27.2969 - out_caps_acc: 0.9564 - val_loss: 0.0646 - val_out_caps_loss: 0.0509 - val_out_recon_loss: 27.3532 - val_out_caps_acc: 0.9273\n",
            "Epoch 51/150\n",
            "11284/11284 [==============================] - 9s 836us/step - loss: 0.0431 - out_caps_loss: 0.0295 - out_recon_loss: 27.2967 - out_caps_acc: 0.9607 - val_loss: 0.0632 - val_out_caps_loss: 0.0496 - val_out_recon_loss: 27.3530 - val_out_caps_acc: 0.9291\n",
            "Epoch 52/150\n",
            "11284/11284 [==============================] - 10s 874us/step - loss: 0.0446 - out_caps_loss: 0.0310 - out_recon_loss: 27.2966 - out_caps_acc: 0.9578 - val_loss: 0.0651 - val_out_caps_loss: 0.0514 - val_out_recon_loss: 27.3545 - val_out_caps_acc: 0.9280\n",
            "Epoch 53/150\n",
            "11284/11284 [==============================] - 10s 927us/step - loss: 0.0428 - out_caps_loss: 0.0292 - out_recon_loss: 27.2971 - out_caps_acc: 0.9602 - val_loss: 0.0725 - val_out_caps_loss: 0.0589 - val_out_recon_loss: 27.3535 - val_out_caps_acc: 0.9139\n",
            "Epoch 54/150\n",
            "11284/11284 [==============================] - 9s 834us/step - loss: 0.0436 - out_caps_loss: 0.0300 - out_recon_loss: 27.2967 - out_caps_acc: 0.9588 - val_loss: 0.0642 - val_out_caps_loss: 0.0505 - val_out_recon_loss: 27.3529 - val_out_caps_acc: 0.9273\n",
            "Epoch 55/150\n",
            "11284/11284 [==============================] - 9s 837us/step - loss: 0.0433 - out_caps_loss: 0.0297 - out_recon_loss: 27.2967 - out_caps_acc: 0.9576 - val_loss: 0.0620 - val_out_caps_loss: 0.0483 - val_out_recon_loss: 27.3529 - val_out_caps_acc: 0.9302\n",
            "Epoch 56/150\n",
            "11284/11284 [==============================] - 9s 838us/step - loss: 0.0432 - out_caps_loss: 0.0295 - out_recon_loss: 27.2967 - out_caps_acc: 0.9602 - val_loss: 0.0638 - val_out_caps_loss: 0.0502 - val_out_recon_loss: 27.3525 - val_out_caps_acc: 0.9316\n",
            "Epoch 57/150\n",
            "11284/11284 [==============================] - 9s 835us/step - loss: 0.0417 - out_caps_loss: 0.0280 - out_recon_loss: 27.2968 - out_caps_acc: 0.9608 - val_loss: 0.0713 - val_out_caps_loss: 0.0576 - val_out_recon_loss: 27.3542 - val_out_caps_acc: 0.9160\n",
            "Epoch 58/150\n",
            "11284/11284 [==============================] - 9s 838us/step - loss: 0.0424 - out_caps_loss: 0.0288 - out_recon_loss: 27.2968 - out_caps_acc: 0.9604 - val_loss: 0.0628 - val_out_caps_loss: 0.0491 - val_out_recon_loss: 27.3535 - val_out_caps_acc: 0.9302\n",
            "Epoch 59/150\n",
            "11284/11284 [==============================] - 9s 837us/step - loss: 0.0415 - out_caps_loss: 0.0279 - out_recon_loss: 27.2964 - out_caps_acc: 0.9622 - val_loss: 0.0705 - val_out_caps_loss: 0.0568 - val_out_recon_loss: 27.3537 - val_out_caps_acc: 0.9192\n",
            "Epoch 60/150\n",
            "11284/11284 [==============================] - 9s 832us/step - loss: 0.0395 - out_caps_loss: 0.0259 - out_recon_loss: 27.2966 - out_caps_acc: 0.9644 - val_loss: 0.0631 - val_out_caps_loss: 0.0494 - val_out_recon_loss: 27.3530 - val_out_caps_acc: 0.9284\n",
            "Epoch 61/150\n",
            "11284/11284 [==============================] - 11s 955us/step - loss: 0.0415 - out_caps_loss: 0.0278 - out_recon_loss: 27.2966 - out_caps_acc: 0.9621 - val_loss: 0.0673 - val_out_caps_loss: 0.0536 - val_out_recon_loss: 27.3538 - val_out_caps_acc: 0.9206\n",
            "Epoch 62/150\n",
            "11284/11284 [==============================] - 10s 843us/step - loss: 0.0406 - out_caps_loss: 0.0270 - out_recon_loss: 27.2971 - out_caps_acc: 0.9638 - val_loss: 0.0637 - val_out_caps_loss: 0.0501 - val_out_recon_loss: 27.3533 - val_out_caps_acc: 0.9280\n",
            "Epoch 63/150\n",
            "11284/11284 [==============================] - 9s 835us/step - loss: 0.0409 - out_caps_loss: 0.0272 - out_recon_loss: 27.2973 - out_caps_acc: 0.9633 - val_loss: 0.0664 - val_out_caps_loss: 0.0527 - val_out_recon_loss: 27.3534 - val_out_caps_acc: 0.9256\n",
            "Epoch 64/150\n",
            "11284/11284 [==============================] - 9s 838us/step - loss: 0.0407 - out_caps_loss: 0.0271 - out_recon_loss: 27.2972 - out_caps_acc: 0.9630 - val_loss: 0.0638 - val_out_caps_loss: 0.0502 - val_out_recon_loss: 27.3536 - val_out_caps_acc: 0.9287\n",
            "Epoch 65/150\n",
            "11284/11284 [==============================] - 9s 837us/step - loss: 0.0384 - out_caps_loss: 0.0247 - out_recon_loss: 27.2971 - out_caps_acc: 0.9670 - val_loss: 0.0626 - val_out_caps_loss: 0.0489 - val_out_recon_loss: 27.3532 - val_out_caps_acc: 0.9316\n",
            "Epoch 66/150\n",
            "11284/11284 [==============================] - 9s 831us/step - loss: 0.0420 - out_caps_loss: 0.0283 - out_recon_loss: 27.2966 - out_caps_acc: 0.9618 - val_loss: 0.0600 - val_out_caps_loss: 0.0464 - val_out_recon_loss: 27.3528 - val_out_caps_acc: 0.9358\n",
            "Epoch 67/150\n",
            "11284/11284 [==============================] - 9s 835us/step - loss: 0.0400 - out_caps_loss: 0.0264 - out_recon_loss: 27.2967 - out_caps_acc: 0.9627 - val_loss: 0.0646 - val_out_caps_loss: 0.0510 - val_out_recon_loss: 27.3529 - val_out_caps_acc: 0.9287\n",
            "Epoch 68/150\n",
            "11284/11284 [==============================] - 9s 839us/step - loss: 0.0387 - out_caps_loss: 0.0251 - out_recon_loss: 27.2967 - out_caps_acc: 0.9669 - val_loss: 0.0623 - val_out_caps_loss: 0.0486 - val_out_recon_loss: 27.3523 - val_out_caps_acc: 0.9326\n",
            "Epoch 69/150\n",
            "11284/11284 [==============================] - 10s 898us/step - loss: 0.0402 - out_caps_loss: 0.0266 - out_recon_loss: 27.2963 - out_caps_acc: 0.9639 - val_loss: 0.0616 - val_out_caps_loss: 0.0479 - val_out_recon_loss: 27.3526 - val_out_caps_acc: 0.9326\n",
            "Epoch 70/150\n",
            "11284/11284 [==============================] - 10s 891us/step - loss: 0.0370 - out_caps_loss: 0.0233 - out_recon_loss: 27.2964 - out_caps_acc: 0.9689 - val_loss: 0.0611 - val_out_caps_loss: 0.0474 - val_out_recon_loss: 27.3525 - val_out_caps_acc: 0.9298\n",
            "Epoch 71/150\n",
            "11284/11284 [==============================] - 9s 832us/step - loss: 0.0375 - out_caps_loss: 0.0239 - out_recon_loss: 27.2968 - out_caps_acc: 0.9692 - val_loss: 0.0624 - val_out_caps_loss: 0.0487 - val_out_recon_loss: 27.3541 - val_out_caps_acc: 0.9341\n",
            "Epoch 72/150\n",
            "11284/11284 [==============================] - 9s 832us/step - loss: 0.0383 - out_caps_loss: 0.0246 - out_recon_loss: 27.2973 - out_caps_acc: 0.9669 - val_loss: 0.0614 - val_out_caps_loss: 0.0477 - val_out_recon_loss: 27.3524 - val_out_caps_acc: 0.9337\n",
            "Epoch 73/150\n",
            "11284/11284 [==============================] - 9s 840us/step - loss: 0.0366 - out_caps_loss: 0.0229 - out_recon_loss: 27.2964 - out_caps_acc: 0.9693 - val_loss: 0.0633 - val_out_caps_loss: 0.0497 - val_out_recon_loss: 27.3533 - val_out_caps_acc: 0.9337\n",
            "Epoch 74/150\n",
            "11284/11284 [==============================] - 9s 835us/step - loss: 0.0379 - out_caps_loss: 0.0242 - out_recon_loss: 27.2963 - out_caps_acc: 0.9672 - val_loss: 0.0600 - val_out_caps_loss: 0.0463 - val_out_recon_loss: 27.3526 - val_out_caps_acc: 0.9362\n",
            "Epoch 75/150\n",
            "11284/11284 [==============================] - 10s 906us/step - loss: 0.0368 - out_caps_loss: 0.0232 - out_recon_loss: 27.2968 - out_caps_acc: 0.9693 - val_loss: 0.0609 - val_out_caps_loss: 0.0472 - val_out_recon_loss: 27.3530 - val_out_caps_acc: 0.9362\n",
            "Epoch 76/150\n",
            "11284/11284 [==============================] - 9s 842us/step - loss: 0.0371 - out_caps_loss: 0.0234 - out_recon_loss: 27.2965 - out_caps_acc: 0.9667 - val_loss: 0.0598 - val_out_caps_loss: 0.0461 - val_out_recon_loss: 27.3536 - val_out_caps_acc: 0.9404\n",
            "Epoch 77/150\n",
            "11284/11284 [==============================] - 10s 868us/step - loss: 0.0351 - out_caps_loss: 0.0215 - out_recon_loss: 27.2966 - out_caps_acc: 0.9717 - val_loss: 0.0587 - val_out_caps_loss: 0.0450 - val_out_recon_loss: 27.3533 - val_out_caps_acc: 0.9362\n",
            "Epoch 78/150\n",
            "11284/11284 [==============================] - 10s 928us/step - loss: 0.0359 - out_caps_loss: 0.0223 - out_recon_loss: 27.2966 - out_caps_acc: 0.9702 - val_loss: 0.0615 - val_out_caps_loss: 0.0478 - val_out_recon_loss: 27.3535 - val_out_caps_acc: 0.9312\n",
            "Epoch 79/150\n",
            "11284/11284 [==============================] - 10s 851us/step - loss: 0.0376 - out_caps_loss: 0.0240 - out_recon_loss: 27.2961 - out_caps_acc: 0.9672 - val_loss: 0.0591 - val_out_caps_loss: 0.0454 - val_out_recon_loss: 27.3520 - val_out_caps_acc: 0.9404\n",
            "Epoch 80/150\n",
            "11284/11284 [==============================] - 11s 971us/step - loss: 0.0358 - out_caps_loss: 0.0221 - out_recon_loss: 27.2970 - out_caps_acc: 0.9708 - val_loss: 0.0560 - val_out_caps_loss: 0.0424 - val_out_recon_loss: 27.3533 - val_out_caps_acc: 0.9415\n",
            "Epoch 81/150\n",
            "11284/11284 [==============================] - 9s 834us/step - loss: 0.0349 - out_caps_loss: 0.0213 - out_recon_loss: 27.2968 - out_caps_acc: 0.9718 - val_loss: 0.0600 - val_out_caps_loss: 0.0463 - val_out_recon_loss: 27.3534 - val_out_caps_acc: 0.9394\n",
            "Epoch 82/150\n",
            "11284/11284 [==============================] - 9s 837us/step - loss: 0.0365 - out_caps_loss: 0.0228 - out_recon_loss: 27.2961 - out_caps_acc: 0.9692 - val_loss: 0.0773 - val_out_caps_loss: 0.0636 - val_out_recon_loss: 27.3532 - val_out_caps_acc: 0.9107\n",
            "Epoch 83/150\n",
            "11284/11284 [==============================] - 9s 839us/step - loss: 0.0358 - out_caps_loss: 0.0222 - out_recon_loss: 27.2967 - out_caps_acc: 0.9708 - val_loss: 0.0597 - val_out_caps_loss: 0.0460 - val_out_recon_loss: 27.3529 - val_out_caps_acc: 0.9362\n",
            "Epoch 84/150\n",
            "11284/11284 [==============================] - 9s 835us/step - loss: 0.0350 - out_caps_loss: 0.0213 - out_recon_loss: 27.2966 - out_caps_acc: 0.9720 - val_loss: 0.0627 - val_out_caps_loss: 0.0491 - val_out_recon_loss: 27.3531 - val_out_caps_acc: 0.9337\n",
            "Epoch 85/150\n",
            "11284/11284 [==============================] - 10s 843us/step - loss: 0.0341 - out_caps_loss: 0.0205 - out_recon_loss: 27.2967 - out_caps_acc: 0.9722 - val_loss: 0.0705 - val_out_caps_loss: 0.0569 - val_out_recon_loss: 27.3526 - val_out_caps_acc: 0.9185\n",
            "Epoch 86/150\n",
            "11284/11284 [==============================] - 11s 957us/step - loss: 0.0341 - out_caps_loss: 0.0205 - out_recon_loss: 27.2968 - out_caps_acc: 0.9711 - val_loss: 0.0592 - val_out_caps_loss: 0.0456 - val_out_recon_loss: 27.3533 - val_out_caps_acc: 0.9373\n",
            "Epoch 87/150\n",
            "11284/11284 [==============================] - 9s 833us/step - loss: 0.0362 - out_caps_loss: 0.0225 - out_recon_loss: 27.2968 - out_caps_acc: 0.9704 - val_loss: 0.0677 - val_out_caps_loss: 0.0541 - val_out_recon_loss: 27.3537 - val_out_caps_acc: 0.9245\n",
            "Epoch 88/150\n",
            "11284/11284 [==============================] - 9s 833us/step - loss: 0.0334 - out_caps_loss: 0.0198 - out_recon_loss: 27.2968 - out_caps_acc: 0.9740 - val_loss: 0.0607 - val_out_caps_loss: 0.0470 - val_out_recon_loss: 27.3534 - val_out_caps_acc: 0.9365\n",
            "Epoch 89/150\n",
            "11284/11284 [==============================] - 9s 833us/step - loss: 0.0344 - out_caps_loss: 0.0207 - out_recon_loss: 27.2966 - out_caps_acc: 0.9714 - val_loss: 0.0801 - val_out_caps_loss: 0.0665 - val_out_recon_loss: 27.3529 - val_out_caps_acc: 0.9089\n",
            "Epoch 90/150\n",
            "11284/11284 [==============================] - 9s 831us/step - loss: 0.0343 - out_caps_loss: 0.0206 - out_recon_loss: 27.2962 - out_caps_acc: 0.9728 - val_loss: 0.0659 - val_out_caps_loss: 0.0522 - val_out_recon_loss: 27.3524 - val_out_caps_acc: 0.9277\n",
            "Epoch 91/150\n",
            "11284/11284 [==============================] - 9s 840us/step - loss: 0.0345 - out_caps_loss: 0.0208 - out_recon_loss: 27.2966 - out_caps_acc: 0.9721 - val_loss: 0.0647 - val_out_caps_loss: 0.0510 - val_out_recon_loss: 27.3540 - val_out_caps_acc: 0.9270\n",
            "Epoch 92/150\n",
            "11284/11284 [==============================] - 9s 837us/step - loss: 0.0338 - out_caps_loss: 0.0201 - out_recon_loss: 27.2968 - out_caps_acc: 0.9747 - val_loss: 0.0609 - val_out_caps_loss: 0.0472 - val_out_recon_loss: 27.3531 - val_out_caps_acc: 0.9330\n",
            "Epoch 93/150\n",
            "11284/11284 [==============================] - 9s 836us/step - loss: 0.0329 - out_caps_loss: 0.0193 - out_recon_loss: 27.2968 - out_caps_acc: 0.9751 - val_loss: 0.0612 - val_out_caps_loss: 0.0475 - val_out_recon_loss: 27.3527 - val_out_caps_acc: 0.9319\n",
            "Epoch 94/150\n",
            "11284/11284 [==============================] - 10s 921us/step - loss: 0.0334 - out_caps_loss: 0.0198 - out_recon_loss: 27.2961 - out_caps_acc: 0.9739 - val_loss: 0.0572 - val_out_caps_loss: 0.0436 - val_out_recon_loss: 27.3531 - val_out_caps_acc: 0.9404\n",
            "Epoch 95/150\n",
            "11284/11284 [==============================] - 10s 875us/step - loss: 0.0322 - out_caps_loss: 0.0186 - out_recon_loss: 27.2963 - out_caps_acc: 0.9758 - val_loss: 0.0643 - val_out_caps_loss: 0.0506 - val_out_recon_loss: 27.3542 - val_out_caps_acc: 0.9305\n",
            "Epoch 96/150\n",
            "11284/11284 [==============================] - 9s 833us/step - loss: 0.0337 - out_caps_loss: 0.0200 - out_recon_loss: 27.2963 - out_caps_acc: 0.9728 - val_loss: 0.0579 - val_out_caps_loss: 0.0442 - val_out_recon_loss: 27.3526 - val_out_caps_acc: 0.9376\n",
            "Epoch 97/150\n",
            "11284/11284 [==============================] - 9s 835us/step - loss: 0.0338 - out_caps_loss: 0.0202 - out_recon_loss: 27.2964 - out_caps_acc: 0.9734 - val_loss: 0.0575 - val_out_caps_loss: 0.0438 - val_out_recon_loss: 27.3528 - val_out_caps_acc: 0.9383\n",
            "Epoch 98/150\n",
            "11284/11284 [==============================] - 9s 833us/step - loss: 0.0336 - out_caps_loss: 0.0199 - out_recon_loss: 27.2966 - out_caps_acc: 0.9731 - val_loss: 0.0686 - val_out_caps_loss: 0.0549 - val_out_recon_loss: 27.3532 - val_out_caps_acc: 0.9245\n",
            "Epoch 99/150\n",
            "11284/11284 [==============================] - 9s 835us/step - loss: 0.0322 - out_caps_loss: 0.0186 - out_recon_loss: 27.2965 - out_caps_acc: 0.9760 - val_loss: 0.0578 - val_out_caps_loss: 0.0442 - val_out_recon_loss: 27.3524 - val_out_caps_acc: 0.9397\n",
            "Epoch 100/150\n",
            "11284/11284 [==============================] - 9s 835us/step - loss: 0.0316 - out_caps_loss: 0.0180 - out_recon_loss: 27.2964 - out_caps_acc: 0.9770 - val_loss: 0.0576 - val_out_caps_loss: 0.0439 - val_out_recon_loss: 27.3525 - val_out_caps_acc: 0.9376\n",
            "Epoch 101/150\n",
            "11284/11284 [==============================] - 9s 836us/step - loss: 0.0324 - out_caps_loss: 0.0187 - out_recon_loss: 27.2961 - out_caps_acc: 0.9760 - val_loss: 0.0626 - val_out_caps_loss: 0.0489 - val_out_recon_loss: 27.3523 - val_out_caps_acc: 0.9302\n",
            "Epoch 102/150\n",
            "11284/11284 [==============================] - 10s 867us/step - loss: 0.0312 - out_caps_loss: 0.0176 - out_recon_loss: 27.2961 - out_caps_acc: 0.9773 - val_loss: 0.0626 - val_out_caps_loss: 0.0490 - val_out_recon_loss: 27.3523 - val_out_caps_acc: 0.9319\n",
            "Epoch 103/150\n",
            "11284/11284 [==============================] - 10s 929us/step - loss: 0.0322 - out_caps_loss: 0.0186 - out_recon_loss: 27.2965 - out_caps_acc: 0.9745 - val_loss: 0.0686 - val_out_caps_loss: 0.0549 - val_out_recon_loss: 27.3526 - val_out_caps_acc: 0.9238\n",
            "Epoch 104/150\n",
            "11284/11284 [==============================] - 9s 834us/step - loss: 0.0321 - out_caps_loss: 0.0184 - out_recon_loss: 27.2961 - out_caps_acc: 0.9763 - val_loss: 0.0613 - val_out_caps_loss: 0.0476 - val_out_recon_loss: 27.3522 - val_out_caps_acc: 0.9365\n",
            "Epoch 105/150\n",
            "11284/11284 [==============================] - 9s 833us/step - loss: 0.0327 - out_caps_loss: 0.0190 - out_recon_loss: 27.2962 - out_caps_acc: 0.9747 - val_loss: 0.0631 - val_out_caps_loss: 0.0494 - val_out_recon_loss: 27.3532 - val_out_caps_acc: 0.9319\n",
            "Epoch 106/150\n",
            "11284/11284 [==============================] - 10s 899us/step - loss: 0.0313 - out_caps_loss: 0.0177 - out_recon_loss: 27.2961 - out_caps_acc: 0.9765 - val_loss: 0.0609 - val_out_caps_loss: 0.0472 - val_out_recon_loss: 27.3528 - val_out_caps_acc: 0.9373\n",
            "Epoch 107/150\n",
            "11284/11284 [==============================] - 10s 844us/step - loss: 0.0321 - out_caps_loss: 0.0184 - out_recon_loss: 27.2963 - out_caps_acc: 0.9758 - val_loss: 0.0642 - val_out_caps_loss: 0.0505 - val_out_recon_loss: 27.3526 - val_out_caps_acc: 0.9312\n",
            "Epoch 108/150\n",
            "11284/11284 [==============================] - 9s 834us/step - loss: 0.0326 - out_caps_loss: 0.0190 - out_recon_loss: 27.2964 - out_caps_acc: 0.9749 - val_loss: 0.0573 - val_out_caps_loss: 0.0436 - val_out_recon_loss: 27.3528 - val_out_caps_acc: 0.9397\n",
            "Epoch 109/150\n",
            "11284/11284 [==============================] - 9s 834us/step - loss: 0.0306 - out_caps_loss: 0.0169 - out_recon_loss: 27.2967 - out_caps_acc: 0.9770 - val_loss: 0.0580 - val_out_caps_loss: 0.0443 - val_out_recon_loss: 27.3524 - val_out_caps_acc: 0.9408\n",
            "Epoch 110/150\n",
            "11284/11284 [==============================] - 9s 838us/step - loss: 0.0315 - out_caps_loss: 0.0179 - out_recon_loss: 27.2960 - out_caps_acc: 0.9761 - val_loss: 0.0637 - val_out_caps_loss: 0.0500 - val_out_recon_loss: 27.3525 - val_out_caps_acc: 0.9326\n",
            "Epoch 111/150\n",
            "11284/11284 [==============================] - 11s 960us/step - loss: 0.0308 - out_caps_loss: 0.0171 - out_recon_loss: 27.2958 - out_caps_acc: 0.9781 - val_loss: 0.0614 - val_out_caps_loss: 0.0477 - val_out_recon_loss: 27.3521 - val_out_caps_acc: 0.9365\n",
            "Epoch 112/150\n",
            "11284/11284 [==============================] - 11s 988us/step - loss: 0.0309 - out_caps_loss: 0.0173 - out_recon_loss: 27.2962 - out_caps_acc: 0.9779 - val_loss: 0.0623 - val_out_caps_loss: 0.0487 - val_out_recon_loss: 27.3523 - val_out_caps_acc: 0.9316\n",
            "Epoch 113/150\n",
            "11284/11284 [==============================] - 9s 837us/step - loss: 0.0308 - out_caps_loss: 0.0171 - out_recon_loss: 27.2963 - out_caps_acc: 0.9785 - val_loss: 0.0558 - val_out_caps_loss: 0.0421 - val_out_recon_loss: 27.3528 - val_out_caps_acc: 0.9412\n",
            "Epoch 114/150\n",
            "11284/11284 [==============================] - 9s 830us/step - loss: 0.0287 - out_caps_loss: 0.0151 - out_recon_loss: 27.2960 - out_caps_acc: 0.9799 - val_loss: 0.0632 - val_out_caps_loss: 0.0495 - val_out_recon_loss: 27.3529 - val_out_caps_acc: 0.9291\n",
            "Epoch 115/150\n",
            "11284/11284 [==============================] - 9s 836us/step - loss: 0.0308 - out_caps_loss: 0.0171 - out_recon_loss: 27.2961 - out_caps_acc: 0.9768 - val_loss: 0.0588 - val_out_caps_loss: 0.0451 - val_out_recon_loss: 27.3524 - val_out_caps_acc: 0.9387\n",
            "Epoch 116/150\n",
            "11284/11284 [==============================] - 9s 837us/step - loss: 0.0305 - out_caps_loss: 0.0169 - out_recon_loss: 27.2962 - out_caps_acc: 0.9794 - val_loss: 0.0610 - val_out_caps_loss: 0.0473 - val_out_recon_loss: 27.3534 - val_out_caps_acc: 0.9351\n",
            "Epoch 117/150\n",
            "11284/11284 [==============================] - 9s 831us/step - loss: 0.0299 - out_caps_loss: 0.0163 - out_recon_loss: 27.2964 - out_caps_acc: 0.9785 - val_loss: 0.0616 - val_out_caps_loss: 0.0479 - val_out_recon_loss: 27.3522 - val_out_caps_acc: 0.9341\n",
            "Epoch 118/150\n",
            "11284/11284 [==============================] - 9s 838us/step - loss: 0.0298 - out_caps_loss: 0.0161 - out_recon_loss: 27.2961 - out_caps_acc: 0.9794 - val_loss: 0.0620 - val_out_caps_loss: 0.0483 - val_out_recon_loss: 27.3520 - val_out_caps_acc: 0.9323\n",
            "Epoch 119/150\n",
            "11284/11284 [==============================] - 11s 934us/step - loss: 0.0302 - out_caps_loss: 0.0165 - out_recon_loss: 27.2960 - out_caps_acc: 0.9772 - val_loss: 0.0575 - val_out_caps_loss: 0.0438 - val_out_recon_loss: 27.3525 - val_out_caps_acc: 0.9415\n",
            "Epoch 120/150\n",
            "11284/11284 [==============================] - 10s 855us/step - loss: 0.0294 - out_caps_loss: 0.0158 - out_recon_loss: 27.2959 - out_caps_acc: 0.9799 - val_loss: 0.0683 - val_out_caps_loss: 0.0546 - val_out_recon_loss: 27.3529 - val_out_caps_acc: 0.9263\n",
            "Epoch 121/150\n",
            "11284/11284 [==============================] - 9s 836us/step - loss: 0.0308 - out_caps_loss: 0.0171 - out_recon_loss: 27.2963 - out_caps_acc: 0.9774 - val_loss: 0.0660 - val_out_caps_loss: 0.0523 - val_out_recon_loss: 27.3533 - val_out_caps_acc: 0.9273\n",
            "Epoch 122/150\n",
            "11284/11284 [==============================] - 9s 841us/step - loss: 0.0296 - out_caps_loss: 0.0159 - out_recon_loss: 27.2968 - out_caps_acc: 0.9802 - val_loss: 0.0654 - val_out_caps_loss: 0.0517 - val_out_recon_loss: 27.3524 - val_out_caps_acc: 0.9273\n",
            "Epoch 123/150\n",
            "11284/11284 [==============================] - 9s 836us/step - loss: 0.0312 - out_caps_loss: 0.0175 - out_recon_loss: 27.2961 - out_caps_acc: 0.9766 - val_loss: 0.0581 - val_out_caps_loss: 0.0445 - val_out_recon_loss: 27.3525 - val_out_caps_acc: 0.9376\n",
            "Epoch 124/150\n",
            "11284/11284 [==============================] - 9s 837us/step - loss: 0.0294 - out_caps_loss: 0.0158 - out_recon_loss: 27.2957 - out_caps_acc: 0.9795 - val_loss: 0.0572 - val_out_caps_loss: 0.0435 - val_out_recon_loss: 27.3522 - val_out_caps_acc: 0.9401\n",
            "Epoch 125/150\n",
            "11284/11284 [==============================] - 9s 834us/step - loss: 0.0305 - out_caps_loss: 0.0168 - out_recon_loss: 27.2960 - out_caps_acc: 0.9771 - val_loss: 0.0764 - val_out_caps_loss: 0.0627 - val_out_recon_loss: 27.3515 - val_out_caps_acc: 0.9139\n",
            "Epoch 126/150\n",
            "11284/11284 [==============================] - 9s 834us/step - loss: 0.0282 - out_caps_loss: 0.0146 - out_recon_loss: 27.2959 - out_caps_acc: 0.9817 - val_loss: 0.0624 - val_out_caps_loss: 0.0487 - val_out_recon_loss: 27.3517 - val_out_caps_acc: 0.9309\n",
            "Epoch 127/150\n",
            "11284/11284 [==============================] - 10s 889us/step - loss: 0.0269 - out_caps_loss: 0.0132 - out_recon_loss: 27.2959 - out_caps_acc: 0.9827 - val_loss: 0.0603 - val_out_caps_loss: 0.0466 - val_out_recon_loss: 27.3522 - val_out_caps_acc: 0.9380\n",
            "Epoch 128/150\n",
            "11284/11284 [==============================] - 10s 904us/step - loss: 0.0324 - out_caps_loss: 0.0187 - out_recon_loss: 27.2962 - out_caps_acc: 0.9751 - val_loss: 0.0587 - val_out_caps_loss: 0.0451 - val_out_recon_loss: 27.3521 - val_out_caps_acc: 0.9376\n",
            "Epoch 129/150\n",
            "11284/11284 [==============================] - 9s 830us/step - loss: 0.0283 - out_caps_loss: 0.0147 - out_recon_loss: 27.2958 - out_caps_acc: 0.9802 - val_loss: 0.0726 - val_out_caps_loss: 0.0589 - val_out_recon_loss: 27.3528 - val_out_caps_acc: 0.9199\n",
            "Epoch 130/150\n",
            "11284/11284 [==============================] - 9s 837us/step - loss: 0.0289 - out_caps_loss: 0.0153 - out_recon_loss: 27.2956 - out_caps_acc: 0.9792 - val_loss: 0.0586 - val_out_caps_loss: 0.0449 - val_out_recon_loss: 27.3520 - val_out_caps_acc: 0.9397\n",
            "Epoch 131/150\n",
            "11284/11284 [==============================] - 9s 835us/step - loss: 0.0297 - out_caps_loss: 0.0160 - out_recon_loss: 27.2960 - out_caps_acc: 0.9781 - val_loss: 0.0596 - val_out_caps_loss: 0.0459 - val_out_recon_loss: 27.3521 - val_out_caps_acc: 0.9365\n",
            "Epoch 132/150\n",
            "11284/11284 [==============================] - 9s 828us/step - loss: 0.0284 - out_caps_loss: 0.0148 - out_recon_loss: 27.2957 - out_caps_acc: 0.9795 - val_loss: 0.0610 - val_out_caps_loss: 0.0474 - val_out_recon_loss: 27.3526 - val_out_caps_acc: 0.9337\n",
            "Epoch 133/150\n",
            "11284/11284 [==============================] - 9s 833us/step - loss: 0.0288 - out_caps_loss: 0.0151 - out_recon_loss: 27.2960 - out_caps_acc: 0.9794 - val_loss: 0.0578 - val_out_caps_loss: 0.0442 - val_out_recon_loss: 27.3527 - val_out_caps_acc: 0.9404\n",
            "Epoch 134/150\n",
            "11284/11284 [==============================] - 9s 834us/step - loss: 0.0304 - out_caps_loss: 0.0168 - out_recon_loss: 27.2956 - out_caps_acc: 0.9778 - val_loss: 0.0565 - val_out_caps_loss: 0.0428 - val_out_recon_loss: 27.3527 - val_out_caps_acc: 0.9401\n",
            "Epoch 135/150\n",
            "11284/11284 [==============================] - 9s 833us/step - loss: 0.0275 - out_caps_loss: 0.0139 - out_recon_loss: 27.2956 - out_caps_acc: 0.9821 - val_loss: 0.0576 - val_out_caps_loss: 0.0439 - val_out_recon_loss: 27.3526 - val_out_caps_acc: 0.9443\n",
            "Epoch 136/150\n",
            "11284/11284 [==============================] - 11s 960us/step - loss: 0.0294 - out_caps_loss: 0.0158 - out_recon_loss: 27.2959 - out_caps_acc: 0.9794 - val_loss: 0.0591 - val_out_caps_loss: 0.0454 - val_out_recon_loss: 27.3516 - val_out_caps_acc: 0.9376\n",
            "Epoch 137/150\n",
            "11284/11284 [==============================] - 9s 836us/step - loss: 0.0295 - out_caps_loss: 0.0158 - out_recon_loss: 27.2958 - out_caps_acc: 0.9795 - val_loss: 0.0615 - val_out_caps_loss: 0.0479 - val_out_recon_loss: 27.3518 - val_out_caps_acc: 0.9337\n",
            "Epoch 138/150\n",
            "11284/11284 [==============================] - 10s 906us/step - loss: 0.0273 - out_caps_loss: 0.0136 - out_recon_loss: 27.2960 - out_caps_acc: 0.9823 - val_loss: 0.0603 - val_out_caps_loss: 0.0467 - val_out_recon_loss: 27.3519 - val_out_caps_acc: 0.9355\n",
            "Epoch 139/150\n",
            "11284/11284 [==============================] - 9s 840us/step - loss: 0.0283 - out_caps_loss: 0.0146 - out_recon_loss: 27.2957 - out_caps_acc: 0.9815 - val_loss: 0.0568 - val_out_caps_loss: 0.0432 - val_out_recon_loss: 27.3517 - val_out_caps_acc: 0.9412\n",
            "Epoch 140/150\n",
            "11284/11284 [==============================] - 9s 835us/step - loss: 0.0285 - out_caps_loss: 0.0149 - out_recon_loss: 27.2957 - out_caps_acc: 0.9803 - val_loss: 0.0602 - val_out_caps_loss: 0.0465 - val_out_recon_loss: 27.3520 - val_out_caps_acc: 0.9365\n",
            "Epoch 141/150\n",
            "11284/11284 [==============================] - 9s 830us/step - loss: 0.0310 - out_caps_loss: 0.0173 - out_recon_loss: 27.2956 - out_caps_acc: 0.9775 - val_loss: 0.0637 - val_out_caps_loss: 0.0500 - val_out_recon_loss: 27.3519 - val_out_caps_acc: 0.9319\n",
            "Epoch 142/150\n",
            "11284/11284 [==============================] - 9s 839us/step - loss: 0.0283 - out_caps_loss: 0.0147 - out_recon_loss: 27.2958 - out_caps_acc: 0.9803 - val_loss: 0.0631 - val_out_caps_loss: 0.0494 - val_out_recon_loss: 27.3521 - val_out_caps_acc: 0.9337\n",
            "Trained model saved to 'trained_model.h5'\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.engine.training.Model at 0x7f3e14436780>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 113
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fiPYygxODKDj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "layer_name = 'out_caps'\n",
        "intermediate_layer_model = models.Model(inputs=model.input[0],\n",
        "                                 outputs=model.get_layer(layer_name).output)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f63-APoiHUtH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "y_pred = intermediate_layer_model.predict(x_test_reshape)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TIrPov4_HdUH",
        "colab_type": "code",
        "outputId": "e23d297b-ca40-4271-ae26-25ec13c75d4d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "cm = confusion_matrix(y_test, np.argmax(y_pred, axis=1))\n",
        "cm"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[2793,  230],\n",
              "       [ 142, 2880]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 116
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2vuSm7Rlgspk",
        "colab_type": "text"
      },
      "source": [
        "# AutoML"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dytUx1xUHfBt",
        "colab_type": "code",
        "outputId": "62ac2bcc-cdb5-43ea-de52-af79b930bdc7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1839
        }
      },
      "source": [
        "!apt-get install swig -y\n",
        "!pip install Cython numpy\n",
        "\n",
        "# sometimes you have to run the next command twice on colab\n",
        "# I haven't figured out why\n",
        "!pip install auto-sklearn"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Reading package lists... Done\n",
            "Building dependency tree       \n",
            "Reading state information... Done\n",
            "The following package was automatically installed and is no longer required:\n",
            "  libnvidia-common-410\n",
            "Use 'apt autoremove' to remove it.\n",
            "The following additional packages will be installed:\n",
            "  swig3.0\n",
            "Suggested packages:\n",
            "  swig-doc swig-examples swig3.0-examples swig3.0-doc\n",
            "The following NEW packages will be installed:\n",
            "  swig swig3.0\n",
            "0 upgraded, 2 newly installed, 0 to remove and 6 not upgraded.\n",
            "Need to get 1,100 kB of archives.\n",
            "After this operation, 5,822 kB of additional disk space will be used.\n",
            "Get:1 http://archive.ubuntu.com/ubuntu bionic/universe amd64 swig3.0 amd64 3.0.12-1 [1,094 kB]\n",
            "Get:2 http://archive.ubuntu.com/ubuntu bionic/universe amd64 swig amd64 3.0.12-1 [6,460 B]\n",
            "Fetched 1,100 kB in 0s (7,872 kB/s)\n",
            "Selecting previously unselected package swig3.0.\n",
            "(Reading database ... 130911 files and directories currently installed.)\n",
            "Preparing to unpack .../swig3.0_3.0.12-1_amd64.deb ...\n",
            "Unpacking swig3.0 (3.0.12-1) ...\n",
            "Selecting previously unselected package swig.\n",
            "Preparing to unpack .../swig_3.0.12-1_amd64.deb ...\n",
            "Unpacking swig (3.0.12-1) ...\n",
            "Setting up swig3.0 (3.0.12-1) ...\n",
            "Processing triggers for man-db (2.8.3-2ubuntu0.1) ...\n",
            "Setting up swig (3.0.12-1) ...\n",
            "Requirement already satisfied: Cython in /usr/local/lib/python3.6/dist-packages (0.29.7)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (1.16.3)\n",
            "Collecting auto-sklearn\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/1d/99/27caac4f6804be48722158e31c630e0737110581774df0615a36b21239aa/auto-sklearn-0.5.2.tar.gz (3.4MB)\n",
            "\u001b[K     || 3.4MB 3.2MB/s \n",
            "\u001b[?25hRequirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from auto-sklearn) (41.0.1)\n",
            "Requirement already satisfied: nose in /usr/local/lib/python3.6/dist-packages (from auto-sklearn) (1.3.7)\n",
            "Requirement already satisfied: Cython in /usr/local/lib/python3.6/dist-packages (from auto-sklearn) (0.29.7)\n",
            "Requirement already satisfied: numpy>=1.9.0 in /usr/local/lib/python3.6/dist-packages (from auto-sklearn) (1.16.3)\n",
            "Requirement already satisfied: scipy>=0.14.1 in /usr/local/lib/python3.6/dist-packages (from auto-sklearn) (1.3.0)\n",
            "Collecting scikit-learn<0.20,>=0.19 (from auto-sklearn)\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/f9/c8/8db4108aba5e2166cd2ea4eafa1a4b82f89240a1fa85733029cc2358ad1f/scikit_learn-0.19.2-cp36-cp36m-manylinux1_x86_64.whl (4.9MB)\n",
            "\u001b[K     || 4.9MB 27.0MB/s \n",
            "\u001b[?25hRequirement already satisfied: xgboost>=0.80 in /usr/local/lib/python3.6/dist-packages (from auto-sklearn) (0.90)\n",
            "Collecting lockfile (from auto-sklearn)\n",
            "  Downloading https://files.pythonhosted.org/packages/c8/22/9460e311f340cb62d26a38c419b1381b8593b0bb6b5d1f056938b086d362/lockfile-0.12.2-py2.py3-none-any.whl\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.6/dist-packages (from auto-sklearn) (0.12.5)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.6/dist-packages (from auto-sklearn) (5.4.8)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.6/dist-packages (from auto-sklearn) (3.13)\n",
            "Collecting liac-arff (from auto-sklearn)\n",
            "  Downloading https://files.pythonhosted.org/packages/e9/35/fbc9217cfa91d98888b43e1a19c03a50d716108c58494c558c65e308f372/liac-arff-2.4.0.tar.gz\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.6/dist-packages (from auto-sklearn) (0.24.2)\n",
            "Collecting ConfigSpace<0.5,>=0.4.0 (from auto-sklearn)\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/42/de/4e8e4f26332fc65404f52baa112defbf822b6738b60bfa6b2993f5c60933/ConfigSpace-0.4.10.tar.gz (882kB)\n",
            "\u001b[K     || 890kB 42.7MB/s \n",
            "\u001b[?25hCollecting pynisher>=0.4.2 (from auto-sklearn)\n",
            "  Downloading https://files.pythonhosted.org/packages/b2/21/c86c64c305da6d43fb89780d33cbc839c07736b71955a8bdb642a02b7538/pynisher-0.5.0.tar.gz\n",
            "Collecting pyrfr<0.8,>=0.7 (from auto-sklearn)\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/c3/c6/c555cfa3c7d0078dded091d4901ed52344bbb925077aa70b871faf35fd58/pyrfr-0.7.4.tar.gz (291kB)\n",
            "\u001b[K     || 296kB 39.7MB/s \n",
            "\u001b[?25hCollecting smac==0.8 (from auto-sklearn)\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/33/ab/2b0a6fb00bd76e2415a04dcca453ad0b0db9b4218b02401306ff2bc6135d/smac-0.8.0.tar.gz (94kB)\n",
            "\u001b[K     || 102kB 30.7MB/s \n",
            "\u001b[?25hRequirement already satisfied: pytz>=2011k in /usr/local/lib/python3.6/dist-packages (from pandas->auto-sklearn) (2018.9)\n",
            "Requirement already satisfied: python-dateutil>=2.5.0 in /usr/local/lib/python3.6/dist-packages (from pandas->auto-sklearn) (2.5.3)\n",
            "Requirement already satisfied: pyparsing in /usr/local/lib/python3.6/dist-packages (from ConfigSpace<0.5,>=0.4.0->auto-sklearn) (2.4.0)\n",
            "Requirement already satisfied: typing in /usr/local/lib/python3.6/dist-packages (from ConfigSpace<0.5,>=0.4.0->auto-sklearn) (3.6.6)\n",
            "Requirement already satisfied: docutils>=0.3 in /usr/local/lib/python3.6/dist-packages (from pynisher>=0.4.2->auto-sklearn) (0.14)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from smac==0.8->auto-sklearn) (1.12.0)\n",
            "Requirement already satisfied: sphinx in /usr/local/lib/python3.6/dist-packages (from smac==0.8->auto-sklearn) (1.8.5)\n",
            "Collecting sphinx_rtd_theme (from smac==0.8->auto-sklearn)\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/60/b4/4df37087a1d36755e3a3bfd2a30263f358d2dea21938240fa02313d45f51/sphinx_rtd_theme-0.4.3-py2.py3-none-any.whl (6.4MB)\n",
            "\u001b[K     || 6.4MB 25.4MB/s \n",
            "\u001b[?25hRequirement already satisfied: snowballstemmer>=1.1 in /usr/local/lib/python3.6/dist-packages (from sphinx->smac==0.8->auto-sklearn) (1.2.1)\n",
            "Requirement already satisfied: babel!=2.0,>=1.3 in /usr/local/lib/python3.6/dist-packages (from sphinx->smac==0.8->auto-sklearn) (2.6.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.6/dist-packages (from sphinx->smac==0.8->auto-sklearn) (19.0)\n",
            "Requirement already satisfied: requests>=2.0.0 in /usr/local/lib/python3.6/dist-packages (from sphinx->smac==0.8->auto-sklearn) (2.21.0)\n",
            "Requirement already satisfied: Pygments>=2.0 in /usr/local/lib/python3.6/dist-packages (from sphinx->smac==0.8->auto-sklearn) (2.1.3)\n",
            "Requirement already satisfied: Jinja2>=2.3 in /usr/local/lib/python3.6/dist-packages (from sphinx->smac==0.8->auto-sklearn) (2.10.1)\n",
            "Requirement already satisfied: sphinxcontrib-websupport in /usr/local/lib/python3.6/dist-packages (from sphinx->smac==0.8->auto-sklearn) (1.1.2)\n",
            "Requirement already satisfied: imagesize in /usr/local/lib/python3.6/dist-packages (from sphinx->smac==0.8->auto-sklearn) (1.1.0)\n",
            "Requirement already satisfied: alabaster<0.8,>=0.7 in /usr/local/lib/python3.6/dist-packages (from sphinx->smac==0.8->auto-sklearn) (0.7.12)\n",
            "Requirement already satisfied: urllib3<1.25,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests>=2.0.0->sphinx->smac==0.8->auto-sklearn) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests>=2.0.0->sphinx->smac==0.8->auto-sklearn) (2019.3.9)\n",
            "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests>=2.0.0->sphinx->smac==0.8->auto-sklearn) (3.0.4)\n",
            "Requirement already satisfied: idna<2.9,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests>=2.0.0->sphinx->smac==0.8->auto-sklearn) (2.8)\n",
            "Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.6/dist-packages (from Jinja2>=2.3->sphinx->smac==0.8->auto-sklearn) (1.1.1)\n",
            "Building wheels for collected packages: auto-sklearn, liac-arff, ConfigSpace, pynisher, pyrfr, smac\n",
            "  Building wheel for auto-sklearn (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Stored in directory: /root/.cache/pip/wheels/67/21/43/182fed664b6474f88600c110c4ebd254d6256ba59175cef3fd\n",
            "  Building wheel for liac-arff (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Stored in directory: /root/.cache/pip/wheels/d1/6a/e7/529dc54d76ecede4346164a09ae3168df358945612710f5203\n",
            "  Building wheel for ConfigSpace (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Stored in directory: /root/.cache/pip/wheels/75/83/cb/28dd42bac69c8867d485138030daa83841c7f84afe68b2fdf7\n",
            "  Building wheel for pynisher (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Stored in directory: /root/.cache/pip/wheels/af/2a/c4/ec3abc8a2f786ef9786ea8fe6ff629a4e54812a3f98cc41b47\n",
            "  Building wheel for pyrfr (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Stored in directory: /root/.cache/pip/wheels/fb/98/fd/b1d53cab6d5ed836980777d9733d7e549d82a727650eed6f6d\n",
            "  Building wheel for smac (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Stored in directory: /root/.cache/pip/wheels/12/52/83/d2d66a840968025d072ddb1cd776fdc5eb5e337e1cc887bc3f\n",
            "Successfully built auto-sklearn liac-arff ConfigSpace pynisher pyrfr smac\n",
            "\u001b[31mERROR: yellowbrick 0.9.1 has requirement scikit-learn>=0.20, but you'll have scikit-learn 0.19.2 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: imbalanced-learn 0.4.3 has requirement scikit-learn>=0.20, but you'll have scikit-learn 0.19.2 which is incompatible.\u001b[0m\n",
            "Installing collected packages: scikit-learn, lockfile, liac-arff, ConfigSpace, pynisher, pyrfr, sphinx-rtd-theme, smac, auto-sklearn\n",
            "  Found existing installation: scikit-learn 0.21.2\n",
            "    Uninstalling scikit-learn-0.21.2:\n",
            "      Successfully uninstalled scikit-learn-0.21.2\n",
            "Successfully installed ConfigSpace-0.4.10 auto-sklearn-0.5.2 liac-arff-2.4.0 lockfile-0.12.2 pynisher-0.5.0 pyrfr-0.7.4 scikit-learn-0.19.2 smac-0.8.0 sphinx-rtd-theme-0.4.3\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ikoTYrDNhF6c",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!pip install --no-cache-dir -v pyrfr\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hBqTx58eh3Hh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!sudo apt-get install build-essential swig\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_5c1QGIniPlE",
        "colab_type": "code",
        "outputId": "7b58bf87-6ee1-45b1-c279-29bbd02102b6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        }
      },
      "source": [
        "import autosklearn.classification"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/ensemble/weight_boosting.py:29: DeprecationWarning: numpy.core.umath_tests is an internal NumPy module and should not be imported. It will be removed in a future NumPy release.\n",
            "  from numpy.core.umath_tests import inner1d\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u48-8-BSksI9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "cls =  autosklearn.classification.AutoSklearnClassifier()\n",
        "     "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S5vizWk5BTro",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "x_train = df.iloc[:,0:20].values\n",
        "y_train = df.iloc[:,20].values\n",
        "x_test = df2.iloc[:,0:20].values\n",
        "y_test = df2.iloc[:,20].values"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mF5VoqgjBcXx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "cls.fit(x_train, y_train)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6DhCfuWEBgGL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "y_pred = cls.predict(x_test)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wgLcgHeCB2Lr",
        "colab_type": "code",
        "outputId": "11165b85-cc61-4c79-8536-71c51a99389a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "from sklearn.metrics import accuracy_score\n",
        "accuracy_score(y_test, y_pred)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.971712158808933"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PFKGdtA_B7xu",
        "colab_type": "code",
        "outputId": "9c5d5ade-7a7a-47be-83c3-f229c38d7131",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 187
        }
      },
      "source": [
        "print(cls.sprint_statistics())"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "auto-sklearn results:\n",
            "  Dataset name: cd34fe534e3c1f704d699b6d2d4b61e0\n",
            "  Metric: accuracy\n",
            "  Best validation score: 0.964286\n",
            "  Number of target algorithm runs: 106\n",
            "  Number of successful target algorithm runs: 102\n",
            "  Number of crashed target algorithm runs: 0\n",
            "  Number of target algorithms that exceeded the time limit: 4\n",
            "  Number of target algorithms that exceeded the memory limit: 0\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QabOTha2RoH2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print(cls.show_models())"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oQk5X57RRxSk",
        "colab_type": "code",
        "outputId": "03ea0b59-c83f-4ede-9a87-28a9819b0f12",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 180
        }
      },
      "source": [
        "from sklearn.metrics import confusion_matrix\n",
        "confusion_matrix(y_true, y_pred)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-20-2f092c4363ee>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmetrics\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mconfusion_matrix\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mconfusion_matrix\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m: name 'y_true' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CchodAWAhs7Y",
        "colab_type": "text"
      },
      "source": [
        "## Auto Sklearn with entire data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Eiqo5AN-gX9v",
        "colab_type": "code",
        "outputId": "b670d0e7-8f0a-4733-9b3e-9e172d6a3c57",
        "colab": {
          "resources": {
            "http://localhost:8080/nbextensions/google.colab/files.js": {
              "data": "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",
              "ok": true,
              "headers": [
                [
                  "content-type",
                  "application/javascript"
                ]
              ],
              "status": 200,
              "status_text": ""
            }
          },
          "base_uri": "https://localhost:8080/",
          "height": 74
        }
      },
      "source": [
        "from google.colab import files\n",
        "uploaded = files.upload()\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-d85a20ac-51b7-4cf9-9e59-5f7778059625\" name=\"files[]\" multiple disabled />\n",
              "     <output id=\"result-d85a20ac-51b7-4cf9-9e59-5f7778059625\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script src=\"/nbextensions/google.colab/files.js\"></script> "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Saving Complete_Data_Allergens.csv to Complete_Data_Allergens.csv\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hji6eF7ph5KS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pandas as pd\n",
        "import io\n",
        "df = pd.read_csv(io.BytesIO(uploaded['Complete_Data_Allergens.csv']))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A3f8-lvUimUC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "x = df.iloc[:, 0:20].values\n",
        "y = df.iloc[:, 20].values"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JSCDdWmxiCsn",
        "colab_type": "code",
        "outputId": "263d78b9-8257-4c58-e613-98478ac22f55",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "import numpy as np\n",
        "unique, counts = np.unique(y, return_counts=True)\n",
        "dict(zip(unique, counts))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{0: 10075, 1: 10075}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gz-KSg0TiulW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import autosklearn.classification\n",
        "cls = autosklearn.classification.AutoSklearnClassifier(resampling_strategy='cv',\n",
        "        resampling_strategy_arguments={'folds': 10})\n",
        "     "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6UeY-i1fjBRx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "x_train, x_test, y_train, y_test = train_test_split(x,y, test_size = 0.2, stratify = y, random_state = 42)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UExQ1sp5jW6K",
        "colab_type": "code",
        "outputId": "7495cba9-95cf-48d7-c883-bfe2b86b5419",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 700
        }
      },
      "source": [
        "cls.fit(x_train, y_train)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[WARNING] [2019-05-30 13:37:13,770:EnsembleBuilder(1):681634fa14b86364a433edd3bede8d78] No models better than random - using Dummy Score!\n",
            "[WARNING] [2019-05-30 13:37:13,785:EnsembleBuilder(1):681634fa14b86364a433edd3bede8d78] No models better than random - using Dummy Score!\n",
            "[WARNING] [2019-05-30 13:37:15,790:EnsembleBuilder(1):681634fa14b86364a433edd3bede8d78] No models better than random - using Dummy Score!\n",
            "[WARNING] [2019-05-30 13:37:17,795:EnsembleBuilder(1):681634fa14b86364a433edd3bede8d78] No models better than random - using Dummy Score!\n",
            "[WARNING] [2019-05-30 13:37:19,810:EnsembleBuilder(1):681634fa14b86364a433edd3bede8d78] No models better than random - using Dummy Score!\n",
            "[WARNING] [2019-05-30 13:37:21,815:EnsembleBuilder(1):681634fa14b86364a433edd3bede8d78] No models better than random - using Dummy Score!\n",
            "[WARNING] [2019-05-30 13:37:23,833:EnsembleBuilder(1):681634fa14b86364a433edd3bede8d78] No models better than random - using Dummy Score!\n",
            "[WARNING] [2019-05-30 13:37:25,848:EnsembleBuilder(1):681634fa14b86364a433edd3bede8d78] No models better than random - using Dummy Score!\n",
            "[WARNING] [2019-05-30 13:37:27,861:EnsembleBuilder(1):681634fa14b86364a433edd3bede8d78] No models better than random - using Dummy Score!\n",
            "[WARNING] [2019-05-30 13:37:29,871:EnsembleBuilder(1):681634fa14b86364a433edd3bede8d78] No models better than random - using Dummy Score!\n",
            "[WARNING] [2019-05-30 13:37:31,890:EnsembleBuilder(1):681634fa14b86364a433edd3bede8d78] No models better than random - using Dummy Score!\n",
            "[WARNING] [2019-05-30 13:37:33,908:EnsembleBuilder(1):681634fa14b86364a433edd3bede8d78] No models better than random - using Dummy Score!\n",
            "[WARNING] [2019-05-30 13:37:35,931:EnsembleBuilder(1):681634fa14b86364a433edd3bede8d78] No models better than random - using Dummy Score!\n",
            "[WARNING] [2019-05-30 13:37:37,935:EnsembleBuilder(1):681634fa14b86364a433edd3bede8d78] No models better than random - using Dummy Score!\n",
            "[WARNING] [2019-05-30 13:37:39,949:EnsembleBuilder(1):681634fa14b86364a433edd3bede8d78] No models better than random - using Dummy Score!\n",
            "[WARNING] [2019-05-30 13:37:41,963:EnsembleBuilder(1):681634fa14b86364a433edd3bede8d78] No models better than random - using Dummy Score!\n",
            "[WARNING] [2019-05-30 13:37:43,978:EnsembleBuilder(1):681634fa14b86364a433edd3bede8d78] No models better than random - using Dummy Score!\n",
            "[WARNING] [2019-05-30 13:37:45,993:EnsembleBuilder(1):681634fa14b86364a433edd3bede8d78] No models better than random - using Dummy Score!\n",
            "[WARNING] [2019-05-30 13:37:48,007:EnsembleBuilder(1):681634fa14b86364a433edd3bede8d78] No models better than random - using Dummy Score!\n",
            "[WARNING] [2019-05-30 13:37:50,022:EnsembleBuilder(1):681634fa14b86364a433edd3bede8d78] No models better than random - using Dummy Score!\n",
            "[WARNING] [2019-05-30 13:37:52,037:EnsembleBuilder(1):681634fa14b86364a433edd3bede8d78] No models better than random - using Dummy Score!\n",
            "[WARNING] [2019-05-30 13:37:54,050:EnsembleBuilder(1):681634fa14b86364a433edd3bede8d78] No models better than random - using Dummy Score!\n",
            "[WARNING] [2019-05-30 13:37:56,064:EnsembleBuilder(1):681634fa14b86364a433edd3bede8d78] No models better than random - using Dummy Score!\n",
            "[WARNING] [2019-05-30 13:37:58,080:EnsembleBuilder(1):681634fa14b86364a433edd3bede8d78] No models better than random - using Dummy Score!\n",
            "1\n",
            "['/tmp/autosklearn_tmp_143_3897/.auto-sklearn/ensembles/1.0000000000.ensemble', '/tmp/autosklearn_tmp_143_3897/.auto-sklearn/ensembles/1.0000000001.ensemble', '/tmp/autosklearn_tmp_143_3897/.auto-sklearn/ensembles/1.0000000002.ensemble', '/tmp/autosklearn_tmp_143_3897/.auto-sklearn/ensembles/1.0000000003.ensemble', '/tmp/autosklearn_tmp_143_3897/.auto-sklearn/ensembles/1.0000000004.ensemble', '/tmp/autosklearn_tmp_143_3897/.auto-sklearn/ensembles/1.0000000005.ensemble', '/tmp/autosklearn_tmp_143_3897/.auto-sklearn/ensembles/1.0000000006.ensemble', '/tmp/autosklearn_tmp_143_3897/.auto-sklearn/ensembles/1.0000000007.ensemble', '/tmp/autosklearn_tmp_143_3897/.auto-sklearn/ensembles/1.0000000008.ensemble', '/tmp/autosklearn_tmp_143_3897/.auto-sklearn/ensembles/1.0000000009.ensemble', '/tmp/autosklearn_tmp_143_3897/.auto-sklearn/ensembles/1.0000000010.ensemble', '/tmp/autosklearn_tmp_143_3897/.auto-sklearn/ensembles/1.0000000011.ensemble', '/tmp/autosklearn_tmp_143_3897/.auto-sklearn/ensembles/1.0000000012.ensemble', '/tmp/autosklearn_tmp_143_3897/.auto-sklearn/ensembles/1.0000000013.ensemble', '/tmp/autosklearn_tmp_143_3897/.auto-sklearn/ensembles/1.0000000014.ensemble', '/tmp/autosklearn_tmp_143_3897/.auto-sklearn/ensembles/1.0000000015.ensemble', '/tmp/autosklearn_tmp_143_3897/.auto-sklearn/ensembles/1.0000000016.ensemble', '/tmp/autosklearn_tmp_143_3897/.auto-sklearn/ensembles/1.0000000017.ensemble', '/tmp/autosklearn_tmp_143_3897/.auto-sklearn/ensembles/1.0000000018.ensemble', '/tmp/autosklearn_tmp_143_3897/.auto-sklearn/ensembles/1.0000000019.ensemble', '/tmp/autosklearn_tmp_143_3897/.auto-sklearn/ensembles/1.0000000020.ensemble', '/tmp/autosklearn_tmp_143_3897/.auto-sklearn/ensembles/1.0000000021.ensemble', '/tmp/autosklearn_tmp_143_3897/.auto-sklearn/ensembles/1.0000000022.ensemble', '/tmp/autosklearn_tmp_143_3897/.auto-sklearn/ensembles/1.0000000023.ensemble', '/tmp/autosklearn_tmp_143_3897/.auto-sklearn/ensembles/1.0000000024.ensemble', '/tmp/autosklearn_tmp_143_3897/.auto-sklearn/ensembles/1.0000000025.ensemble', '/tmp/autosklearn_tmp_143_3897/.auto-sklearn/ensembles/1.0000000026.ensemble', '/tmp/autosklearn_tmp_143_3897/.auto-sklearn/ensembles/1.0000000027.ensemble', '/tmp/autosklearn_tmp_143_3897/.auto-sklearn/ensembles/1.0000000028.ensemble']\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "AutoSklearnClassifier(delete_output_folder_after_terminate=True,\n",
              "           delete_tmp_folder_after_terminate=True,\n",
              "           disable_evaluator_output=False, ensemble_memory_limit=1024,\n",
              "           ensemble_nbest=50, ensemble_size=50, exclude_estimators=None,\n",
              "           exclude_preprocessors=None, get_smac_object_callback=None,\n",
              "           include_estimators=None, include_preprocessors=None,\n",
              "           initial_configurations_via_metalearning=25, logging_config=None,\n",
              "           metadata_directory=None, ml_memory_limit=3072, n_jobs=None,\n",
              "           output_folder=None, per_run_time_limit=360,\n",
              "           resampling_strategy='cv',\n",
              "           resampling_strategy_arguments={'folds': 10}, seed=1,\n",
              "           shared_mode=False, smac_scenario_args=None,\n",
              "           time_left_for_this_task=3600, tmp_folder=None)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 48
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "umhVX50Mj3rG",
        "colab_type": "code",
        "outputId": "94005661-638b-43bf-d344-d909ba0f1c34",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 38576
        }
      },
      "source": [
        "y_pred = cls.refit(x.copy(), y.copy())\n",
        "cls.cv_results_"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'mean_fit_time': array([ 45.39333773, 221.397861  , 108.75488901,  31.94562531,\n",
              "        215.15770936, 191.84766197, 130.60086608, 360.11775088,\n",
              "        312.59700274,  19.98884392,  35.35358596, 125.36279559,\n",
              "         41.36913657,   9.83233643,  17.94522023,   5.56589651,\n",
              "         46.92602515,  35.48386478,  47.09789705, 360.1178956 ,\n",
              "         31.96032047,  55.35086775,  52.24155927,  28.08233094,\n",
              "        360.08591533,  20.13560081,  25.71572185, 104.14301491,\n",
              "         82.14506149,  21.13012505,  82.76960564, 344.0286665 ]),\n",
              " 'mean_test_score': array([0.96501241, 0.95626551, 0.97115385, 0.91098015, 0.95465261,\n",
              "        0.96544665, 0.95403226, 0.        , 0.88058313, 0.89621588,\n",
              "        0.85887097, 0.92022333, 0.96724566, 0.80384615, 0.73684864,\n",
              "        0.77884615, 0.96792804, 0.93411911, 0.94050868, 0.        ,\n",
              "        0.94863524, 0.93200993, 0.95849876, 0.93926799, 0.        ,\n",
              "        0.9117866 , 0.86271712, 0.91532258, 0.95880893, 0.91643921,\n",
              "        0.95744417, 0.        ]),\n",
              " 'param_balancing:strategy': masked_array(data=['none', 'weighting', 'none', 'weighting', 'weighting',\n",
              "                    'none', 'none', 'weighting', 'weighting', 'weighting',\n",
              "                    'weighting', 'none', 'weighting', 'weighting', 'none',\n",
              "                    'weighting', 'weighting', 'none', 'none', 'none',\n",
              "                    'none', 'weighting', 'none', 'none', 'none',\n",
              "                    'weighting', 'weighting', 'none', 'weighting', 'none',\n",
              "                    'weighting', 'weighting'],\n",
              "              mask=[False, False, False, False, False, False, False, False,\n",
              "                    False, False, False, False, False, False, False, False,\n",
              "                    False, False, False, False, False, False, False, False,\n",
              "                    False, False, False, False, False, False, False, False],\n",
              "        fill_value='N/A',\n",
              "             dtype='<U9'),\n",
              " 'param_categorical_encoding:__choice__': masked_array(data=['one_hot_encoding', 'no_encoding', 'one_hot_encoding',\n",
              "                    'no_encoding', 'one_hot_encoding', 'one_hot_encoding',\n",
              "                    'no_encoding', 'one_hot_encoding', 'one_hot_encoding',\n",
              "                    'one_hot_encoding', 'no_encoding', 'one_hot_encoding',\n",
              "                    'one_hot_encoding', 'one_hot_encoding',\n",
              "                    'one_hot_encoding', 'one_hot_encoding', 'no_encoding',\n",
              "                    'one_hot_encoding', 'one_hot_encoding', 'no_encoding',\n",
              "                    'one_hot_encoding', 'one_hot_encoding', 'no_encoding',\n",
              "                    'no_encoding', 'one_hot_encoding', 'no_encoding',\n",
              "                    'one_hot_encoding', 'no_encoding', 'one_hot_encoding',\n",
              "                    'no_encoding', 'one_hot_encoding', 'no_encoding'],\n",
              "              mask=[False, False, False, False, False, False, False, False,\n",
              "                    False, False, False, False, False, False, False, False,\n",
              "                    False, False, False, False, False, False, False, False,\n",
              "                    False, False, False, False, False, False, False, False],\n",
              "        fill_value='N/A',\n",
              "             dtype='<U16'),\n",
              " 'param_categorical_encoding:one_hot_encoding:minimum_fraction': masked_array(data=[0.01, --, --, --, --, --, --, --, 0.3837398524575939,\n",
              "                    --, --, --, --, 0.0003173723611800348, --,\n",
              "                    0.00034835629696198427, --, --, 0.00012586572428922356,\n",
              "                    --, 0.010000000000000004, --, --, --, --, --,\n",
              "                    0.010000000000000004, --, --, --, --, --],\n",
              "              mask=[False,  True,  True,  True,  True,  True,  True,  True,\n",
              "                    False,  True,  True,  True,  True, False,  True, False,\n",
              "                     True,  True, False,  True, False,  True,  True,  True,\n",
              "                     True,  True, False,  True,  True,  True,  True,  True],\n",
              "        fill_value=1e+20),\n",
              " 'param_categorical_encoding:one_hot_encoding:use_minimum_fraction': masked_array(data=['True', --, 'False', --, 'False', 'False', --, 'False',\n",
              "                    'True', 'False', --, 'False', 'False', 'True', 'False',\n",
              "                    'True', --, 'False', 'True', --, 'True', 'False', --,\n",
              "                    --, 'False', --, 'True', --, 'False', --, 'False', --],\n",
              "              mask=[False,  True, False,  True, False, False,  True, False,\n",
              "                    False, False,  True, False, False, False, False, False,\n",
              "                     True, False, False,  True, False, False,  True,  True,\n",
              "                    False,  True, False,  True, False,  True, False,  True],\n",
              "        fill_value='N/A',\n",
              "             dtype='<U5'),\n",
              " 'param_classifier:__choice__': masked_array(data=['random_forest', 'extra_trees', 'libsvm_svc',\n",
              "                    'k_nearest_neighbors', 'gradient_boosting',\n",
              "                    'extra_trees', 'gradient_boosting',\n",
              "                    'gradient_boosting', 'gradient_boosting',\n",
              "                    'extra_trees', 'extra_trees', 'random_forest',\n",
              "                    'extra_trees', 'lda', 'extra_trees', 'gaussian_nb',\n",
              "                    'extra_trees', 'gradient_boosting', 'random_forest',\n",
              "                    'gradient_boosting', 'extra_trees', 'random_forest',\n",
              "                    'gradient_boosting', 'extra_trees',\n",
              "                    'gradient_boosting', 'extra_trees',\n",
              "                    'gradient_boosting', 'adaboost', 'gradient_boosting',\n",
              "                    'k_nearest_neighbors', 'gradient_boosting', 'adaboost'],\n",
              "              mask=[False, False, False, False, False, False, False, False,\n",
              "                    False, False, False, False, False, False, False, False,\n",
              "                    False, False, False, False, False, False, False, False,\n",
              "                    False, False, False, False, False, False, False, False],\n",
              "        fill_value='N/A',\n",
              "             dtype='<U19'),\n",
              " 'param_classifier:adaboost:algorithm': masked_array(data=[--, --, --, --, --, --, --, --, --, --, --, --, --, --,\n",
              "                    --, --, --, --, --, --, --, --, --, --, --, --, --,\n",
              "                    'SAMME.R', --, --, --, 'SAMME'],\n",
              "              mask=[ True,  True,  True,  True,  True,  True,  True,  True,\n",
              "                     True,  True,  True,  True,  True,  True,  True,  True,\n",
              "                     True,  True,  True,  True,  True,  True,  True,  True,\n",
              "                     True,  True,  True, False,  True,  True,  True, False],\n",
              "        fill_value='N/A',\n",
              "             dtype='<U32'),\n",
              " 'param_classifier:adaboost:learning_rate': masked_array(data=[--, --, --, --, --, --, --, --, --, --, --, --, --, --,\n",
              "                    --, --, --, --, --, --, --, --, --, --, --, --, --,\n",
              "                    0.022579252711776604, --, --, --, 0.01819582056969416],\n",
              "              mask=[ True,  True,  True,  True,  True,  True,  True,  True,\n",
              "                     True,  True,  True,  True,  True,  True,  True,  True,\n",
              "                     True,  True,  True,  True,  True,  True,  True,  True,\n",
              "                     True,  True,  True, False,  True,  True,  True, False],\n",
              "        fill_value=1e+20),\n",
              " 'param_classifier:adaboost:max_depth': masked_array(data=[--, --, --, --, --, --, --, --, --, --, --, --, --, --,\n",
              "                    --, --, --, --, --, --, --, --, --, --, --, --, --,\n",
              "                    7.0, --, --, --, 9.0],\n",
              "              mask=[ True,  True,  True,  True,  True,  True,  True,  True,\n",
              "                     True,  True,  True,  True,  True,  True,  True,  True,\n",
              "                     True,  True,  True,  True,  True,  True,  True,  True,\n",
              "                     True,  True,  True, False,  True,  True,  True, False],\n",
              "        fill_value=1e+20),\n",
              " 'param_classifier:adaboost:n_estimators': masked_array(data=[--, --, --, --, --, --, --, --, --, --, --, --, --, --,\n",
              "                    --, --, --, --, --, --, --, --, --, --, --, --, --,\n",
              "                    157.0, --, --, --, 370.0],\n",
              "              mask=[ True,  True,  True,  True,  True,  True,  True,  True,\n",
              "                     True,  True,  True,  True,  True,  True,  True,  True,\n",
              "                     True,  True,  True,  True,  True,  True,  True,  True,\n",
              "                     True,  True,  True, False,  True,  True,  True, False],\n",
              "        fill_value=1e+20),\n",
              " 'param_classifier:bernoulli_nb:alpha': masked_array(data=[--, --, --, --, --, --, --, --, --, --, --, --, --, --,\n",
              "                    --, --, --, --, --, --, --, --, --, --, --, --, --, --,\n",
              "                    --, --, --, --],\n",
              "              mask=[ True,  True,  True,  True,  True,  True,  True,  True,\n",
              "                     True,  True,  True,  True,  True,  True,  True,  True,\n",
              "                     True,  True,  True,  True,  True,  True,  True,  True,\n",
              "                     True,  True,  True,  True,  True,  True,  True,  True],\n",
              "        fill_value=1e+20,\n",
              "             dtype=float64),\n",
              " 'param_classifier:bernoulli_nb:fit_prior': masked_array(data=[--, --, --, --, --, --, --, --, --, --, --, --, --, --,\n",
              "                    --, --, --, --, --, --, --, --, --, --, --, --, --, --,\n",
              "                    --, --, --, --],\n",
              "              mask=[ True,  True,  True,  True,  True,  True,  True,  True,\n",
              "                     True,  True,  True,  True,  True,  True,  True,  True,\n",
              "                     True,  True,  True,  True,  True,  True,  True,  True,\n",
              "                     True,  True,  True,  True,  True,  True,  True,  True],\n",
              "        fill_value=1e+20,\n",
              "             dtype=float64),\n",
              " 'param_classifier:decision_tree:criterion': masked_array(data=[--, --, --, --, --, --, --, --, --, --, --, --, --, --,\n",
              "                    --, --, --, --, --, --, --, --, --, --, --, --, --, --,\n",
              "                    --, --, --, --],\n",
              "              mask=[ True,  True,  True,  True,  True,  True,  True,  True,\n",
              "                     True,  True,  True,  True,  True,  True,  True,  True,\n",
              "                     True,  True,  True,  True,  True,  True,  True,  True,\n",
              "                     True,  True,  True,  True,  True,  True,  True,  True],\n",
              "        fill_value=1e+20,\n",
              "             dtype=float64),\n",
              " 'param_classifier:decision_tree:max_depth_factor': masked_array(data=[--, --, --, --, --, --, --, --, --, --, --, --, --, --,\n",
              "                    --, --, --, --, --, --, --, --, --, --, --, --, --, --,\n",
              "                    --, --, --, --],\n",
              "              mask=[ True,  True,  True,  True,  True,  True,  True,  True,\n",
              "                     True,  True,  True,  True,  True,  True,  True,  True,\n",
              "                     True,  True,  True,  True,  True,  True,  True,  True,\n",
              "                     True,  True,  True,  True,  True,  True,  True,  True],\n",
              "        fill_value=1e+20,\n",
              "             dtype=float64),\n",
              " 'param_classifier:decision_tree:max_features': masked_array(data=[--, --, --, --, --, --, --, --, --, --, --, --, --, --,\n",
              "                    --, --, --, --, --, --, --, --, --, --, --, --, --, --,\n",
              "                    --, --, --, --],\n",
              "              mask=[ True,  True,  True,  True,  True,  True,  True,  True,\n",
              "                     True,  True,  True,  True,  True,  True,  True,  True,\n",
              "                     True,  True,  True,  True,  True,  True,  True,  True,\n",
              "                     True,  True,  True,  True,  True,  True,  True,  True],\n",
              "        fill_value=1e+20,\n",
              "             dtype=float64),\n",
              " 'param_classifier:decision_tree:max_leaf_nodes': masked_array(data=[--, --, --, --, --, --, --, --, --, --, --, --, --, --,\n",
              "                    --, --, --, --, --, --, --, --, --, --, --, --, --, --,\n",
              "                    --, --, --, --],\n",
              "              mask=[ True,  True,  True,  True,  True,  True,  True,  True,\n",
              "                     True,  True,  True,  True,  True,  True,  True,  True,\n",
              "                     True,  True,  True,  True,  True,  True,  True,  True,\n",
              "                     True,  True,  True,  True,  True,  True,  True,  True],\n",
              "        fill_value=1e+20,\n",
              "             dtype=float64),\n",
              " 'param_classifier:decision_tree:min_impurity_decrease': masked_array(data=[--, --, --, --, --, --, --, --, --, --, --, --, --, --,\n",
              "                    --, --, --, --, --, --, --, --, --, --, --, --, --, --,\n",
              "                    --, --, --, --],\n",
              "              mask=[ True,  True,  True,  True,  True,  True,  True,  True,\n",
              "                     True,  True,  True,  True,  True,  True,  True,  True,\n",
              "                     True,  True,  True,  True,  True,  True,  True,  True,\n",
              "                     True,  True,  True,  True,  True,  True,  True,  True],\n",
              "        fill_value=1e+20,\n",
              "             dtype=float64),\n",
              " 'param_classifier:decision_tree:min_samples_leaf': masked_array(data=[--, --, --, --, --, --, --, --, --, --, --, --, --, --,\n",
              "                    --, --, --, --, --, --, --, --, --, --, --, --, --, --,\n",
              "                    --, --, --, --],\n",
              "              mask=[ True,  True,  True,  True,  True,  True,  True,  True,\n",
              "                     True,  True,  True,  True,  True,  True,  True,  True,\n",
              "                     True,  True,  True,  True,  True,  True,  True,  True,\n",
              "                     True,  True,  True,  True,  True,  True,  True,  True],\n",
              "        fill_value=1e+20,\n",
              "             dtype=float64),\n",
              " 'param_classifier:decision_tree:min_samples_split': masked_array(data=[--, --, --, --, --, --, --, --, --, --, --, --, --, --,\n",
              "                    --, --, --, --, --, --, --, --, --, --, --, --, --, --,\n",
              "                    --, --, --, --],\n",
              "              mask=[ True,  True,  True,  True,  True,  True,  True,  True,\n",
              "                     True,  True,  True,  True,  True,  True,  True,  True,\n",
              "                     True,  True,  True,  True,  True,  True,  True,  True,\n",
              "                     True,  True,  True,  True,  True,  True,  True,  True],\n",
              "        fill_value=1e+20,\n",
              "             dtype=float64),\n",
              " 'param_classifier:decision_tree:min_weight_fraction_leaf': masked_array(data=[--, --, --, --, --, --, --, --, --, --, --, --, --, --,\n",
              "                    --, --, --, --, --, --, --, --, --, --, --, --, --, --,\n",
              "                    --, --, --, --],\n",
              "              mask=[ True,  True,  True,  True,  True,  True,  True,  True,\n",
              "                     True,  True,  True,  True,  True,  True,  True,  True,\n",
              "                     True,  True,  True,  True,  True,  True,  True,  True,\n",
              "                     True,  True,  True,  True,  True,  True,  True,  True],\n",
              "        fill_value=1e+20,\n",
              "             dtype=float64),\n",
              " 'param_classifier:extra_trees:bootstrap': masked_array(data=[--, 'False', --, --, --, 'False', --, --, --, 'False',\n",
              "                    'False', --, 'False', --, 'False', --, 'False', --, --,\n",
              "                    --, 'True', --, --, 'False', --, 'True', --, --, --,\n",
              "                    --, --, --],\n",
              "              mask=[ True, False,  True,  True,  True, False,  True,  True,\n",
              "                     True, False, False,  True, False,  True, False,  True,\n",
              "                    False,  True,  True,  True, False,  True,  True, False,\n",
              "                     True, False,  True,  True,  True,  True,  True,  True],\n",
              "        fill_value='N/A',\n",
              "             dtype='<U32'),\n",
              " 'param_classifier:extra_trees:criterion': masked_array(data=[--, 'gini', --, --, --, 'gini', --, --, --, 'entropy',\n",
              "                    'entropy', --, 'gini', --, 'entropy', --, 'entropy',\n",
              "                    --, --, --, 'entropy', --, --, 'entropy', --,\n",
              "                    'entropy', --, --, --, --, --, --],\n",
              "              mask=[ True, False,  True,  True,  True, False,  True,  True,\n",
              "                     True, False, False,  True, False,  True, False,  True,\n",
              "                    False,  True,  True,  True, False,  True,  True, False,\n",
              "                     True, False,  True,  True,  True,  True,  True,  True],\n",
              "        fill_value='N/A',\n",
              "             dtype='<U32'),\n",
              " 'param_classifier:extra_trees:max_depth': masked_array(data=[--, 'None', --, --, --, 'None', --, --, --, 'None',\n",
              "                    'None', --, 'None', --, 'None', --, 'None', --, --, --,\n",
              "                    'None', --, --, 'None', --, 'None', --, --, --, --, --,\n",
              "                    --],\n",
              "              mask=[ True, False,  True,  True,  True, False,  True,  True,\n",
              "                     True, False, False,  True, False,  True, False,  True,\n",
              "                    False,  True,  True,  True, False,  True,  True, False,\n",
              "                     True, False,  True,  True,  True,  True,  True,  True],\n",
              "        fill_value='N/A',\n",
              "             dtype='<U32'),\n",
              " 'param_classifier:extra_trees:max_features': masked_array(data=[--, 0.5670424455696162, --, --, --, 0.5, --, --, --,\n",
              "                    0.5765793990908161, 0.9541039630394388, --,\n",
              "                    0.609975998293528, --, 0.39536192447534535, --,\n",
              "                    0.9455638720565651, --, --, --, 0.7062102387181676, --,\n",
              "                    --, 0.9990999604490303, --, 0.8338751730606484, --, --,\n",
              "                    --, --, --, --],\n",
              "              mask=[ True, False,  True,  True,  True, False,  True,  True,\n",
              "                     True, False, False,  True, False,  True, False,  True,\n",
              "                    False,  True,  True,  True, False,  True,  True, False,\n",
              "                     True, False,  True,  True,  True,  True,  True,  True],\n",
              "        fill_value=1e+20),\n",
              " 'param_classifier:extra_trees:max_leaf_nodes': masked_array(data=[--, 'None', --, --, --, 'None', --, --, --, 'None',\n",
              "                    'None', --, 'None', --, 'None', --, 'None', --, --, --,\n",
              "                    'None', --, --, 'None', --, 'None', --, --, --, --, --,\n",
              "                    --],\n",
              "              mask=[ True, False,  True,  True,  True, False,  True,  True,\n",
              "                     True, False, False,  True, False,  True, False,  True,\n",
              "                    False,  True,  True,  True, False,  True,  True, False,\n",
              "                     True, False,  True,  True,  True,  True,  True,  True],\n",
              "        fill_value='N/A',\n",
              "             dtype='<U32'),\n",
              " 'param_classifier:extra_trees:min_impurity_decrease': masked_array(data=[--, 0.0, --, --, --, 0.0, --, --, --, 0.0, 0.0, --,\n",
              "                    0.0, --, 0.0, --, 0.0, --, --, --, 0.0, --, --, 0.0,\n",
              "                    --, 0.0, --, --, --, --, --, --],\n",
              "              mask=[ True, False,  True,  True,  True, False,  True,  True,\n",
              "                     True, False, False,  True, False,  True, False,  True,\n",
              "                    False,  True,  True,  True, False,  True,  True, False,\n",
              "                     True, False,  True,  True,  True,  True,  True,  True],\n",
              "        fill_value=1e+20),\n",
              " 'param_classifier:extra_trees:min_samples_leaf': masked_array(data=[--, 8.0, --, --, --, 1.0, --, --, --, 11.0, 16.0, --,\n",
              "                    1.0, --, 19.0, --, 1.0, --, --, --, 1.0, --, --, 6.0,\n",
              "                    --, 17.0, --, --, --, --, --, --],\n",
              "              mask=[ True, False,  True,  True,  True, False,  True,  True,\n",
              "                     True, False, False,  True, False,  True, False,  True,\n",
              "                    False,  True,  True,  True, False,  True,  True, False,\n",
              "                     True, False,  True,  True,  True,  True,  True,  True],\n",
              "        fill_value=1e+20),\n",
              " 'param_classifier:extra_trees:min_samples_split': masked_array(data=[--, 16.0, --, --, --, 2.0, --, --, --, 20.0, 14.0, --,\n",
              "                    2.0, --, 3.0, --, 2.0, --, --, --, 20.0, --, --, 4.0,\n",
              "                    --, 12.0, --, --, --, --, --, --],\n",
              "              mask=[ True, False,  True,  True,  True, False,  True,  True,\n",
              "                     True, False, False,  True, False,  True, False,  True,\n",
              "                    False,  True,  True,  True, False,  True,  True, False,\n",
              "                     True, False,  True,  True,  True,  True,  True,  True],\n",
              "        fill_value=1e+20),\n",
              " 'param_classifier:extra_trees:min_weight_fraction_leaf': masked_array(data=[--, 0.0, --, --, --, 0.0, --, --, --, 0.0, 0.0, --,\n",
              "                    0.0, --, 0.0, --, 0.0, --, --, --, 0.0, --, --, 0.0,\n",
              "                    --, 0.0, --, --, --, --, --, --],\n",
              "              mask=[ True, False,  True,  True,  True, False,  True,  True,\n",
              "                     True, False, False,  True, False,  True, False,  True,\n",
              "                    False,  True,  True,  True, False,  True,  True, False,\n",
              "                     True, False,  True,  True,  True,  True,  True,  True],\n",
              "        fill_value=1e+20),\n",
              " 'param_classifier:extra_trees:n_estimators': masked_array(data=[--, 100.0, --, --, --, 100.0, --, --, --, 100.0, 100.0,\n",
              "                    --, 100.0, --, 100.0, --, 100.0, --, --, --, 100.0, --,\n",
              "                    --, 100.0, --, 100.0, --, --, --, --, --, --],\n",
              "              mask=[ True, False,  True,  True,  True, False,  True,  True,\n",
              "                     True, False, False,  True, False,  True, False,  True,\n",
              "                    False,  True,  True,  True, False,  True,  True, False,\n",
              "                     True, False,  True,  True,  True,  True,  True,  True],\n",
              "        fill_value=1e+20),\n",
              " 'param_classifier:gradient_boosting:criterion': masked_array(data=[--, --, --, --, 'mse', --, 'mse', 'friedman_mse',\n",
              "                    'mse', --, --, --, --, --, --, --, --, 'friedman_mse',\n",
              "                    --, 'friedman_mse', --, --, 'mse', --, 'mae', --,\n",
              "                    'mse', --, 'mse', --, 'mse', --],\n",
              "              mask=[ True,  True,  True,  True, False,  True, False, False,\n",
              "                    False,  True,  True,  True,  True,  True,  True,  True,\n",
              "                     True, False,  True, False,  True,  True, False,  True,\n",
              "                    False,  True, False,  True, False,  True, False,  True],\n",
              "        fill_value='N/A',\n",
              "             dtype='<U32'),\n",
              " 'param_classifier:gradient_boosting:learning_rate': masked_array(data=[--, --, --, --, 0.1958974686405233, --,\n",
              "                    0.02102683283349326, 0.07463196642416368,\n",
              "                    0.018356703878357986, --, --, --, --, --, --, --, --,\n",
              "                    0.24729845478857812, --, 0.04093642460278944, --, --,\n",
              "                    0.24729845478857812, --, 0.24729845478857812, --,\n",
              "                    0.018356703878357986, --, 0.24729845478857812, --,\n",
              "                    0.24729845478857812, --],\n",
              "              mask=[ True,  True,  True,  True, False,  True, False, False,\n",
              "                    False,  True,  True,  True,  True,  True,  True,  True,\n",
              "                     True, False,  True, False,  True,  True, False,  True,\n",
              "                    False,  True, False,  True, False,  True, False,  True],\n",
              "        fill_value=1e+20),\n",
              " 'param_classifier:gradient_boosting:loss': masked_array(data=[--, --, --, --, 'deviance', --, 'deviance', 'deviance',\n",
              "                    'deviance', --, --, --, --, --, --, --, --, 'deviance',\n",
              "                    --, 'deviance', --, --, 'deviance', --, 'deviance', --,\n",
              "                    'deviance', --, 'deviance', --, 'deviance', --],\n",
              "              mask=[ True,  True,  True,  True, False,  True, False, False,\n",
              "                    False,  True,  True,  True,  True,  True,  True,  True,\n",
              "                     True, False,  True, False,  True,  True, False,  True,\n",
              "                    False,  True, False,  True, False,  True, False,  True],\n",
              "        fill_value='N/A',\n",
              "             dtype='<U32'),\n",
              " 'param_classifier:gradient_boosting:max_depth': masked_array(data=[--, --, --, --, 5.0, --, 10.0, 7.0, 3.0, --, --, --,\n",
              "                    --, --, --, --, --, 3.0, --, 7.0, --, --, 5.0, --, 3.0,\n",
              "                    --, 3.0, --, 6.0, --, 6.0, --],\n",
              "              mask=[ True,  True,  True,  True, False,  True, False, False,\n",
              "                    False,  True,  True,  True,  True,  True,  True,  True,\n",
              "                     True, False,  True, False,  True,  True, False,  True,\n",
              "                    False,  True, False,  True, False,  True, False,  True],\n",
              "        fill_value=1e+20),\n",
              " 'param_classifier:gradient_boosting:max_features': masked_array(data=[--, --, --, --, 0.33885235607979314, --,\n",
              "                    0.2797288369369436, 0.8603242247379981,\n",
              "                    0.9690352514774068, --, --, --, --, --, --, --, --,\n",
              "                    0.6564306719064884, --, 0.5495014745530306, --, --,\n",
              "                    0.6564306719064884, --, 0.6564306719064884, --,\n",
              "                    0.4445141928073373, --, 0.6564306719064884, --,\n",
              "                    0.6564306719064884, --],\n",
              "              mask=[ True,  True,  True,  True, False,  True, False, False,\n",
              "                    False,  True,  True,  True,  True,  True,  True,  True,\n",
              "                     True, False,  True, False,  True,  True, False,  True,\n",
              "                    False,  True, False,  True, False,  True, False,  True],\n",
              "        fill_value=1e+20),\n",
              " 'param_classifier:gradient_boosting:max_leaf_nodes': masked_array(data=[--, --, --, --, 'None', --, 'None', 'None', 'None', --,\n",
              "                    --, --, --, --, --, --, --, 'None', --, 'None', --, --,\n",
              "                    'None', --, 'None', --, 'None', --, 'None', --, 'None',\n",
              "                    --],\n",
              "              mask=[ True,  True,  True,  True, False,  True, False, False,\n",
              "                    False,  True,  True,  True,  True,  True,  True,  True,\n",
              "                     True, False,  True, False,  True,  True, False,  True,\n",
              "                    False,  True, False,  True, False,  True, False,  True],\n",
              "        fill_value='N/A',\n",
              "             dtype='<U32'),\n",
              " 'param_classifier:gradient_boosting:min_impurity_decrease': masked_array(data=[--, --, --, --, 0.0, --, 0.0, 0.0, 0.0, --, --, --, --,\n",
              "                    --, --, --, --, 0.0, --, 0.0, --, --, 0.0, --, 0.0, --,\n",
              "                    0.0, --, 0.0, --, 0.0, --],\n",
              "              mask=[ True,  True,  True,  True, False,  True, False, False,\n",
              "                    False,  True,  True,  True,  True,  True,  True,  True,\n",
              "                     True, False,  True, False,  True,  True, False,  True,\n",
              "                    False,  True, False,  True, False,  True, False,  True],\n",
              "        fill_value=1e+20),\n",
              " 'param_classifier:gradient_boosting:min_samples_leaf': masked_array(data=[--, --, --, --, 6.0, --, 14.0, 2.0, 12.0, --, --, --,\n",
              "                    --, --, --, --, --, 15.0, --, 20.0, --, --, 15.0, --,\n",
              "                    15.0, --, 19.0, --, 15.0, --, 15.0, --],\n",
              "              mask=[ True,  True,  True,  True, False,  True, False, False,\n",
              "                    False,  True,  True,  True,  True,  True,  True,  True,\n",
              "                     True, False,  True, False,  True,  True, False,  True,\n",
              "                    False,  True, False,  True, False,  True, False,  True],\n",
              "        fill_value=1e+20),\n",
              " 'param_classifier:gradient_boosting:min_samples_split': masked_array(data=[--, --, --, --, 4.0, --, 9.0, 6.0, 3.0, --, --, --, --,\n",
              "                    --, --, --, --, 14.0, --, 18.0, --, --, 14.0, --, 14.0,\n",
              "                    --, 3.0, --, 14.0, --, 14.0, --],\n",
              "              mask=[ True,  True,  True,  True, False,  True, False, False,\n",
              "                    False,  True,  True,  True,  True,  True,  True,  True,\n",
              "                     True, False,  True, False,  True,  True, False,  True,\n",
              "                    False,  True, False,  True, False,  True, False,  True],\n",
              "        fill_value=1e+20),\n",
              " 'param_classifier:gradient_boosting:min_weight_fraction_leaf': masked_array(data=[--, --, --, --, 0.0, --, 0.0, 0.0, 0.0, --, --, --, --,\n",
              "                    --, --, --, --, 0.0, --, 0.0, --, --, 0.0, --, 0.0, --,\n",
              "                    0.0, --, 0.0, --, 0.0, --],\n",
              "              mask=[ True,  True,  True,  True, False,  True, False, False,\n",
              "                    False,  True,  True,  True,  True,  True,  True,  True,\n",
              "                     True, False,  True, False,  True,  True, False,  True,\n",
              "                    False,  True, False,  True, False,  True, False,  True],\n",
              "        fill_value=1e+20),\n",
              " 'param_classifier:gradient_boosting:n_estimators': masked_array(data=[--, --, --, --, 125.0, --, 480.0, 500.0, 234.0, --, --,\n",
              "                    --, --, --, --, --, --, 220.0, --, 141.0, --, --,\n",
              "                    220.0, --, 220.0, --, 234.0, --, 220.0, --, 220.0, --],\n",
              "              mask=[ True,  True,  True,  True, False,  True, False, False,\n",
              "                    False,  True,  True,  True,  True,  True,  True,  True,\n",
              "                     True, False,  True, False,  True,  True, False,  True,\n",
              "                    False,  True, False,  True, False,  True, False,  True],\n",
              "        fill_value=1e+20),\n",
              " 'param_classifier:gradient_boosting:subsample': masked_array(data=[--, --, --, --, 0.9448890820738562, --,\n",
              "                    0.5778972273820631, 0.8447665577491962,\n",
              "                    0.3870344708308441, --, --, --, --, --, --, --, --,\n",
              "                    0.8082564085714649, --, 0.6905343807995293, --, --,\n",
              "                    0.8082564085714649, --, 0.8082564085714649, --,\n",
              "                    0.3870344708308441, --, 0.8082564085714649, --,\n",
              "                    0.8082564085714649, --],\n",
              "              mask=[ True,  True,  True,  True, False,  True, False, False,\n",
              "                    False,  True,  True,  True,  True,  True,  True,  True,\n",
              "                     True, False,  True, False,  True,  True, False,  True,\n",
              "                    False,  True, False,  True, False,  True, False,  True],\n",
              "        fill_value=1e+20),\n",
              " 'param_classifier:k_nearest_neighbors:n_neighbors': masked_array(data=[--, --, --, 59.0, --, --, --, --, --, --, --, --, --,\n",
              "                    --, --, --, --, --, --, --, --, --, --, --, --, --, --,\n",
              "                    --, --, 5.0, --, --],\n",
              "              mask=[ True,  True,  True, False,  True,  True,  True,  True,\n",
              "                     True,  True,  True,  True,  True,  True,  True,  True,\n",
              "                     True,  True,  True,  True,  True,  True,  True,  True,\n",
              "                     True,  True,  True,  True,  True, False,  True,  True],\n",
              "        fill_value=1e+20),\n",
              " 'param_classifier:k_nearest_neighbors:p': masked_array(data=[--, --, --, 1.0, --, --, --, --, --, --, --, --, --,\n",
              "                    --, --, --, --, --, --, --, --, --, --, --, --, --, --,\n",
              "                    --, --, 2.0, --, --],\n",
              "              mask=[ True,  True,  True, False,  True,  True,  True,  True,\n",
              "                     True,  True,  True,  True,  True,  True,  True,  True,\n",
              "                     True,  True,  True,  True,  True,  True,  True,  True,\n",
              "                     True,  True,  True,  True,  True, False,  True,  True],\n",
              "        fill_value=1e+20),\n",
              " 'param_classifier:k_nearest_neighbors:weights': masked_array(data=[--, --, --, 'distance', --, --, --, --, --, --, --, --,\n",
              "                    --, --, --, --, --, --, --, --, --, --, --, --, --, --,\n",
              "                    --, --, --, 'uniform', --, --],\n",
              "              mask=[ True,  True,  True, False,  True,  True,  True,  True,\n",
              "                     True,  True,  True,  True,  True,  True,  True,  True,\n",
              "                     True,  True,  True,  True,  True,  True,  True,  True,\n",
              "                     True,  True,  True,  True,  True, False,  True,  True],\n",
              "        fill_value='N/A',\n",
              "             dtype='<U32'),\n",
              " 'param_classifier:lda:n_components': masked_array(data=[--, --, --, --, --, --, --, --, --, --, --, --, --,\n",
              "                    244.0, --, --, --, --, --, --, --, --, --, --, --, --,\n",
              "                    --, --, --, --, --, --],\n",
              "              mask=[ True,  True,  True,  True,  True,  True,  True,  True,\n",
              "                     True,  True,  True,  True,  True, False,  True,  True,\n",
              "                     True,  True,  True,  True,  True,  True,  True,  True,\n",
              "                     True,  True,  True,  True,  True,  True,  True,  True],\n",
              "        fill_value=1e+20),\n",
              " 'param_classifier:lda:shrinkage': masked_array(data=[--, --, --, --, --, --, --, --, --, --, --, --, --,\n",
              "                    'None', --, --, --, --, --, --, --, --, --, --, --, --,\n",
              "                    --, --, --, --, --, --],\n",
              "              mask=[ True,  True,  True,  True,  True,  True,  True,  True,\n",
              "                     True,  True,  True,  True,  True, False,  True,  True,\n",
              "                     True,  True,  True,  True,  True,  True,  True,  True,\n",
              "                     True,  True,  True,  True,  True,  True,  True,  True],\n",
              "        fill_value='N/A',\n",
              "             dtype='<U32'),\n",
              " 'param_classifier:lda:shrinkage_factor': masked_array(data=[--, --, --, --, --, --, --, --, --, --, --, --, --, --,\n",
              "                    --, --, --, --, --, --, --, --, --, --, --, --, --, --,\n",
              "                    --, --, --, --],\n",
              "              mask=[ True,  True,  True,  True,  True,  True,  True,  True,\n",
              "                     True,  True,  True,  True,  True,  True,  True,  True,\n",
              "                     True,  True,  True,  True,  True,  True,  True,  True,\n",
              "                     True,  True,  True,  True,  True,  True,  True,  True],\n",
              "        fill_value=1e+20,\n",
              "             dtype=float64),\n",
              " 'param_classifier:lda:tol': masked_array(data=[--, --, --, --, --, --, --, --, --, --, --, --, --,\n",
              "                    2.3065111488706024e-05, --, --, --, --, --, --, --, --,\n",
              "                    --, --, --, --, --, --, --, --, --, --],\n",
              "              mask=[ True,  True,  True,  True,  True,  True,  True,  True,\n",
              "                     True,  True,  True,  True,  True, False,  True,  True,\n",
              "                     True,  True,  True,  True,  True,  True,  True,  True,\n",
              "                     True,  True,  True,  True,  True,  True,  True,  True],\n",
              "        fill_value=1e+20),\n",
              " 'param_classifier:liblinear_svc:C': masked_array(data=[--, --, --, --, --, --, --, --, --, --, --, --, --, --,\n",
              "                    --, --, --, --, --, --, --, --, --, --, --, --, --, --,\n",
              "                    --, --, --, --],\n",
              "              mask=[ True,  True,  True,  True,  True,  True,  True,  True,\n",
              "                     True,  True,  True,  True,  True,  True,  True,  True,\n",
              "                     True,  True,  True,  True,  True,  True,  True,  True,\n",
              "                     True,  True,  True,  True,  True,  True,  True,  True],\n",
              "        fill_value=1e+20,\n",
              "             dtype=float64),\n",
              " 'param_classifier:liblinear_svc:dual': masked_array(data=[--, --, --, --, --, --, --, --, --, --, --, --, --, --,\n",
              "                    --, --, --, --, --, --, --, --, --, --, --, --, --, --,\n",
              "                    --, --, --, --],\n",
              "              mask=[ True,  True,  True,  True,  True,  True,  True,  True,\n",
              "                     True,  True,  True,  True,  True,  True,  True,  True,\n",
              "                     True,  True,  True,  True,  True,  True,  True,  True,\n",
              "                     True,  True,  True,  True,  True,  True,  True,  True],\n",
              "        fill_value=1e+20,\n",
              "             dtype=float64),\n",
              " 'param_classifier:liblinear_svc:fit_intercept': masked_array(data=[--, --, --, --, --, --, --, --, --, --, --, --, --, --,\n",
              "                    --, --, --, --, --, --, --, --, --, --, --, --, --, --,\n",
              "                    --, --, --, --],\n",
              "              mask=[ True,  True,  True,  True,  True,  True,  True,  True,\n",
              "                     True,  True,  True,  True,  True,  True,  True,  True,\n",
              "                     True,  True,  True,  True,  True,  True,  True,  True,\n",
              "                     True,  True,  True,  True,  True,  True,  True,  True],\n",
              "        fill_value=1e+20,\n",
              "             dtype=float64),\n",
              " 'param_classifier:liblinear_svc:intercept_scaling': masked_array(data=[--, --, --, --, --, --, --, --, --, --, --, --, --, --,\n",
              "                    --, --, --, --, --, --, --, --, --, --, --, --, --, --,\n",
              "                    --, --, --, --],\n",
              "              mask=[ True,  True,  True,  True,  True,  True,  True,  True,\n",
              "                     True,  True,  True,  True,  True,  True,  True,  True,\n",
              "                     True,  True,  True,  True,  True,  True,  True,  True,\n",
              "                     True,  True,  True,  True,  True,  True,  True,  True],\n",
              "        fill_value=1e+20,\n",
              "             dtype=float64),\n",
              " 'param_classifier:liblinear_svc:loss': masked_array(data=[--, --, --, --, --, --, --, --, --, --, --, --, --, --,\n",
              "                    --, --, --, --, --, --, --, --, --, --, --, --, --, --,\n",
              "                    --, --, --, --],\n",
              "              mask=[ True,  True,  True,  True,  True,  True,  True,  True,\n",
              "                     True,  True,  True,  True,  True,  True,  True,  True,\n",
              "                     True,  True,  True,  True,  True,  True,  True,  True,\n",
              "                     True,  True,  True,  True,  True,  True,  True,  True],\n",
              "        fill_value=1e+20,\n",
              "             dtype=float64),\n",
              " 'param_classifier:liblinear_svc:multi_class': masked_array(data=[--, --, --, --, --, --, --, --, --, --, --, --, --, --,\n",
              "                    --, --, --, --, --, --, --, --, --, --, --, --, --, --,\n",
              "                    --, --, --, --],\n",
              "              mask=[ True,  True,  True,  True,  True,  True,  True,  True,\n",
              "                     True,  True,  True,  True,  True,  True,  True,  True,\n",
              "                     True,  True,  True,  True,  True,  True,  True,  True,\n",
              "                     True,  True,  True,  True,  True,  True,  True,  True],\n",
              "        fill_value=1e+20,\n",
              "             dtype=float64),\n",
              " 'param_classifier:liblinear_svc:penalty': masked_array(data=[--, --, --, --, --, --, --, --, --, --, --, --, --, --,\n",
              "                    --, --, --, --, --, --, --, --, --, --, --, --, --, --,\n",
              "                    --, --, --, --],\n",
              "              mask=[ True,  True,  True,  True,  True,  True,  True,  True,\n",
              "                     True,  True,  True,  True,  True,  True,  True,  True,\n",
              "                     True,  True,  True,  True,  True,  True,  True,  True,\n",
              "                     True,  True,  True,  True,  True,  True,  True,  True],\n",
              "        fill_value=1e+20,\n",
              "             dtype=float64),\n",
              " 'param_classifier:liblinear_svc:tol': masked_array(data=[--, --, --, --, --, --, --, --, --, --, --, --, --, --,\n",
              "                    --, --, --, --, --, --, --, --, --, --, --, --, --, --,\n",
              "                    --, --, --, --],\n",
              "              mask=[ True,  True,  True,  True,  True,  True,  True,  True,\n",
              "                     True,  True,  True,  True,  True,  True,  True,  True,\n",
              "                     True,  True,  True,  True,  True,  True,  True,  True,\n",
              "                     True,  True,  True,  True,  True,  True,  True,  True],\n",
              "        fill_value=1e+20,\n",
              "             dtype=float64),\n",
              " 'param_classifier:libsvm_svc:C': masked_array(data=[--, --, 6.342897164595882, --, --, --, --, --, --, --,\n",
              "                    --, --, --, --, --, --, --, --, --, --, --, --, --, --,\n",
              "                    --, --, --, --, --, --, --, --],\n",
              "              mask=[ True,  True, False,  True,  True,  True,  True,  True,\n",
              "                     True,  True,  True,  True,  True,  True,  True,  True,\n",
              "                     True,  True,  True,  True,  True,  True,  True,  True,\n",
              "                     True,  True,  True,  True,  True,  True,  True,  True],\n",
              "        fill_value=1e+20),\n",
              " 'param_classifier:libsvm_svc:coef0': masked_array(data=[--, --, --, --, --, --, --, --, --, --, --, --, --, --,\n",
              "                    --, --, --, --, --, --, --, --, --, --, --, --, --, --,\n",
              "                    --, --, --, --],\n",
              "              mask=[ True,  True,  True,  True,  True,  True,  True,  True,\n",
              "                     True,  True,  True,  True,  True,  True,  True,  True,\n",
              "                     True,  True,  True,  True,  True,  True,  True,  True,\n",
              "                     True,  True,  True,  True,  True,  True,  True,  True],\n",
              "        fill_value=1e+20,\n",
              "             dtype=float64),\n",
              " 'param_classifier:libsvm_svc:degree': masked_array(data=[--, --, --, --, --, --, --, --, --, --, --, --, --, --,\n",
              "                    --, --, --, --, --, --, --, --, --, --, --, --, --, --,\n",
              "                    --, --, --, --],\n",
              "              mask=[ True,  True,  True,  True,  True,  True,  True,  True,\n",
              "                     True,  True,  True,  True,  True,  True,  True,  True,\n",
              "                     True,  True,  True,  True,  True,  True,  True,  True,\n",
              "                     True,  True,  True,  True,  True,  True,  True,  True],\n",
              "        fill_value=1e+20,\n",
              "             dtype=float64),\n",
              " 'param_classifier:libsvm_svc:gamma': masked_array(data=[--, --, 0.2229870623330047, --, --, --, --, --, --, --,\n",
              "                    --, --, --, --, --, --, --, --, --, --, --, --, --, --,\n",
              "                    --, --, --, --, --, --, --, --],\n",
              "              mask=[ True,  True, False,  True,  True,  True,  True,  True,\n",
              "                     True,  True,  True,  True,  True,  True,  True,  True,\n",
              "                     True,  True,  True,  True,  True,  True,  True,  True,\n",
              "                     True,  True,  True,  True,  True,  True,  True,  True],\n",
              "        fill_value=1e+20),\n",
              " 'param_classifier:libsvm_svc:kernel': masked_array(data=[--, --, 'rbf', --, --, --, --, --, --, --, --, --, --,\n",
              "                    --, --, --, --, --, --, --, --, --, --, --, --, --, --,\n",
              "                    --, --, --, --, --],\n",
              "              mask=[ True,  True, False,  True,  True,  True,  True,  True,\n",
              "                     True,  True,  True,  True,  True,  True,  True,  True,\n",
              "                     True,  True,  True,  True,  True,  True,  True,  True,\n",
              "                     True,  True,  True,  True,  True,  True,  True,  True],\n",
              "        fill_value='N/A',\n",
              "             dtype='<U32'),\n",
              " 'param_classifier:libsvm_svc:max_iter': masked_array(data=[--, --, -1.0, --, --, --, --, --, --, --, --, --, --,\n",
              "                    --, --, --, --, --, --, --, --, --, --, --, --, --, --,\n",
              "                    --, --, --, --, --],\n",
              "              mask=[ True,  True, False,  True,  True,  True,  True,  True,\n",
              "                     True,  True,  True,  True,  True,  True,  True,  True,\n",
              "                     True,  True,  True,  True,  True,  True,  True,  True,\n",
              "                     True,  True,  True,  True,  True,  True,  True,  True],\n",
              "        fill_value=1e+20),\n",
              " 'param_classifier:libsvm_svc:shrinking': masked_array(data=[--, --, 'False', --, --, --, --, --, --, --, --, --,\n",
              "                    --, --, --, --, --, --, --, --, --, --, --, --, --, --,\n",
              "                    --, --, --, --, --, --],\n",
              "              mask=[ True,  True, False,  True,  True,  True,  True,  True,\n",
              "                     True,  True,  True,  True,  True,  True,  True,  True,\n",
              "                     True,  True,  True,  True,  True,  True,  True,  True,\n",
              "                     True,  True,  True,  True,  True,  True,  True,  True],\n",
              "        fill_value='N/A',\n",
              "             dtype='<U32'),\n",
              " 'param_classifier:libsvm_svc:tol': masked_array(data=[--, --, 2.006345264381097e-05, --, --, --, --, --, --,\n",
              "                    --, --, --, --, --, --, --, --, --, --, --, --, --, --,\n",
              "                    --, --, --, --, --, --, --, --, --],\n",
              "              mask=[ True,  True, False,  True,  True,  True,  True,  True,\n",
              "                     True,  True,  True,  True,  True,  True,  True,  True,\n",
              "                     True,  True,  True,  True,  True,  True,  True,  True,\n",
              "                     True,  True,  True,  True,  True,  True,  True,  True],\n",
              "        fill_value=1e+20),\n",
              " 'param_classifier:multinomial_nb:alpha': masked_array(data=[--, --, --, --, --, --, --, --, --, --, --, --, --, --,\n",
              "                    --, --, --, --, --, --, --, --, --, --, --, --, --, --,\n",
              "                    --, --, --, --],\n",
              "              mask=[ True,  True,  True,  True,  True,  True,  True,  True,\n",
              "                     True,  True,  True,  True,  True,  True,  True,  True,\n",
              "                     True,  True,  True,  True,  True,  True,  True,  True,\n",
              "                     True,  True,  True,  True,  True,  True,  True,  True],\n",
              "        fill_value=1e+20,\n",
              "             dtype=float64),\n",
              " 'param_classifier:multinomial_nb:fit_prior': masked_array(data=[--, --, --, --, --, --, --, --, --, --, --, --, --, --,\n",
              "                    --, --, --, --, --, --, --, --, --, --, --, --, --, --,\n",
              "                    --, --, --, --],\n",
              "              mask=[ True,  True,  True,  True,  True,  True,  True,  True,\n",
              "                     True,  True,  True,  True,  True,  True,  True,  True,\n",
              "                     True,  True,  True,  True,  True,  True,  True,  True,\n",
              "                     True,  True,  True,  True,  True,  True,  True,  True],\n",
              "        fill_value=1e+20,\n",
              "             dtype=float64),\n",
              " 'param_classifier:passive_aggressive:C': masked_array(data=[--, --, --, --, --, --, --, --, --, --, --, --, --, --,\n",
              "                    --, --, --, --, --, --, --, --, --, --, --, --, --, --,\n",
              "                    --, --, --, --],\n",
              "              mask=[ True,  True,  True,  True,  True,  True,  True,  True,\n",
              "                     True,  True,  True,  True,  True,  True,  True,  True,\n",
              "                     True,  True,  True,  True,  True,  True,  True,  True,\n",
              "                     True,  True,  True,  True,  True,  True,  True,  True],\n",
              "        fill_value=1e+20,\n",
              "             dtype=float64),\n",
              " 'param_classifier:passive_aggressive:average': masked_array(data=[--, --, --, --, --, --, --, --, --, --, --, --, --, --,\n",
              "                    --, --, --, --, --, --, --, --, --, --, --, --, --, --,\n",
              "                    --, --, --, --],\n",
              "              mask=[ True,  True,  True,  True,  True,  True,  True,  True,\n",
              "                     True,  True,  True,  True,  True,  True,  True,  True,\n",
              "                     True,  True,  True,  True,  True,  True,  True,  True,\n",
              "                     True,  True,  True,  True,  True,  True,  True,  True],\n",
              "        fill_value=1e+20,\n",
              "             dtype=float64),\n",
              " 'param_classifier:passive_aggressive:fit_intercept': masked_array(data=[--, --, --, --, --, --, --, --, --, --, --, --, --, --,\n",
              "                    --, --, --, --, --, --, --, --, --, --, --, --, --, --,\n",
              "                    --, --, --, --],\n",
              "              mask=[ True,  True,  True,  True,  True,  True,  True,  True,\n",
              "                     True,  True,  True,  True,  True,  True,  True,  True,\n",
              "                     True,  True,  True,  True,  True,  True,  True,  True,\n",
              "                     True,  True,  True,  True,  True,  True,  True,  True],\n",
              "        fill_value=1e+20,\n",
              "             dtype=float64),\n",
              " 'param_classifier:passive_aggressive:loss': masked_array(data=[--, --, --, --, --, --, --, --, --, --, --, --, --, --,\n",
              "                    --, --, --, --, --, --, --, --, --, --, --, --, --, --,\n",
              "                    --, --, --, --],\n",
              "              mask=[ True,  True,  True,  True,  True,  True,  True,  True,\n",
              "                     True,  True,  True,  True,  True,  True,  True,  True,\n",
              "                     True,  True,  True,  True,  True,  True,  True,  True,\n",
              "                     True,  True,  True,  True,  True,  True,  True,  True],\n",
              "        fill_value=1e+20,\n",
              "             dtype=float64),\n",
              " 'param_classifier:passive_aggressive:tol': masked_array(data=[--, --, --, --, --, --, --, --, --, --, --, --, --, --,\n",
              "                    --, --, --, --, --, --, --, --, --, --, --, --, --, --,\n",
              "                    --, --, --, --],\n",
              "              mask=[ True,  True,  True,  True,  True,  True,  True,  True,\n",
              "                     True,  True,  True,  True,  True,  True,  True,  True,\n",
              "                     True,  True,  True,  True,  True,  True,  True,  True,\n",
              "                     True,  True,  True,  True,  True,  True,  True,  True],\n",
              "        fill_value=1e+20,\n",
              "             dtype=float64),\n",
              " 'param_classifier:qda:reg_param': masked_array(data=[--, --, --, --, --, --, --, --, --, --, --, --, --, --,\n",
              "                    --, --, --, --, --, --, --, --, --, --, --, --, --, --,\n",
              "                    --, --, --, --],\n",
              "              mask=[ True,  True,  True,  True,  True,  True,  True,  True,\n",
              "                     True,  True,  True,  True,  True,  True,  True,  True,\n",
              "                     True,  True,  True,  True,  True,  True,  True,  True,\n",
              "                     True,  True,  True,  True,  True,  True,  True,  True],\n",
              "        fill_value=1e+20,\n",
              "             dtype=float64),\n",
              " 'param_classifier:random_forest:bootstrap': masked_array(data=['True', --, --, --, --, --, --, --, --, --, --, 'True',\n",
              "                    --, --, --, --, --, --, 'True', --, --, 'False', --,\n",
              "                    --, --, --, --, --, --, --, --, --],\n",
              "              mask=[False,  True,  True,  True,  True,  True,  True,  True,\n",
              "                     True,  True,  True, False,  True,  True,  True,  True,\n",
              "                     True,  True, False,  True,  True, False,  True,  True,\n",
              "                     True,  True,  True,  True,  True,  True,  True,  True],\n",
              "        fill_value='N/A',\n",
              "             dtype='<U5'),\n",
              " 'param_classifier:random_forest:criterion': masked_array(data=['gini', --, --, --, --, --, --, --, --, --, --, 'gini',\n",
              "                    --, --, --, --, --, --, 'gini', --, --, 'gini', --, --,\n",
              "                    --, --, --, --, --, --, --, --],\n",
              "              mask=[False,  True,  True,  True,  True,  True,  True,  True,\n",
              "                     True,  True,  True, False,  True,  True,  True,  True,\n",
              "                     True,  True, False,  True,  True, False,  True,  True,\n",
              "                     True,  True,  True,  True,  True,  True,  True,  True],\n",
              "        fill_value='N/A',\n",
              "             dtype='<U4'),\n",
              " 'param_classifier:random_forest:max_depth': masked_array(data=['None', --, --, --, --, --, --, --, --, --, --, 'None',\n",
              "                    --, --, --, --, --, --, 'None', --, --, 'None', --, --,\n",
              "                    --, --, --, --, --, --, --, --],\n",
              "              mask=[False,  True,  True,  True,  True,  True,  True,  True,\n",
              "                     True,  True,  True, False,  True,  True,  True,  True,\n",
              "                     True,  True, False,  True,  True, False,  True,  True,\n",
              "                     True,  True,  True,  True,  True,  True,  True,  True],\n",
              "        fill_value='N/A',\n",
              "             dtype='<U4'),\n",
              " 'param_classifier:random_forest:max_features': masked_array(data=[0.5, --, --, --, --, --, --, --, --, --, --,\n",
              "                    0.9260795160807372, --, --, --, --, --, --,\n",
              "                    0.5240592829918601, --, --, 0.7983157215145903, --, --,\n",
              "                    --, --, --, --, --, --, --, --],\n",
              "              mask=[False,  True,  True,  True,  True,  True,  True,  True,\n",
              "                     True,  True,  True, False,  True,  True,  True,  True,\n",
              "                     True,  True, False,  True,  True, False,  True,  True,\n",
              "                     True,  True,  True,  True,  True,  True,  True,  True],\n",
              "        fill_value=1e+20),\n",
              " 'param_classifier:random_forest:max_leaf_nodes': masked_array(data=['None', --, --, --, --, --, --, --, --, --, --, 'None',\n",
              "                    --, --, --, --, --, --, 'None', --, --, 'None', --, --,\n",
              "                    --, --, --, --, --, --, --, --],\n",
              "              mask=[False,  True,  True,  True,  True,  True,  True,  True,\n",
              "                     True,  True,  True, False,  True,  True,  True,  True,\n",
              "                     True,  True, False,  True,  True, False,  True,  True,\n",
              "                     True,  True,  True,  True,  True,  True,  True,  True],\n",
              "        fill_value='N/A',\n",
              "             dtype='<U4'),\n",
              " 'param_classifier:random_forest:min_impurity_decrease': masked_array(data=[0.0, --, --, --, --, --, --, --, --, --, --, 0.0, --,\n",
              "                    --, --, --, --, --, 0.0, --, --, 0.0, --, --, --, --,\n",
              "                    --, --, --, --, --, --],\n",
              "              mask=[False,  True,  True,  True,  True,  True,  True,  True,\n",
              "                     True,  True,  True, False,  True,  True,  True,  True,\n",
              "                     True,  True, False,  True,  True, False,  True,  True,\n",
              "                     True,  True,  True,  True,  True,  True,  True,  True],\n",
              "        fill_value=1e+20),\n",
              " 'param_classifier:random_forest:min_samples_leaf': masked_array(data=[1.0, --, --, --, --, --, --, --, --, --, --, 17.0, --,\n",
              "                    --, --, --, --, --, 10.0, --, --, 4.0, --, --, --, --,\n",
              "                    --, --, --, --, --, --],\n",
              "              mask=[False,  True,  True,  True,  True,  True,  True,  True,\n",
              "                     True,  True,  True, False,  True,  True,  True,  True,\n",
              "                     True,  True, False,  True,  True, False,  True,  True,\n",
              "                     True,  True,  True,  True,  True,  True,  True,  True],\n",
              "        fill_value=1e+20),\n",
              " 'param_classifier:random_forest:min_samples_split': masked_array(data=[2.0, --, --, --, --, --, --, --, --, --, --, 7.0, --,\n",
              "                    --, --, --, --, --, 16.0, --, --, 15.0, --, --, --, --,\n",
              "                    --, --, --, --, --, --],\n",
              "              mask=[False,  True,  True,  True,  True,  True,  True,  True,\n",
              "                     True,  True,  True, False,  True,  True,  True,  True,\n",
              "                     True,  True, False,  True,  True, False,  True,  True,\n",
              "                     True,  True,  True,  True,  True,  True,  True,  True],\n",
              "        fill_value=1e+20),\n",
              " 'param_classifier:random_forest:min_weight_fraction_leaf': masked_array(data=[0.0, --, --, --, --, --, --, --, --, --, --, 0.0, --,\n",
              "                    --, --, --, --, --, 0.0, --, --, 0.0, --, --, --, --,\n",
              "                    --, --, --, --, --, --],\n",
              "              mask=[False,  True,  True,  True,  True,  True,  True,  True,\n",
              "                     True,  True,  True, False,  True,  True,  True,  True,\n",
              "                     True,  True, False,  True,  True, False,  True,  True,\n",
              "                     True,  True,  True,  True,  True,  True,  True,  True],\n",
              "        fill_value=1e+20),\n",
              " 'param_classifier:random_forest:n_estimators': masked_array(data=[100.0, --, --, --, --, --, --, --, --, --, --, 100.0,\n",
              "                    --, --, --, --, --, --, 100.0, --, --, 100.0, --, --,\n",
              "                    --, --, --, --, --, --, --, --],\n",
              "              mask=[False,  True,  True,  True,  True,  True,  True,  True,\n",
              "                     True,  True,  True, False,  True,  True,  True,  True,\n",
              "                     True,  True, False,  True,  True, False,  True,  True,\n",
              "                     True,  True,  True,  True,  True,  True,  True,  True],\n",
              "        fill_value=1e+20),\n",
              " 'param_classifier:sgd:alpha': masked_array(data=[--, --, --, --, --, --, --, --, --, --, --, --, --, --,\n",
              "                    --, --, --, --, --, --, --, --, --, --, --, --, --, --,\n",
              "                    --, --, --, --],\n",
              "              mask=[ True,  True,  True,  True,  True,  True,  True,  True,\n",
              "                     True,  True,  True,  True,  True,  True,  True,  True,\n",
              "                     True,  True,  True,  True,  True,  True,  True,  True,\n",
              "                     True,  True,  True,  True,  True,  True,  True,  True],\n",
              "        fill_value=1e+20,\n",
              "             dtype=float64),\n",
              " 'param_classifier:sgd:average': masked_array(data=[--, --, --, --, --, --, --, --, --, --, --, --, --, --,\n",
              "                    --, --, --, --, --, --, --, --, --, --, --, --, --, --,\n",
              "                    --, --, --, --],\n",
              "              mask=[ True,  True,  True,  True,  True,  True,  True,  True,\n",
              "                     True,  True,  True,  True,  True,  True,  True,  True,\n",
              "                     True,  True,  True,  True,  True,  True,  True,  True,\n",
              "                     True,  True,  True,  True,  True,  True,  True,  True],\n",
              "        fill_value=1e+20,\n",
              "             dtype=float64),\n",
              " 'param_classifier:sgd:epsilon': masked_array(data=[--, --, --, --, --, --, --, --, --, --, --, --, --, --,\n",
              "                    --, --, --, --, --, --, --, --, --, --, --, --, --, --,\n",
              "                    --, --, --, --],\n",
              "              mask=[ True,  True,  True,  True,  True,  True,  True,  True,\n",
              "                     True,  True,  True,  True,  True,  True,  True,  True,\n",
              "                     True,  True,  True,  True,  True,  True,  True,  True,\n",
              "                     True,  True,  True,  True,  True,  True,  True,  True],\n",
              "        fill_value=1e+20,\n",
              "             dtype=float64),\n",
              " 'param_classifier:sgd:eta0': masked_array(data=[--, --, --, --, --, --, --, --, --, --, --, --, --, --,\n",
              "                    --, --, --, --, --, --, --, --, --, --, --, --, --, --,\n",
              "                    --, --, --, --],\n",
              "              mask=[ True,  True,  True,  True,  True,  True,  True,  True,\n",
              "                     True,  True,  True,  True,  True,  True,  True,  True,\n",
              "                     True,  True,  True,  True,  True,  True,  True,  True,\n",
              "                     True,  True,  True,  True,  True,  True,  True,  True],\n",
              "        fill_value=1e+20,\n",
              "             dtype=float64),\n",
              " 'param_classifier:sgd:fit_intercept': masked_array(data=[--, --, --, --, --, --, --, --, --, --, --, --, --, --,\n",
              "                    --, --, --, --, --, --, --, --, --, --, --, --, --, --,\n",
              "                    --, --, --, --],\n",
              "              mask=[ True,  True,  True,  True,  True,  True,  True,  True,\n",
              "                     True,  True,  True,  True,  True,  True,  True,  True,\n",
              "                     True,  True,  True,  True,  True,  True,  True,  True,\n",
              "                     True,  True,  True,  True,  True,  True,  True,  True],\n",
              "        fill_value=1e+20,\n",
              "             dtype=float64),\n",
              " 'param_classifier:sgd:l1_ratio': masked_array(data=[--, --, --, --, --, --, --, --, --, --, --, --, --, --,\n",
              "                    --, --, --, --, --, --, --, --, --, --, --, --, --, --,\n",
              "                    --, --, --, --],\n",
              "              mask=[ True,  True,  True,  True,  True,  True,  True,  True,\n",
              "                     True,  True,  True,  True,  True,  True,  True,  True,\n",
              "                     True,  True,  True,  True,  True,  True,  True,  True,\n",
              "                     True,  True,  True,  True,  True,  True,  True,  True],\n",
              "        fill_value=1e+20,\n",
              "             dtype=float64),\n",
              " 'param_classifier:sgd:learning_rate': masked_array(data=[--, --, --, --, --, --, --, --, --, --, --, --, --, --,\n",
              "                    --, --, --, --, --, --, --, --, --, --, --, --, --, --,\n",
              "                    --, --, --, --],\n",
              "              mask=[ True,  True,  True,  True,  True,  True,  True,  True,\n",
              "                     True,  True,  True,  True,  True,  True,  True,  True,\n",
              "                     True,  True,  True,  True,  True,  True,  True,  True,\n",
              "                     True,  True,  True,  True,  True,  True,  True,  True],\n",
              "        fill_value=1e+20,\n",
              "             dtype=float64),\n",
              " 'param_classifier:sgd:loss': masked_array(data=[--, --, --, --, --, --, --, --, --, --, --, --, --, --,\n",
              "                    --, --, --, --, --, --, --, --, --, --, --, --, --, --,\n",
              "                    --, --, --, --],\n",
              "              mask=[ True,  True,  True,  True,  True,  True,  True,  True,\n",
              "                     True,  True,  True,  True,  True,  True,  True,  True,\n",
              "                     True,  True,  True,  True,  True,  True,  True,  True,\n",
              "                     True,  True,  True,  True,  True,  True,  True,  True],\n",
              "        fill_value=1e+20,\n",
              "             dtype=float64),\n",
              " 'param_classifier:sgd:penalty': masked_array(data=[--, --, --, --, --, --, --, --, --, --, --, --, --, --,\n",
              "                    --, --, --, --, --, --, --, --, --, --, --, --, --, --,\n",
              "                    --, --, --, --],\n",
              "              mask=[ True,  True,  True,  True,  True,  True,  True,  True,\n",
              "                     True,  True,  True,  True,  True,  True,  True,  True,\n",
              "                     True,  True,  True,  True,  True,  True,  True,  True,\n",
              "                     True,  True,  True,  True,  True,  True,  True,  True],\n",
              "        fill_value=1e+20,\n",
              "             dtype=float64),\n",
              " 'param_classifier:sgd:power_t': masked_array(data=[--, --, --, --, --, --, --, --, --, --, --, --, --, --,\n",
              "                    --, --, --, --, --, --, --, --, --, --, --, --, --, --,\n",
              "                    --, --, --, --],\n",
              "              mask=[ True,  True,  True,  True,  True,  True,  True,  True,\n",
              "                     True,  True,  True,  True,  True,  True,  True,  True,\n",
              "                     True,  True,  True,  True,  True,  True,  True,  True,\n",
              "                     True,  True,  True,  True,  True,  True,  True,  True],\n",
              "        fill_value=1e+20,\n",
              "             dtype=float64),\n",
              " 'param_classifier:sgd:tol': masked_array(data=[--, --, --, --, --, --, --, --, --, --, --, --, --, --,\n",
              "                    --, --, --, --, --, --, --, --, --, --, --, --, --, --,\n",
              "                    --, --, --, --],\n",
              "              mask=[ True,  True,  True,  True,  True,  True,  True,  True,\n",
              "                     True,  True,  True,  True,  True,  True,  True,  True,\n",
              "                     True,  True,  True,  True,  True,  True,  True,  True,\n",
              "                     True,  True,  True,  True,  True,  True,  True,  True],\n",
              "        fill_value=1e+20,\n",
              "             dtype=float64),\n",
              " 'param_classifier:xgradient_boosting:base_score': masked_array(data=[--, --, --, --, --, --, --, --, --, --, --, --, --, --,\n",
              "                    --, --, --, --, --, --, --, --, --, --, --, --, --, --,\n",
              "                    --, --, --, --],\n",
              "              mask=[ True,  True,  True,  True,  True,  True,  True,  True,\n",
              "                     True,  True,  True,  True,  True,  True,  True,  True,\n",
              "                     True,  True,  True,  True,  True,  True,  True,  True,\n",
              "                     True,  True,  True,  True,  True,  True,  True,  True],\n",
              "        fill_value=1e+20,\n",
              "             dtype=float64),\n",
              " 'param_classifier:xgradient_boosting:booster': masked_array(data=[--, --, --, --, --, --, --, --, --, --, --, --, --, --,\n",
              "                    --, --, --, --, --, --, --, --, --, --, --, --, --, --,\n",
              "                    --, --, --, --],\n",
              "              mask=[ True,  True,  True,  True,  True,  True,  True,  True,\n",
              "                     True,  True,  True,  True,  True,  True,  True,  True,\n",
              "                     True,  True,  True,  True,  True,  True,  True,  True,\n",
              "                     True,  True,  True,  True,  True,  True,  True,  True],\n",
              "        fill_value=1e+20,\n",
              "             dtype=float64),\n",
              " 'param_classifier:xgradient_boosting:colsample_bylevel': masked_array(data=[--, --, --, --, --, --, --, --, --, --, --, --, --, --,\n",
              "                    --, --, --, --, --, --, --, --, --, --, --, --, --, --,\n",
              "                    --, --, --, --],\n",
              "              mask=[ True,  True,  True,  True,  True,  True,  True,  True,\n",
              "                     True,  True,  True,  True,  True,  True,  True,  True,\n",
              "                     True,  True,  True,  True,  True,  True,  True,  True,\n",
              "                     True,  True,  True,  True,  True,  True,  True,  True],\n",
              "        fill_value=1e+20,\n",
              "             dtype=float64),\n",
              " 'param_classifier:xgradient_boosting:colsample_bytree': masked_array(data=[--, --, --, --, --, --, --, --, --, --, --, --, --, --,\n",
              "                    --, --, --, --, --, --, --, --, --, --, --, --, --, --,\n",
              "                    --, --, --, --],\n",
              "              mask=[ True,  True,  True,  True,  True,  True,  True,  True,\n",
              "                     True,  True,  True,  True,  True,  True,  True,  True,\n",
              "                     True,  True,  True,  True,  True,  True,  True,  True,\n",
              "                     True,  True,  True,  True,  True,  True,  True,  True],\n",
              "        fill_value=1e+20,\n",
              "             dtype=float64),\n",
              " 'param_classifier:xgradient_boosting:gamma': masked_array(data=[--, --, --, --, --, --, --, --, --, --, --, --, --, --,\n",
              "                    --, --, --, --, --, --, --, --, --, --, --, --, --, --,\n",
              "                    --, --, --, --],\n",
              "              mask=[ True,  True,  True,  True,  True,  True,  True,  True,\n",
              "                     True,  True,  True,  True,  True,  True,  True,  True,\n",
              "                     True,  True,  True,  True,  True,  True,  True,  True,\n",
              "                     True,  True,  True,  True,  True,  True,  True,  True],\n",
              "        fill_value=1e+20,\n",
              "             dtype=float64),\n",
              " 'param_classifier:xgradient_boosting:learning_rate': masked_array(data=[--, --, --, --, --, --, --, --, --, --, --, --, --, --,\n",
              "                    --, --, --, --, --, --, --, --, --, --, --, --, --, --,\n",
              "                    --, --, --, --],\n",
              "              mask=[ True,  True,  True,  True,  True,  True,  True,  True,\n",
              "                     True,  True,  True,  True,  True,  True,  True,  True,\n",
              "                     True,  True,  True,  True,  True,  True,  True,  True,\n",
              "                     True,  True,  True,  True,  True,  True,  True,  True],\n",
              "        fill_value=1e+20,\n",
              "             dtype=float64),\n",
              " 'param_classifier:xgradient_boosting:max_delta_step': masked_array(data=[--, --, --, --, --, --, --, --, --, --, --, --, --, --,\n",
              "                    --, --, --, --, --, --, --, --, --, --, --, --, --, --,\n",
              "                    --, --, --, --],\n",
              "              mask=[ True,  True,  True,  True,  True,  True,  True,  True,\n",
              "                     True,  True,  True,  True,  True,  True,  True,  True,\n",
              "                     True,  True,  True,  True,  True,  True,  True,  True,\n",
              "                     True,  True,  True,  True,  True,  True,  True,  True],\n",
              "        fill_value=1e+20,\n",
              "             dtype=float64),\n",
              " 'param_classifier:xgradient_boosting:max_depth': masked_array(data=[--, --, --, --, --, --, --, --, --, --, --, --, --, --,\n",
              "                    --, --, --, --, --, --, --, --, --, --, --, --, --, --,\n",
              "                    --, --, --, --],\n",
              "              mask=[ True,  True,  True,  True,  True,  True,  True,  True,\n",
              "                     True,  True,  True,  True,  True,  True,  True,  True,\n",
              "                     True,  True,  True,  True,  True,  True,  True,  True,\n",
              "                     True,  True,  True,  True,  True,  True,  True,  True],\n",
              "        fill_value=1e+20,\n",
              "             dtype=float64),\n",
              " 'param_classifier:xgradient_boosting:min_child_weight': masked_array(data=[--, --, --, --, --, --, --, --, --, --, --, --, --, --,\n",
              "                    --, --, --, --, --, --, --, --, --, --, --, --, --, --,\n",
              "                    --, --, --, --],\n",
              "              mask=[ True,  True,  True,  True,  True,  True,  True,  True,\n",
              "                     True,  True,  True,  True,  True,  True,  True,  True,\n",
              "                     True,  True,  True,  True,  True,  True,  True,  True,\n",
              "                     True,  True,  True,  True,  True,  True,  True,  True],\n",
              "        fill_value=1e+20,\n",
              "             dtype=float64),\n",
              " 'param_classifier:xgradient_boosting:n_estimators': masked_array(data=[--, --, --, --, --, --, --, --, --, --, --, --, --, --,\n",
              "                    --, --, --, --, --, --, --, --, --, --, --, --, --, --,\n",
              "                    --, --, --, --],\n",
              "              mask=[ True,  True,  True,  True,  True,  True,  True,  True,\n",
              "                     True,  True,  True,  True,  True,  True,  True,  True,\n",
              "                     True,  True,  True,  True,  True,  True,  True,  True,\n",
              "                     True,  True,  True,  True,  True,  True,  True,  True],\n",
              "        fill_value=1e+20,\n",
              "             dtype=float64),\n",
              " 'param_classifier:xgradient_boosting:normalize_type': masked_array(data=[--, --, --, --, --, --, --, --, --, --, --, --, --, --,\n",
              "                    --, --, --, --, --, --, --, --, --, --, --, --, --, --,\n",
              "                    --, --, --, --],\n",
              "              mask=[ True,  True,  True,  True,  True,  True,  True,  True,\n",
              "                     True,  True,  True,  True,  True,  True,  True,  True,\n",
              "                     True,  True,  True,  True,  True,  True,  True,  True,\n",
              "                     True,  True,  True,  True,  True,  True,  True,  True],\n",
              "        fill_value=1e+20,\n",
              "             dtype=float64),\n",
              " 'param_classifier:xgradient_boosting:rate_drop': masked_array(data=[--, --, --, --, --, --, --, --, --, --, --, --, --, --,\n",
              "                    --, --, --, --, --, --, --, --, --, --, --, --, --, --,\n",
              "                    --, --, --, --],\n",
              "              mask=[ True,  True,  True,  True,  True,  True,  True,  True,\n",
              "                     True,  True,  True,  True,  True,  True,  True,  True,\n",
              "                     True,  True,  True,  True,  True,  True,  True,  True,\n",
              "                     True,  True,  True,  True,  True,  True,  True,  True],\n",
              "        fill_value=1e+20,\n",
              "             dtype=float64),\n",
              " 'param_classifier:xgradient_boosting:reg_alpha': masked_array(data=[--, --, --, --, --, --, --, --, --, --, --, --, --, --,\n",
              "                    --, --, --, --, --, --, --, --, --, --, --, --, --, --,\n",
              "                    --, --, --, --],\n",
              "              mask=[ True,  True,  True,  True,  True,  True,  True,  True,\n",
              "                     True,  True,  True,  True,  True,  True,  True,  True,\n",
              "                     True,  True,  True,  True,  True,  True,  True,  True,\n",
              "                     True,  True,  True,  True,  True,  True,  True,  True],\n",
              "        fill_value=1e+20,\n",
              "             dtype=float64),\n",
              " 'param_classifier:xgradient_boosting:reg_lambda': masked_array(data=[--, --, --, --, --, --, --, --, --, --, --, --, --, --,\n",
              "                    --, --, --, --, --, --, --, --, --, --, --, --, --, --,\n",
              "                    --, --, --, --],\n",
              "              mask=[ True,  True,  True,  True,  True,  True,  True,  True,\n",
              "                     True,  True,  True,  True,  True,  True,  True,  True,\n",
              "                     True,  True,  True,  True,  True,  True,  True,  True,\n",
              "                     True,  True,  True,  True,  True,  True,  True,  True],\n",
              "        fill_value=1e+20,\n",
              "             dtype=float64),\n",
              " 'param_classifier:xgradient_boosting:sample_type': masked_array(data=[--, --, --, --, --, --, --, --, --, --, --, --, --, --,\n",
              "                    --, --, --, --, --, --, --, --, --, --, --, --, --, --,\n",
              "                    --, --, --, --],\n",
              "              mask=[ True,  True,  True,  True,  True,  True,  True,  True,\n",
              "                     True,  True,  True,  True,  True,  True,  True,  True,\n",
              "                     True,  True,  True,  True,  True,  True,  True,  True,\n",
              "                     True,  True,  True,  True,  True,  True,  True,  True],\n",
              "        fill_value=1e+20,\n",
              "             dtype=float64),\n",
              " 'param_classifier:xgradient_boosting:scale_pos_weight': masked_array(data=[--, --, --, --, --, --, --, --, --, --, --, --, --, --,\n",
              "                    --, --, --, --, --, --, --, --, --, --, --, --, --, --,\n",
              "                    --, --, --, --],\n",
              "              mask=[ True,  True,  True,  True,  True,  True,  True,  True,\n",
              "                     True,  True,  True,  True,  True,  True,  True,  True,\n",
              "                     True,  True,  True,  True,  True,  True,  True,  True,\n",
              "                     True,  True,  True,  True,  True,  True,  True,  True],\n",
              "        fill_value=1e+20,\n",
              "             dtype=float64),\n",
              " 'param_classifier:xgradient_boosting:subsample': masked_array(data=[--, --, --, --, --, --, --, --, --, --, --, --, --, --,\n",
              "                    --, --, --, --, --, --, --, --, --, --, --, --, --, --,\n",
              "                    --, --, --, --],\n",
              "              mask=[ True,  True,  True,  True,  True,  True,  True,  True,\n",
              "                     True,  True,  True,  True,  True,  True,  True,  True,\n",
              "                     True,  True,  True,  True,  True,  True,  True,  True,\n",
              "                     True,  True,  True,  True,  True,  True,  True,  True],\n",
              "        fill_value=1e+20,\n",
              "             dtype=float64),\n",
              " 'param_imputation:strategy': masked_array(data=['mean', 'median', 'most_frequent', 'most_frequent',\n",
              "                    'median', 'mean', 'mean', 'mean', 'most_frequent',\n",
              "                    'median', 'most_frequent', 'mean', 'most_frequent',\n",
              "                    'mean', 'most_frequent', 'mean', 'most_frequent',\n",
              "                    'median', 'mean', 'most_frequent', 'most_frequent',\n",
              "                    'most_frequent', 'mean', 'mean', 'median', 'mean',\n",
              "                    'mean', 'mean', 'median', 'mean', 'median', 'median'],\n",
              "              mask=[False, False, False, False, False, False, False, False,\n",
              "                    False, False, False, False, False, False, False, False,\n",
              "                    False, False, False, False, False, False, False, False,\n",
              "                    False, False, False, False, False, False, False, False],\n",
              "        fill_value='N/A',\n",
              "             dtype='<U13'),\n",
              " 'param_preprocessor:__choice__': masked_array(data=['no_preprocessing', 'polynomial', 'no_preprocessing',\n",
              "                    'liblinear_svc_preprocessor', 'polynomial',\n",
              "                    'polynomial', 'select_percentile_classification',\n",
              "                    'polynomial', 'polynomial', 'fast_ica',\n",
              "                    'extra_trees_preproc_for_classification',\n",
              "                    'no_preprocessing', 'fast_ica', 'fast_ica',\n",
              "                    'random_trees_embedding', 'no_preprocessing',\n",
              "                    'fast_ica', 'feature_agglomeration',\n",
              "                    'no_preprocessing', 'polynomial', 'fast_ica',\n",
              "                    'select_percentile_classification',\n",
              "                    'feature_agglomeration', 'liblinear_svc_preprocessor',\n",
              "                    'fast_ica', 'feature_agglomeration',\n",
              "                    'no_preprocessing',\n",
              "                    'extra_trees_preproc_for_classification', 'pca', 'pca',\n",
              "                    'pca', 'select_rates'],\n",
              "              mask=[False, False, False, False, False, False, False, False,\n",
              "                    False, False, False, False, False, False, False, False,\n",
              "                    False, False, False, False, False, False, False, False,\n",
              "                    False, False, False, False, False, False, False, False],\n",
              "        fill_value='N/A',\n",
              "             dtype='<U38'),\n",
              " 'param_preprocessor:extra_trees_preproc_for_classification:bootstrap': masked_array(data=[--, --, --, --, --, --, --, --, --, --, 'True', --, --,\n",
              "                    --, --, --, --, --, --, --, --, --, --, --, --, --, --,\n",
              "                    'False', --, --, --, --],\n",
              "              mask=[ True,  True,  True,  True,  True,  True,  True,  True,\n",
              "                     True,  True, False,  True,  True,  True,  True,  True,\n",
              "                     True,  True,  True,  True,  True,  True,  True,  True,\n",
              "                     True,  True,  True, False,  True,  True,  True,  True],\n",
              "        fill_value='N/A',\n",
              "             dtype='<U32'),\n",
              " 'param_preprocessor:extra_trees_preproc_for_classification:criterion': masked_array(data=[--, --, --, --, --, --, --, --, --, --, 'entropy', --,\n",
              "                    --, --, --, --, --, --, --, --, --, --, --, --, --, --,\n",
              "                    --, 'entropy', --, --, --, --],\n",
              "              mask=[ True,  True,  True,  True,  True,  True,  True,  True,\n",
              "                     True,  True, False,  True,  True,  True,  True,  True,\n",
              "                     True,  True,  True,  True,  True,  True,  True,  True,\n",
              "                     True,  True,  True, False,  True,  True,  True,  True],\n",
              "        fill_value='N/A',\n",
              "             dtype='<U32'),\n",
              " 'param_preprocessor:extra_trees_preproc_for_classification:max_depth': masked_array(data=[--, --, --, --, --, --, --, --, --, --, 'None', --, --,\n",
              "                    --, --, --, --, --, --, --, --, --, --, --, --, --, --,\n",
              "                    'None', --, --, --, --],\n",
              "              mask=[ True,  True,  True,  True,  True,  True,  True,  True,\n",
              "                     True,  True, False,  True,  True,  True,  True,  True,\n",
              "                     True,  True,  True,  True,  True,  True,  True,  True,\n",
              "                     True,  True,  True, False,  True,  True,  True,  True],\n",
              "        fill_value='N/A',\n",
              "             dtype='<U32'),\n",
              " 'param_preprocessor:extra_trees_preproc_for_classification:max_features': masked_array(data=[--, --, --, --, --, --, --, --, --, --,\n",
              "                    0.9082628722828775, --, --, --, --, --, --, --, --, --,\n",
              "                    --, --, --, --, --, --, --, 0.6459545924335193, --, --,\n",
              "                    --, --],\n",
              "              mask=[ True,  True,  True,  True,  True,  True,  True,  True,\n",
              "                     True,  True, False,  True,  True,  True,  True,  True,\n",
              "                     True,  True,  True,  True,  True,  True,  True,  True,\n",
              "                     True,  True,  True, False,  True,  True,  True,  True],\n",
              "        fill_value=1e+20),\n",
              " 'param_preprocessor:extra_trees_preproc_for_classification:max_leaf_nodes': masked_array(data=[--, --, --, --, --, --, --, --, --, --, 'None', --, --,\n",
              "                    --, --, --, --, --, --, --, --, --, --, --, --, --, --,\n",
              "                    'None', --, --, --, --],\n",
              "              mask=[ True,  True,  True,  True,  True,  True,  True,  True,\n",
              "                     True,  True, False,  True,  True,  True,  True,  True,\n",
              "                     True,  True,  True,  True,  True,  True,  True,  True,\n",
              "                     True,  True,  True, False,  True,  True,  True,  True],\n",
              "        fill_value='N/A',\n",
              "             dtype='<U32'),\n",
              " 'param_preprocessor:extra_trees_preproc_for_classification:min_impurity_decrease': masked_array(data=[--, --, --, --, --, --, --, --, --, --, 0.0, --, --,\n",
              "                    --, --, --, --, --, --, --, --, --, --, --, --, --, --,\n",
              "                    0.0, --, --, --, --],\n",
              "              mask=[ True,  True,  True,  True,  True,  True,  True,  True,\n",
              "                     True,  True, False,  True,  True,  True,  True,  True,\n",
              "                     True,  True,  True,  True,  True,  True,  True,  True,\n",
              "                     True,  True,  True, False,  True,  True,  True,  True],\n",
              "        fill_value=1e+20),\n",
              " 'param_preprocessor:extra_trees_preproc_for_classification:min_samples_leaf': masked_array(data=[--, --, --, --, --, --, --, --, --, --, 2.0, --, --,\n",
              "                    --, --, --, --, --, --, --, --, --, --, --, --, --, --,\n",
              "                    7.0, --, --, --, --],\n",
              "              mask=[ True,  True,  True,  True,  True,  True,  True,  True,\n",
              "                     True,  True, False,  True,  True,  True,  True,  True,\n",
              "                     True,  True,  True,  True,  True,  True,  True,  True,\n",
              "                     True,  True,  True, False,  True,  True,  True,  True],\n",
              "        fill_value=1e+20),\n",
              " 'param_preprocessor:extra_trees_preproc_for_classification:min_samples_split': masked_array(data=[--, --, --, --, --, --, --, --, --, --, 18.0, --, --,\n",
              "                    --, --, --, --, --, --, --, --, --, --, --, --, --, --,\n",
              "                    14.0, --, --, --, --],\n",
              "              mask=[ True,  True,  True,  True,  True,  True,  True,  True,\n",
              "                     True,  True, False,  True,  True,  True,  True,  True,\n",
              "                     True,  True,  True,  True,  True,  True,  True,  True,\n",
              "                     True,  True,  True, False,  True,  True,  True,  True],\n",
              "        fill_value=1e+20),\n",
              " 'param_preprocessor:extra_trees_preproc_for_classification:min_weight_fraction_leaf': masked_array(data=[--, --, --, --, --, --, --, --, --, --, 0.0, --, --,\n",
              "                    --, --, --, --, --, --, --, --, --, --, --, --, --, --,\n",
              "                    0.0, --, --, --, --],\n",
              "              mask=[ True,  True,  True,  True,  True,  True,  True,  True,\n",
              "                     True,  True, False,  True,  True,  True,  True,  True,\n",
              "                     True,  True,  True,  True,  True,  True,  True,  True,\n",
              "                     True,  True,  True, False,  True,  True,  True,  True],\n",
              "        fill_value=1e+20),\n",
              " 'param_preprocessor:extra_trees_preproc_for_classification:n_estimators': masked_array(data=[--, --, --, --, --, --, --, --, --, --, 100.0, --, --,\n",
              "                    --, --, --, --, --, --, --, --, --, --, --, --, --, --,\n",
              "                    100.0, --, --, --, --],\n",
              "              mask=[ True,  True,  True,  True,  True,  True,  True,  True,\n",
              "                     True,  True, False,  True,  True,  True,  True,  True,\n",
              "                     True,  True,  True,  True,  True,  True,  True,  True,\n",
              "                     True,  True,  True, False,  True,  True,  True,  True],\n",
              "        fill_value=1e+20),\n",
              " 'param_preprocessor:fast_ica:algorithm': masked_array(data=[--, --, --, --, --, --, --, --, --, 'deflation', --,\n",
              "                    --, 'parallel', 'deflation', --, --, 'deflation', --,\n",
              "                    --, --, 'parallel', --, --, --, 'parallel', --, --, --,\n",
              "                    --, --, --, --],\n",
              "              mask=[ True,  True,  True,  True,  True,  True,  True,  True,\n",
              "                     True, False,  True,  True, False, False,  True,  True,\n",
              "                    False,  True,  True,  True, False,  True,  True,  True,\n",
              "                    False,  True,  True,  True,  True,  True,  True,  True],\n",
              "        fill_value='N/A',\n",
              "             dtype='<U32'),\n",
              " 'param_preprocessor:fast_ica:fun': masked_array(data=[--, --, --, --, --, --, --, --, --, 'exp', --, --,\n",
              "                    'logcosh', 'exp', --, --, 'cube', --, --, --, 'exp',\n",
              "                    --, --, --, 'logcosh', --, --, --, --, --, --, --],\n",
              "              mask=[ True,  True,  True,  True,  True,  True,  True,  True,\n",
              "                     True, False,  True,  True, False, False,  True,  True,\n",
              "                    False,  True,  True,  True, False,  True,  True,  True,\n",
              "                    False,  True,  True,  True,  True,  True,  True,  True],\n",
              "        fill_value='N/A',\n",
              "             dtype='<U32'),\n",
              " 'param_preprocessor:fast_ica:n_components': masked_array(data=[--, --, --, --, --, --, --, --, --, 10.0, --, --,\n",
              "                    2000.0, 1862.0, --, --, --, --, --, --, 100.0, --, --,\n",
              "                    --, --, --, --, --, --, --, --, --],\n",
              "              mask=[ True,  True,  True,  True,  True,  True,  True,  True,\n",
              "                     True, False,  True,  True, False, False,  True,  True,\n",
              "                     True,  True,  True,  True, False,  True,  True,  True,\n",
              "                     True,  True,  True,  True,  True,  True,  True,  True],\n",
              "        fill_value=1e+20),\n",
              " 'param_preprocessor:fast_ica:whiten': masked_array(data=[--, --, --, --, --, --, --, --, --, 'True', --, --,\n",
              "                    'True', 'True', --, --, 'False', --, --, --, 'True',\n",
              "                    --, --, --, 'False', --, --, --, --, --, --, --],\n",
              "              mask=[ True,  True,  True,  True,  True,  True,  True,  True,\n",
              "                     True, False,  True,  True, False, False,  True,  True,\n",
              "                    False,  True,  True,  True, False,  True,  True,  True,\n",
              "                    False,  True,  True,  True,  True,  True,  True,  True],\n",
              "        fill_value='N/A',\n",
              "             dtype='<U32'),\n",
              " 'param_preprocessor:feature_agglomeration:affinity': masked_array(data=[--, --, --, --, --, --, --, --, --, --, --, --, --, --,\n",
              "                    --, --, --, 'euclidean', --, --, --, --, 'euclidean',\n",
              "                    --, --, 'cosine', --, --, --, --, --, --],\n",
              "              mask=[ True,  True,  True,  True,  True,  True,  True,  True,\n",
              "                     True,  True,  True,  True,  True,  True,  True,  True,\n",
              "                     True, False,  True,  True,  True,  True, False,  True,\n",
              "                     True, False,  True,  True,  True,  True,  True,  True],\n",
              "        fill_value='N/A',\n",
              "             dtype='<U32'),\n",
              " 'param_preprocessor:feature_agglomeration:linkage': masked_array(data=[--, --, --, --, --, --, --, --, --, --, --, --, --, --,\n",
              "                    --, --, --, 'complete', --, --, --, --, 'complete', --,\n",
              "                    --, 'average', --, --, --, --, --, --],\n",
              "              mask=[ True,  True,  True,  True,  True,  True,  True,  True,\n",
              "                     True,  True,  True,  True,  True,  True,  True,  True,\n",
              "                     True, False,  True,  True,  True,  True, False,  True,\n",
              "                     True, False,  True,  True,  True,  True,  True,  True],\n",
              "        fill_value='N/A',\n",
              "             dtype='<U32'),\n",
              " 'param_preprocessor:feature_agglomeration:n_clusters': masked_array(data=[--, --, --, --, --, --, --, --, --, --, --, --, --, --,\n",
              "                    --, --, --, 332.0, --, --, --, --, 332.0, --, --,\n",
              "                    140.0, --, --, --, --, --, --],\n",
              "              mask=[ True,  True,  True,  True,  True,  True,  True,  True,\n",
              "                     True,  True,  True,  True,  True,  True,  True,  True,\n",
              "                     True, False,  True,  True,  True,  True, False,  True,\n",
              "                     True, False,  True,  True,  True,  True,  True,  True],\n",
              "        fill_value=1e+20),\n",
              " 'param_preprocessor:feature_agglomeration:pooling_func': masked_array(data=[--, --, --, --, --, --, --, --, --, --, --, --, --, --,\n",
              "                    --, --, --, 'max', --, --, --, --, 'max', --, --,\n",
              "                    'mean', --, --, --, --, --, --],\n",
              "              mask=[ True,  True,  True,  True,  True,  True,  True,  True,\n",
              "                     True,  True,  True,  True,  True,  True,  True,  True,\n",
              "                     True, False,  True,  True,  True,  True, False,  True,\n",
              "                     True, False,  True,  True,  True,  True,  True,  True],\n",
              "        fill_value='N/A',\n",
              "             dtype='<U32'),\n",
              " 'param_preprocessor:kernel_pca:coef0': masked_array(data=[--, --, --, --, --, --, --, --, --, --, --, --, --, --,\n",
              "                    --, --, --, --, --, --, --, --, --, --, --, --, --, --,\n",
              "                    --, --, --, --],\n",
              "              mask=[ True,  True,  True,  True,  True,  True,  True,  True,\n",
              "                     True,  True,  True,  True,  True,  True,  True,  True,\n",
              "                     True,  True,  True,  True,  True,  True,  True,  True,\n",
              "                     True,  True,  True,  True,  True,  True,  True,  True],\n",
              "        fill_value=1e+20,\n",
              "             dtype=float64),\n",
              " 'param_preprocessor:kernel_pca:degree': masked_array(data=[--, --, --, --, --, --, --, --, --, --, --, --, --, --,\n",
              "                    --, --, --, --, --, --, --, --, --, --, --, --, --, --,\n",
              "                    --, --, --, --],\n",
              "              mask=[ True,  True,  True,  True,  True,  True,  True,  True,\n",
              "                     True,  True,  True,  True,  True,  True,  True,  True,\n",
              "                     True,  True,  True,  True,  True,  True,  True,  True,\n",
              "                     True,  True,  True,  True,  True,  True,  True,  True],\n",
              "        fill_value=1e+20,\n",
              "             dtype=float64),\n",
              " 'param_preprocessor:kernel_pca:gamma': masked_array(data=[--, --, --, --, --, --, --, --, --, --, --, --, --, --,\n",
              "                    --, --, --, --, --, --, --, --, --, --, --, --, --, --,\n",
              "                    --, --, --, --],\n",
              "              mask=[ True,  True,  True,  True,  True,  True,  True,  True,\n",
              "                     True,  True,  True,  True,  True,  True,  True,  True,\n",
              "                     True,  True,  True,  True,  True,  True,  True,  True,\n",
              "                     True,  True,  True,  True,  True,  True,  True,  True],\n",
              "        fill_value=1e+20,\n",
              "             dtype=float64),\n",
              " 'param_preprocessor:kernel_pca:kernel': masked_array(data=[--, --, --, --, --, --, --, --, --, --, --, --, --, --,\n",
              "                    --, --, --, --, --, --, --, --, --, --, --, --, --, --,\n",
              "                    --, --, --, --],\n",
              "              mask=[ True,  True,  True,  True,  True,  True,  True,  True,\n",
              "                     True,  True,  True,  True,  True,  True,  True,  True,\n",
              "                     True,  True,  True,  True,  True,  True,  True,  True,\n",
              "                     True,  True,  True,  True,  True,  True,  True,  True],\n",
              "        fill_value=1e+20,\n",
              "             dtype=float64),\n",
              " 'param_preprocessor:kernel_pca:n_components': masked_array(data=[--, --, --, --, --, --, --, --, --, --, --, --, --, --,\n",
              "                    --, --, --, --, --, --, --, --, --, --, --, --, --, --,\n",
              "                    --, --, --, --],\n",
              "              mask=[ True,  True,  True,  True,  True,  True,  True,  True,\n",
              "                     True,  True,  True,  True,  True,  True,  True,  True,\n",
              "                     True,  True,  True,  True,  True,  True,  True,  True,\n",
              "                     True,  True,  True,  True,  True,  True,  True,  True],\n",
              "        fill_value=1e+20,\n",
              "             dtype=float64),\n",
              " 'param_preprocessor:kitchen_sinks:gamma': masked_array(data=[--, --, --, --, --, --, --, --, --, --, --, --, --, --,\n",
              "                    --, --, --, --, --, --, --, --, --, --, --, --, --, --,\n",
              "                    --, --, --, --],\n",
              "              mask=[ True,  True,  True,  True,  True,  True,  True,  True,\n",
              "                     True,  True,  True,  True,  True,  True,  True,  True,\n",
              "                     True,  True,  True,  True,  True,  True,  True,  True,\n",
              "                     True,  True,  True,  True,  True,  True,  True,  True],\n",
              "        fill_value=1e+20,\n",
              "             dtype=float64),\n",
              " 'param_preprocessor:kitchen_sinks:n_components': masked_array(data=[--, --, --, --, --, --, --, --, --, --, --, --, --, --,\n",
              "                    --, --, --, --, --, --, --, --, --, --, --, --, --, --,\n",
              "                    --, --, --, --],\n",
              "              mask=[ True,  True,  True,  True,  True,  True,  True,  True,\n",
              "                     True,  True,  True,  True,  True,  True,  True,  True,\n",
              "                     True,  True,  True,  True,  True,  True,  True,  True,\n",
              "                     True,  True,  True,  True,  True,  True,  True,  True],\n",
              "        fill_value=1e+20,\n",
              "             dtype=float64),\n",
              " 'param_preprocessor:liblinear_svc_preprocessor:C': masked_array(data=[--, --, --, 8074.423891892491, --, --, --, --, --, --,\n",
              "                    --, --, --, --, --, --, --, --, --, --, --, --, --,\n",
              "                    24.136430622242006, --, --, --, --, --, --, --, --],\n",
              "              mask=[ True,  True,  True, False,  True,  True,  True,  True,\n",
              "                     True,  True,  True,  True,  True,  True,  True,  True,\n",
              "                     True,  True,  True,  True,  True,  True,  True, False,\n",
              "                     True,  True,  True,  True,  True,  True,  True,  True],\n",
              "        fill_value=1e+20),\n",
              " 'param_preprocessor:liblinear_svc_preprocessor:dual': masked_array(data=[--, --, --, 'False', --, --, --, --, --, --, --, --,\n",
              "                    --, --, --, --, --, --, --, --, --, --, --, 'False',\n",
              "                    --, --, --, --, --, --, --, --],\n",
              "              mask=[ True,  True,  True, False,  True,  True,  True,  True,\n",
              "                     True,  True,  True,  True,  True,  True,  True,  True,\n",
              "                     True,  True,  True,  True,  True,  True,  True, False,\n",
              "                     True,  True,  True,  True,  True,  True,  True,  True],\n",
              "        fill_value='N/A',\n",
              "             dtype='<U32'),\n",
              " 'param_preprocessor:liblinear_svc_preprocessor:fit_intercept': masked_array(data=[--, --, --, 'True', --, --, --, --, --, --, --, --, --,\n",
              "                    --, --, --, --, --, --, --, --, --, --, 'True', --, --,\n",
              "                    --, --, --, --, --, --],\n",
              "              mask=[ True,  True,  True, False,  True,  True,  True,  True,\n",
              "                     True,  True,  True,  True,  True,  True,  True,  True,\n",
              "                     True,  True,  True,  True,  True,  True,  True, False,\n",
              "                     True,  True,  True,  True,  True,  True,  True,  True],\n",
              "        fill_value='N/A',\n",
              "             dtype='<U32'),\n",
              " 'param_preprocessor:liblinear_svc_preprocessor:intercept_scaling': masked_array(data=[--, --, --, 1.0, --, --, --, --, --, --, --, --, --,\n",
              "                    --, --, --, --, --, --, --, --, --, --, 1.0, --, --,\n",
              "                    --, --, --, --, --, --],\n",
              "              mask=[ True,  True,  True, False,  True,  True,  True,  True,\n",
              "                     True,  True,  True,  True,  True,  True,  True,  True,\n",
              "                     True,  True,  True,  True,  True,  True,  True, False,\n",
              "                     True,  True,  True,  True,  True,  True,  True,  True],\n",
              "        fill_value=1e+20),\n",
              " 'param_preprocessor:liblinear_svc_preprocessor:loss': masked_array(data=[--, --, --, 'squared_hinge', --, --, --, --, --, --,\n",
              "                    --, --, --, --, --, --, --, --, --, --, --, --, --,\n",
              "                    'squared_hinge', --, --, --, --, --, --, --, --],\n",
              "              mask=[ True,  True,  True, False,  True,  True,  True,  True,\n",
              "                     True,  True,  True,  True,  True,  True,  True,  True,\n",
              "                     True,  True,  True,  True,  True,  True,  True, False,\n",
              "                     True,  True,  True,  True,  True,  True,  True,  True],\n",
              "        fill_value='N/A',\n",
              "             dtype='<U32'),\n",
              " 'param_preprocessor:liblinear_svc_preprocessor:multi_class': masked_array(data=[--, --, --, 'ovr', --, --, --, --, --, --, --, --, --,\n",
              "                    --, --, --, --, --, --, --, --, --, --, 'ovr', --, --,\n",
              "                    --, --, --, --, --, --],\n",
              "              mask=[ True,  True,  True, False,  True,  True,  True,  True,\n",
              "                     True,  True,  True,  True,  True,  True,  True,  True,\n",
              "                     True,  True,  True,  True,  True,  True,  True, False,\n",
              "                     True,  True,  True,  True,  True,  True,  True,  True],\n",
              "        fill_value='N/A',\n",
              "             dtype='<U32'),\n",
              " 'param_preprocessor:liblinear_svc_preprocessor:penalty': masked_array(data=[--, --, --, 'l1', --, --, --, --, --, --, --, --, --,\n",
              "                    --, --, --, --, --, --, --, --, --, --, 'l1', --, --,\n",
              "                    --, --, --, --, --, --],\n",
              "              mask=[ True,  True,  True, False,  True,  True,  True,  True,\n",
              "                     True,  True,  True,  True,  True,  True,  True,  True,\n",
              "                     True,  True,  True,  True,  True,  True,  True, False,\n",
              "                     True,  True,  True,  True,  True,  True,  True,  True],\n",
              "        fill_value='N/A',\n",
              "             dtype='<U32'),\n",
              " 'param_preprocessor:liblinear_svc_preprocessor:tol': masked_array(data=[--, --, --, 0.003592235404478327, --, --, --, --, --,\n",
              "                    --, --, --, --, --, --, --, --, --, --, --, --, --, --,\n",
              "                    0.003077255210337526, --, --, --, --, --, --, --, --],\n",
              "              mask=[ True,  True,  True, False,  True,  True,  True,  True,\n",
              "                     True,  True,  True,  True,  True,  True,  True,  True,\n",
              "                     True,  True,  True,  True,  True,  True,  True, False,\n",
              "                     True,  True,  True,  True,  True,  True,  True,  True],\n",
              "        fill_value=1e+20),\n",
              " 'param_preprocessor:nystroem_sampler:coef0': masked_array(data=[--, --, --, --, --, --, --, --, --, --, --, --, --, --,\n",
              "                    --, --, --, --, --, --, --, --, --, --, --, --, --, --,\n",
              "                    --, --, --, --],\n",
              "              mask=[ True,  True,  True,  True,  True,  True,  True,  True,\n",
              "                     True,  True,  True,  True,  True,  True,  True,  True,\n",
              "                     True,  True,  True,  True,  True,  True,  True,  True,\n",
              "                     True,  True,  True,  True,  True,  True,  True,  True],\n",
              "        fill_value=1e+20,\n",
              "             dtype=float64),\n",
              " 'param_preprocessor:nystroem_sampler:degree': masked_array(data=[--, --, --, --, --, --, --, --, --, --, --, --, --, --,\n",
              "                    --, --, --, --, --, --, --, --, --, --, --, --, --, --,\n",
              "                    --, --, --, --],\n",
              "              mask=[ True,  True,  True,  True,  True,  True,  True,  True,\n",
              "                     True,  True,  True,  True,  True,  True,  True,  True,\n",
              "                     True,  True,  True,  True,  True,  True,  True,  True,\n",
              "                     True,  True,  True,  True,  True,  True,  True,  True],\n",
              "        fill_value=1e+20,\n",
              "             dtype=float64),\n",
              " 'param_preprocessor:nystroem_sampler:gamma': masked_array(data=[--, --, --, --, --, --, --, --, --, --, --, --, --, --,\n",
              "                    --, --, --, --, --, --, --, --, --, --, --, --, --, --,\n",
              "                    --, --, --, --],\n",
              "              mask=[ True,  True,  True,  True,  True,  True,  True,  True,\n",
              "                     True,  True,  True,  True,  True,  True,  True,  True,\n",
              "                     True,  True,  True,  True,  True,  True,  True,  True,\n",
              "                     True,  True,  True,  True,  True,  True,  True,  True],\n",
              "        fill_value=1e+20,\n",
              "             dtype=float64),\n",
              " 'param_preprocessor:nystroem_sampler:kernel': masked_array(data=[--, --, --, --, --, --, --, --, --, --, --, --, --, --,\n",
              "                    --, --, --, --, --, --, --, --, --, --, --, --, --, --,\n",
              "                    --, --, --, --],\n",
              "              mask=[ True,  True,  True,  True,  True,  True,  True,  True,\n",
              "                     True,  True,  True,  True,  True,  True,  True,  True,\n",
              "                     True,  True,  True,  True,  True,  True,  True,  True,\n",
              "                     True,  True,  True,  True,  True,  True,  True,  True],\n",
              "        fill_value=1e+20,\n",
              "             dtype=float64),\n",
              " 'param_preprocessor:nystroem_sampler:n_components': masked_array(data=[--, --, --, --, --, --, --, --, --, --, --, --, --, --,\n",
              "                    --, --, --, --, --, --, --, --, --, --, --, --, --, --,\n",
              "                    --, --, --, --],\n",
              "              mask=[ True,  True,  True,  True,  True,  True,  True,  True,\n",
              "                     True,  True,  True,  True,  True,  True,  True,  True,\n",
              "                     True,  True,  True,  True,  True,  True,  True,  True,\n",
              "                     True,  True,  True,  True,  True,  True,  True,  True],\n",
              "        fill_value=1e+20,\n",
              "             dtype=float64),\n",
              " 'param_preprocessor:pca:keep_variance': masked_array(data=[--, --, --, --, --, --, --, --, --, --, --, --, --, --,\n",
              "                    --, --, --, --, --, --, --, --, --, --, --, --, --, --,\n",
              "                    0.9999, 0.840817378270975, 0.9999, --],\n",
              "              mask=[ True,  True,  True,  True,  True,  True,  True,  True,\n",
              "                     True,  True,  True,  True,  True,  True,  True,  True,\n",
              "                     True,  True,  True,  True,  True,  True,  True,  True,\n",
              "                     True,  True,  True,  True, False, False, False,  True],\n",
              "        fill_value=1e+20),\n",
              " 'param_preprocessor:pca:whiten': masked_array(data=[--, --, --, --, --, --, --, --, --, --, --, --, --, --,\n",
              "                    --, --, --, --, --, --, --, --, --, --, --, --, --, --,\n",
              "                    'False', 'True', 'False', --],\n",
              "              mask=[ True,  True,  True,  True,  True,  True,  True,  True,\n",
              "                     True,  True,  True,  True,  True,  True,  True,  True,\n",
              "                     True,  True,  True,  True,  True,  True,  True,  True,\n",
              "                     True,  True,  True,  True, False, False, False,  True],\n",
              "        fill_value='N/A',\n",
              "             dtype='<U32'),\n",
              " 'param_preprocessor:polynomial:degree': masked_array(data=[--, 3.0, --, --, 2.0, 3.0, --, 2.0, 2.0, --, --, --,\n",
              "                    --, --, --, --, --, --, --, 3.0, --, --, --, --, --,\n",
              "                    --, --, --, --, --, --, --],\n",
              "              mask=[ True, False,  True,  True, False, False,  True, False,\n",
              "                    False,  True,  True,  True,  True,  True,  True,  True,\n",
              "                     True,  True,  True, False,  True,  True,  True,  True,\n",
              "                     True,  True,  True,  True,  True,  True,  True,  True],\n",
              "        fill_value=1e+20),\n",
              " 'param_preprocessor:polynomial:include_bias': masked_array(data=[--, 'True', --, --, 'False', 'False', --, 'True',\n",
              "                    'False', --, --, --, --, --, --, --, --, --, --,\n",
              "                    'True', --, --, --, --, --, --, --, --, --, --, --, --],\n",
              "              mask=[ True, False,  True,  True, False, False,  True, False,\n",
              "                    False,  True,  True,  True,  True,  True,  True,  True,\n",
              "                     True,  True,  True, False,  True,  True,  True,  True,\n",
              "                     True,  True,  True,  True,  True,  True,  True,  True],\n",
              "        fill_value='N/A',\n",
              "             dtype='<U32'),\n",
              " 'param_preprocessor:polynomial:interaction_only': masked_array(data=[--, 'False', --, --, 'False', 'True', --, 'False',\n",
              "                    'True', --, --, --, --, --, --, --, --, --, --,\n",
              "                    'False', --, --, --, --, --, --, --, --, --, --, --,\n",
              "                    --],\n",
              "              mask=[ True, False,  True,  True, False, False,  True, False,\n",
              "                    False,  True,  True,  True,  True,  True,  True,  True,\n",
              "                     True,  True,  True, False,  True,  True,  True,  True,\n",
              "                     True,  True,  True,  True,  True,  True,  True,  True],\n",
              "        fill_value='N/A',\n",
              "             dtype='<U32'),\n",
              " 'param_preprocessor:random_trees_embedding:bootstrap': masked_array(data=[--, --, --, --, --, --, --, --, --, --, --, --, --, --,\n",
              "                    'False', --, --, --, --, --, --, --, --, --, --, --,\n",
              "                    --, --, --, --, --, --],\n",
              "              mask=[ True,  True,  True,  True,  True,  True,  True,  True,\n",
              "                     True,  True,  True,  True,  True,  True, False,  True,\n",
              "                     True,  True,  True,  True,  True,  True,  True,  True,\n",
              "                     True,  True,  True,  True,  True,  True,  True,  True],\n",
              "        fill_value='N/A',\n",
              "             dtype='<U32'),\n",
              " 'param_preprocessor:random_trees_embedding:max_depth': masked_array(data=[--, --, --, --, --, --, --, --, --, --, --, --, --, --,\n",
              "                    5.0, --, --, --, --, --, --, --, --, --, --, --, --,\n",
              "                    --, --, --, --, --],\n",
              "              mask=[ True,  True,  True,  True,  True,  True,  True,  True,\n",
              "                     True,  True,  True,  True,  True,  True, False,  True,\n",
              "                     True,  True,  True,  True,  True,  True,  True,  True,\n",
              "                     True,  True,  True,  True,  True,  True,  True,  True],\n",
              "        fill_value=1e+20),\n",
              " 'param_preprocessor:random_trees_embedding:max_leaf_nodes': masked_array(data=[--, --, --, --, --, --, --, --, --, --, --, --, --, --,\n",
              "                    'None', --, --, --, --, --, --, --, --, --, --, --, --,\n",
              "                    --, --, --, --, --],\n",
              "              mask=[ True,  True,  True,  True,  True,  True,  True,  True,\n",
              "                     True,  True,  True,  True,  True,  True, False,  True,\n",
              "                     True,  True,  True,  True,  True,  True,  True,  True,\n",
              "                     True,  True,  True,  True,  True,  True,  True,  True],\n",
              "        fill_value='N/A',\n",
              "             dtype='<U32'),\n",
              " 'param_preprocessor:random_trees_embedding:min_samples_leaf': masked_array(data=[--, --, --, --, --, --, --, --, --, --, --, --, --, --,\n",
              "                    11.0, --, --, --, --, --, --, --, --, --, --, --, --,\n",
              "                    --, --, --, --, --],\n",
              "              mask=[ True,  True,  True,  True,  True,  True,  True,  True,\n",
              "                     True,  True,  True,  True,  True,  True, False,  True,\n",
              "                     True,  True,  True,  True,  True,  True,  True,  True,\n",
              "                     True,  True,  True,  True,  True,  True,  True,  True],\n",
              "        fill_value=1e+20),\n",
              " 'param_preprocessor:random_trees_embedding:min_samples_split': masked_array(data=[--, --, --, --, --, --, --, --, --, --, --, --, --, --,\n",
              "                    11.0, --, --, --, --, --, --, --, --, --, --, --, --,\n",
              "                    --, --, --, --, --],\n",
              "              mask=[ True,  True,  True,  True,  True,  True,  True,  True,\n",
              "                     True,  True,  True,  True,  True,  True, False,  True,\n",
              "                     True,  True,  True,  True,  True,  True,  True,  True,\n",
              "                     True,  True,  True,  True,  True,  True,  True,  True],\n",
              "        fill_value=1e+20),\n",
              " 'param_preprocessor:random_trees_embedding:min_weight_fraction_leaf': masked_array(data=[--, --, --, --, --, --, --, --, --, --, --, --, --, --,\n",
              "                    1.0, --, --, --, --, --, --, --, --, --, --, --, --,\n",
              "                    --, --, --, --, --],\n",
              "              mask=[ True,  True,  True,  True,  True,  True,  True,  True,\n",
              "                     True,  True,  True,  True,  True,  True, False,  True,\n",
              "                     True,  True,  True,  True,  True,  True,  True,  True,\n",
              "                     True,  True,  True,  True,  True,  True,  True,  True],\n",
              "        fill_value=1e+20),\n",
              " 'param_preprocessor:random_trees_embedding:n_estimators': masked_array(data=[--, --, --, --, --, --, --, --, --, --, --, --, --, --,\n",
              "                    12.0, --, --, --, --, --, --, --, --, --, --, --, --,\n",
              "                    --, --, --, --, --],\n",
              "              mask=[ True,  True,  True,  True,  True,  True,  True,  True,\n",
              "                     True,  True,  True,  True,  True,  True, False,  True,\n",
              "                     True,  True,  True,  True,  True,  True,  True,  True,\n",
              "                     True,  True,  True,  True,  True,  True,  True,  True],\n",
              "        fill_value=1e+20),\n",
              " 'param_preprocessor:select_percentile_classification:percentile': masked_array(data=[--, --, --, --, --, --, 58.88123233170863, --, --, --,\n",
              "                    --, --, --, --, --, --, --, --, --, --, --, 50.0, --,\n",
              "                    --, --, --, --, --, --, --, --, --],\n",
              "              mask=[ True,  True,  True,  True,  True,  True, False,  True,\n",
              "                     True,  True,  True,  True,  True,  True,  True,  True,\n",
              "                     True,  True,  True,  True,  True, False,  True,  True,\n",
              "                     True,  True,  True,  True,  True,  True,  True,  True],\n",
              "        fill_value=1e+20),\n",
              " 'param_preprocessor:select_percentile_classification:score_func': masked_array(data=[--, --, --, --, --, --, 'mutual_info', --, --, --, --,\n",
              "                    --, --, --, --, --, --, --, --, --, --, 'chi2', --, --,\n",
              "                    --, --, --, --, --, --, --, --],\n",
              "              mask=[ True,  True,  True,  True,  True,  True, False,  True,\n",
              "                     True,  True,  True,  True,  True,  True,  True,  True,\n",
              "                     True,  True,  True,  True,  True, False,  True,  True,\n",
              "                     True,  True,  True,  True,  True,  True,  True,  True],\n",
              "        fill_value='N/A',\n",
              "             dtype='<U32'),\n",
              " 'param_preprocessor:select_rates:alpha': masked_array(data=[--, --, --, --, --, --, --, --, --, --, --, --, --, --,\n",
              "                    --, --, --, --, --, --, --, --, --, --, --, --, --, --,\n",
              "                    --, --, --, 0.24514751291114034],\n",
              "              mask=[ True,  True,  True,  True,  True,  True,  True,  True,\n",
              "                     True,  True,  True,  True,  True,  True,  True,  True,\n",
              "                     True,  True,  True,  True,  True,  True,  True,  True,\n",
              "                     True,  True,  True,  True,  True,  True,  True, False],\n",
              "        fill_value=1e+20),\n",
              " 'param_preprocessor:select_rates:mode': masked_array(data=[--, --, --, --, --, --, --, --, --, --, --, --, --, --,\n",
              "                    --, --, --, --, --, --, --, --, --, --, --, --, --, --,\n",
              "                    --, --, --, 'fdr'],\n",
              "              mask=[ True,  True,  True,  True,  True,  True,  True,  True,\n",
              "                     True,  True,  True,  True,  True,  True,  True,  True,\n",
              "                     True,  True,  True,  True,  True,  True,  True,  True,\n",
              "                     True,  True,  True,  True,  True,  True,  True, False],\n",
              "        fill_value='N/A',\n",
              "             dtype='<U32'),\n",
              " 'param_preprocessor:select_rates:score_func': masked_array(data=[--, --, --, --, --, --, --, --, --, --, --, --, --, --,\n",
              "                    --, --, --, --, --, --, --, --, --, --, --, --, --, --,\n",
              "                    --, --, --, 'chi2'],\n",
              "              mask=[ True,  True,  True,  True,  True,  True,  True,  True,\n",
              "                     True,  True,  True,  True,  True,  True,  True,  True,\n",
              "                     True,  True,  True,  True,  True,  True,  True,  True,\n",
              "                     True,  True,  True,  True,  True,  True,  True, False],\n",
              "        fill_value='N/A',\n",
              "             dtype='<U32'),\n",
              " 'param_rescaling:__choice__': masked_array(data=['standardize', 'minmax', 'standardize', 'standardize',\n",
              "                    'none', 'standardize', 'robust_scaler', 'none',\n",
              "                    'minmax', 'none', 'none', 'minmax', 'robust_scaler',\n",
              "                    'robust_scaler', 'robust_scaler', 'robust_scaler',\n",
              "                    'robust_scaler', 'quantile_transformer', 'normalize',\n",
              "                    'robust_scaler', 'robust_scaler', 'standardize',\n",
              "                    'robust_scaler', 'robust_scaler', 'standardize',\n",
              "                    'none', 'minmax', 'minmax', 'robust_scaler', 'none',\n",
              "                    'standardize', 'quantile_transformer'],\n",
              "              mask=[False, False, False, False, False, False, False, False,\n",
              "                    False, False, False, False, False, False, False, False,\n",
              "                    False, False, False, False, False, False, False, False,\n",
              "                    False, False, False, False, False, False, False, False],\n",
              "        fill_value='N/A',\n",
              "             dtype='<U20'),\n",
              " 'param_rescaling:quantile_transformer:n_quantiles': masked_array(data=[--, --, --, --, --, --, --, --, --, --, --, --, --, --,\n",
              "                    --, --, --, 1000.0, --, --, --, --, --, --, --, --, --,\n",
              "                    --, --, --, --, 397.0],\n",
              "              mask=[ True,  True,  True,  True,  True,  True,  True,  True,\n",
              "                     True,  True,  True,  True,  True,  True,  True,  True,\n",
              "                     True, False,  True,  True,  True,  True,  True,  True,\n",
              "                     True,  True,  True,  True,  True,  True,  True, False],\n",
              "        fill_value=1e+20),\n",
              " 'param_rescaling:quantile_transformer:output_distribution': masked_array(data=[--, --, --, --, --, --, --, --, --, --, --, --, --, --,\n",
              "                    --, --, --, 'uniform', --, --, --, --, --, --, --, --,\n",
              "                    --, --, --, --, --, 'uniform'],\n",
              "              mask=[ True,  True,  True,  True,  True,  True,  True,  True,\n",
              "                     True,  True,  True,  True,  True,  True,  True,  True,\n",
              "                     True, False,  True,  True,  True,  True,  True,  True,\n",
              "                     True,  True,  True,  True,  True,  True,  True, False],\n",
              "        fill_value='N/A',\n",
              "             dtype='<U32'),\n",
              " 'param_rescaling:robust_scaler:q_max': masked_array(data=[--, --, --, --, --, --, 0.75, --, --, --, --, --,\n",
              "                    0.8430415644014919, 0.7851234479882973,\n",
              "                    0.8928631650245873, 0.8245132980938538,\n",
              "                    0.8255464552647293, --, --, 0.75, 0.7065776353150109,\n",
              "                    --, 0.75, 0.8963215641064294, --, --, --, --, 0.75, --,\n",
              "                    --, --],\n",
              "              mask=[ True,  True,  True,  True,  True,  True, False,  True,\n",
              "                     True,  True,  True,  True, False, False, False, False,\n",
              "                    False,  True,  True, False, False,  True, False, False,\n",
              "                     True,  True,  True,  True, False,  True,  True,  True],\n",
              "        fill_value=1e+20),\n",
              " 'param_rescaling:robust_scaler:q_min': masked_array(data=[--, --, --, --, --, --, 0.25, --, --, --, --, --,\n",
              "                    0.2863750565331575, 0.2237528085136715,\n",
              "                    0.1581877760687084, 0.08947420373097192,\n",
              "                    0.19162485555463182, --, --, 0.25, 0.23782974987118102,\n",
              "                    --, 0.25, 0.160294089836629, --, --, --, --, 0.25, --,\n",
              "                    --, --],\n",
              "              mask=[ True,  True,  True,  True,  True,  True, False,  True,\n",
              "                     True,  True,  True,  True, False, False, False, False,\n",
              "                    False,  True,  True, False, False,  True, False, False,\n",
              "                     True,  True,  True,  True, False,  True,  True,  True],\n",
              "        fill_value=1e+20),\n",
              " 'params': [{'balancing:strategy': 'none',\n",
              "   'categorical_encoding:__choice__': 'one_hot_encoding',\n",
              "   'categorical_encoding:one_hot_encoding:minimum_fraction': 0.01,\n",
              "   'categorical_encoding:one_hot_encoding:use_minimum_fraction': 'True',\n",
              "   'classifier:__choice__': 'random_forest',\n",
              "   'classifier:random_forest:bootstrap': 'True',\n",
              "   'classifier:random_forest:criterion': 'gini',\n",
              "   'classifier:random_forest:max_depth': 'None',\n",
              "   'classifier:random_forest:max_features': 0.5,\n",
              "   'classifier:random_forest:max_leaf_nodes': 'None',\n",
              "   'classifier:random_forest:min_impurity_decrease': 0.0,\n",
              "   'classifier:random_forest:min_samples_leaf': 1,\n",
              "   'classifier:random_forest:min_samples_split': 2,\n",
              "   'classifier:random_forest:min_weight_fraction_leaf': 0.0,\n",
              "   'classifier:random_forest:n_estimators': 100,\n",
              "   'imputation:strategy': 'mean',\n",
              "   'preprocessor:__choice__': 'no_preprocessing',\n",
              "   'rescaling:__choice__': 'standardize'},\n",
              "  {'balancing:strategy': 'weighting',\n",
              "   'categorical_encoding:__choice__': 'no_encoding',\n",
              "   'classifier:__choice__': 'extra_trees',\n",
              "   'classifier:extra_trees:bootstrap': 'False',\n",
              "   'classifier:extra_trees:criterion': 'gini',\n",
              "   'classifier:extra_trees:max_depth': 'None',\n",
              "   'classifier:extra_trees:max_features': 0.5670424455696162,\n",
              "   'classifier:extra_trees:max_leaf_nodes': 'None',\n",
              "   'classifier:extra_trees:min_impurity_decrease': 0.0,\n",
              "   'classifier:extra_trees:min_samples_leaf': 8,\n",
              "   'classifier:extra_trees:min_samples_split': 16,\n",
              "   'classifier:extra_trees:min_weight_fraction_leaf': 0.0,\n",
              "   'classifier:extra_trees:n_estimators': 100,\n",
              "   'imputation:strategy': 'median',\n",
              "   'preprocessor:__choice__': 'polynomial',\n",
              "   'preprocessor:polynomial:degree': 3,\n",
              "   'preprocessor:polynomial:include_bias': 'True',\n",
              "   'preprocessor:polynomial:interaction_only': 'False',\n",
              "   'rescaling:__choice__': 'minmax'},\n",
              "  {'balancing:strategy': 'none',\n",
              "   'categorical_encoding:__choice__': 'one_hot_encoding',\n",
              "   'categorical_encoding:one_hot_encoding:use_minimum_fraction': 'False',\n",
              "   'classifier:__choice__': 'libsvm_svc',\n",
              "   'classifier:libsvm_svc:C': 6.342897164595882,\n",
              "   'classifier:libsvm_svc:gamma': 0.2229870623330047,\n",
              "   'classifier:libsvm_svc:kernel': 'rbf',\n",
              "   'classifier:libsvm_svc:max_iter': -1,\n",
              "   'classifier:libsvm_svc:shrinking': 'False',\n",
              "   'classifier:libsvm_svc:tol': 2.006345264381097e-05,\n",
              "   'imputation:strategy': 'most_frequent',\n",
              "   'preprocessor:__choice__': 'no_preprocessing',\n",
              "   'rescaling:__choice__': 'standardize'},\n",
              "  {'balancing:strategy': 'weighting',\n",
              "   'categorical_encoding:__choice__': 'no_encoding',\n",
              "   'classifier:__choice__': 'k_nearest_neighbors',\n",
              "   'classifier:k_nearest_neighbors:n_neighbors': 59,\n",
              "   'classifier:k_nearest_neighbors:p': 1,\n",
              "   'classifier:k_nearest_neighbors:weights': 'distance',\n",
              "   'imputation:strategy': 'most_frequent',\n",
              "   'preprocessor:__choice__': 'liblinear_svc_preprocessor',\n",
              "   'preprocessor:liblinear_svc_preprocessor:C': 8074.423891892491,\n",
              "   'preprocessor:liblinear_svc_preprocessor:dual': 'False',\n",
              "   'preprocessor:liblinear_svc_preprocessor:fit_intercept': 'True',\n",
              "   'preprocessor:liblinear_svc_preprocessor:intercept_scaling': 1,\n",
              "   'preprocessor:liblinear_svc_preprocessor:loss': 'squared_hinge',\n",
              "   'preprocessor:liblinear_svc_preprocessor:multi_class': 'ovr',\n",
              "   'preprocessor:liblinear_svc_preprocessor:penalty': 'l1',\n",
              "   'preprocessor:liblinear_svc_preprocessor:tol': 0.003592235404478327,\n",
              "   'rescaling:__choice__': 'standardize'},\n",
              "  {'balancing:strategy': 'weighting',\n",
              "   'categorical_encoding:__choice__': 'one_hot_encoding',\n",
              "   'categorical_encoding:one_hot_encoding:use_minimum_fraction': 'False',\n",
              "   'classifier:__choice__': 'gradient_boosting',\n",
              "   'classifier:gradient_boosting:criterion': 'mse',\n",
              "   'classifier:gradient_boosting:learning_rate': 0.1958974686405233,\n",
              "   'classifier:gradient_boosting:loss': 'deviance',\n",
              "   'classifier:gradient_boosting:max_depth': 5,\n",
              "   'classifier:gradient_boosting:max_features': 0.33885235607979314,\n",
              "   'classifier:gradient_boosting:max_leaf_nodes': 'None',\n",
              "   'classifier:gradient_boosting:min_impurity_decrease': 0.0,\n",
              "   'classifier:gradient_boosting:min_samples_leaf': 6,\n",
              "   'classifier:gradient_boosting:min_samples_split': 4,\n",
              "   'classifier:gradient_boosting:min_weight_fraction_leaf': 0.0,\n",
              "   'classifier:gradient_boosting:n_estimators': 125,\n",
              "   'classifier:gradient_boosting:subsample': 0.9448890820738562,\n",
              "   'imputation:strategy': 'median',\n",
              "   'preprocessor:__choice__': 'polynomial',\n",
              "   'preprocessor:polynomial:degree': 2,\n",
              "   'preprocessor:polynomial:include_bias': 'False',\n",
              "   'preprocessor:polynomial:interaction_only': 'False',\n",
              "   'rescaling:__choice__': 'none'},\n",
              "  {'balancing:strategy': 'none',\n",
              "   'categorical_encoding:__choice__': 'one_hot_encoding',\n",
              "   'categorical_encoding:one_hot_encoding:use_minimum_fraction': 'False',\n",
              "   'classifier:__choice__': 'extra_trees',\n",
              "   'classifier:extra_trees:bootstrap': 'False',\n",
              "   'classifier:extra_trees:criterion': 'gini',\n",
              "   'classifier:extra_trees:max_depth': 'None',\n",
              "   'classifier:extra_trees:max_features': 0.5,\n",
              "   'classifier:extra_trees:max_leaf_nodes': 'None',\n",
              "   'classifier:extra_trees:min_impurity_decrease': 0.0,\n",
              "   'classifier:extra_trees:min_samples_leaf': 1,\n",
              "   'classifier:extra_trees:min_samples_split': 2,\n",
              "   'classifier:extra_trees:min_weight_fraction_leaf': 0.0,\n",
              "   'classifier:extra_trees:n_estimators': 100,\n",
              "   'imputation:strategy': 'mean',\n",
              "   'preprocessor:__choice__': 'polynomial',\n",
              "   'preprocessor:polynomial:degree': 3,\n",
              "   'preprocessor:polynomial:include_bias': 'False',\n",
              "   'preprocessor:polynomial:interaction_only': 'True',\n",
              "   'rescaling:__choice__': 'standardize'},\n",
              "  {'balancing:strategy': 'none',\n",
              "   'categorical_encoding:__choice__': 'no_encoding',\n",
              "   'classifier:__choice__': 'gradient_boosting',\n",
              "   'classifier:gradient_boosting:criterion': 'mse',\n",
              "   'classifier:gradient_boosting:learning_rate': 0.02102683283349326,\n",
              "   'classifier:gradient_boosting:loss': 'deviance',\n",
              "   'classifier:gradient_boosting:max_depth': 10,\n",
              "   'classifier:gradient_boosting:max_features': 0.2797288369369436,\n",
              "   'classifier:gradient_boosting:max_leaf_nodes': 'None',\n",
              "   'classifier:gradient_boosting:min_impurity_decrease': 0.0,\n",
              "   'classifier:gradient_boosting:min_samples_leaf': 14,\n",
              "   'classifier:gradient_boosting:min_samples_split': 9,\n",
              "   'classifier:gradient_boosting:min_weight_fraction_leaf': 0.0,\n",
              "   'classifier:gradient_boosting:n_estimators': 480,\n",
              "   'classifier:gradient_boosting:subsample': 0.5778972273820631,\n",
              "   'imputation:strategy': 'mean',\n",
              "   'preprocessor:__choice__': 'select_percentile_classification',\n",
              "   'preprocessor:select_percentile_classification:percentile': 58.88123233170863,\n",
              "   'preprocessor:select_percentile_classification:score_func': 'mutual_info',\n",
              "   'rescaling:__choice__': 'robust_scaler',\n",
              "   'rescaling:robust_scaler:q_max': 0.75,\n",
              "   'rescaling:robust_scaler:q_min': 0.25},\n",
              "  {'balancing:strategy': 'weighting',\n",
              "   'categorical_encoding:__choice__': 'one_hot_encoding',\n",
              "   'categorical_encoding:one_hot_encoding:use_minimum_fraction': 'False',\n",
              "   'classifier:__choice__': 'gradient_boosting',\n",
              "   'classifier:gradient_boosting:criterion': 'friedman_mse',\n",
              "   'classifier:gradient_boosting:learning_rate': 0.07463196642416368,\n",
              "   'classifier:gradient_boosting:loss': 'deviance',\n",
              "   'classifier:gradient_boosting:max_depth': 7,\n",
              "   'classifier:gradient_boosting:max_features': 0.8603242247379981,\n",
              "   'classifier:gradient_boosting:max_leaf_nodes': 'None',\n",
              "   'classifier:gradient_boosting:min_impurity_decrease': 0.0,\n",
              "   'classifier:gradient_boosting:min_samples_leaf': 2,\n",
              "   'classifier:gradient_boosting:min_samples_split': 6,\n",
              "   'classifier:gradient_boosting:min_weight_fraction_leaf': 0.0,\n",
              "   'classifier:gradient_boosting:n_estimators': 500,\n",
              "   'classifier:gradient_boosting:subsample': 0.8447665577491962,\n",
              "   'imputation:strategy': 'mean',\n",
              "   'preprocessor:__choice__': 'polynomial',\n",
              "   'preprocessor:polynomial:degree': 2,\n",
              "   'preprocessor:polynomial:include_bias': 'True',\n",
              "   'preprocessor:polynomial:interaction_only': 'False',\n",
              "   'rescaling:__choice__': 'none'},\n",
              "  {'balancing:strategy': 'weighting',\n",
              "   'categorical_encoding:__choice__': 'one_hot_encoding',\n",
              "   'categorical_encoding:one_hot_encoding:minimum_fraction': 0.3837398524575939,\n",
              "   'categorical_encoding:one_hot_encoding:use_minimum_fraction': 'True',\n",
              "   'classifier:__choice__': 'gradient_boosting',\n",
              "   'classifier:gradient_boosting:criterion': 'mse',\n",
              "   'classifier:gradient_boosting:learning_rate': 0.018356703878357986,\n",
              "   'classifier:gradient_boosting:loss': 'deviance',\n",
              "   'classifier:gradient_boosting:max_depth': 3,\n",
              "   'classifier:gradient_boosting:max_features': 0.9690352514774068,\n",
              "   'classifier:gradient_boosting:max_leaf_nodes': 'None',\n",
              "   'classifier:gradient_boosting:min_impurity_decrease': 0.0,\n",
              "   'classifier:gradient_boosting:min_samples_leaf': 12,\n",
              "   'classifier:gradient_boosting:min_samples_split': 3,\n",
              "   'classifier:gradient_boosting:min_weight_fraction_leaf': 0.0,\n",
              "   'classifier:gradient_boosting:n_estimators': 234,\n",
              "   'classifier:gradient_boosting:subsample': 0.3870344708308441,\n",
              "   'imputation:strategy': 'most_frequent',\n",
              "   'preprocessor:__choice__': 'polynomial',\n",
              "   'preprocessor:polynomial:degree': 2,\n",
              "   'preprocessor:polynomial:include_bias': 'False',\n",
              "   'preprocessor:polynomial:interaction_only': 'True',\n",
              "   'rescaling:__choice__': 'minmax'},\n",
              "  {'balancing:strategy': 'weighting',\n",
              "   'categorical_encoding:__choice__': 'one_hot_encoding',\n",
              "   'categorical_encoding:one_hot_encoding:use_minimum_fraction': 'False',\n",
              "   'classifier:__choice__': 'extra_trees',\n",
              "   'classifier:extra_trees:bootstrap': 'False',\n",
              "   'classifier:extra_trees:criterion': 'entropy',\n",
              "   'classifier:extra_trees:max_depth': 'None',\n",
              "   'classifier:extra_trees:max_features': 0.5765793990908161,\n",
              "   'classifier:extra_trees:max_leaf_nodes': 'None',\n",
              "   'classifier:extra_trees:min_impurity_decrease': 0.0,\n",
              "   'classifier:extra_trees:min_samples_leaf': 11,\n",
              "   'classifier:extra_trees:min_samples_split': 20,\n",
              "   'classifier:extra_trees:min_weight_fraction_leaf': 0.0,\n",
              "   'classifier:extra_trees:n_estimators': 100,\n",
              "   'imputation:strategy': 'median',\n",
              "   'preprocessor:__choice__': 'fast_ica',\n",
              "   'preprocessor:fast_ica:algorithm': 'deflation',\n",
              "   'preprocessor:fast_ica:fun': 'exp',\n",
              "   'preprocessor:fast_ica:n_components': 10,\n",
              "   'preprocessor:fast_ica:whiten': 'True',\n",
              "   'rescaling:__choice__': 'none'},\n",
              "  {'balancing:strategy': 'weighting',\n",
              "   'categorical_encoding:__choice__': 'no_encoding',\n",
              "   'classifier:__choice__': 'extra_trees',\n",
              "   'classifier:extra_trees:bootstrap': 'False',\n",
              "   'classifier:extra_trees:criterion': 'entropy',\n",
              "   'classifier:extra_trees:max_depth': 'None',\n",
              "   'classifier:extra_trees:max_features': 0.9541039630394388,\n",
              "   'classifier:extra_trees:max_leaf_nodes': 'None',\n",
              "   'classifier:extra_trees:min_impurity_decrease': 0.0,\n",
              "   'classifier:extra_trees:min_samples_leaf': 16,\n",
              "   'classifier:extra_trees:min_samples_split': 14,\n",
              "   'classifier:extra_trees:min_weight_fraction_leaf': 0.0,\n",
              "   'classifier:extra_trees:n_estimators': 100,\n",
              "   'imputation:strategy': 'most_frequent',\n",
              "   'preprocessor:__choice__': 'extra_trees_preproc_for_classification',\n",
              "   'preprocessor:extra_trees_preproc_for_classification:bootstrap': 'True',\n",
              "   'preprocessor:extra_trees_preproc_for_classification:criterion': 'entropy',\n",
              "   'preprocessor:extra_trees_preproc_for_classification:max_depth': 'None',\n",
              "   'preprocessor:extra_trees_preproc_for_classification:max_features': 0.9082628722828775,\n",
              "   'preprocessor:extra_trees_preproc_for_classification:max_leaf_nodes': 'None',\n",
              "   'preprocessor:extra_trees_preproc_for_classification:min_impurity_decrease': 0.0,\n",
              "   'preprocessor:extra_trees_preproc_for_classification:min_samples_leaf': 2,\n",
              "   'preprocessor:extra_trees_preproc_for_classification:min_samples_split': 18,\n",
              "   'preprocessor:extra_trees_preproc_for_classification:min_weight_fraction_leaf': 0.0,\n",
              "   'preprocessor:extra_trees_preproc_for_classification:n_estimators': 100,\n",
              "   'rescaling:__choice__': 'none'},\n",
              "  {'balancing:strategy': 'none',\n",
              "   'categorical_encoding:__choice__': 'one_hot_encoding',\n",
              "   'categorical_encoding:one_hot_encoding:use_minimum_fraction': 'False',\n",
              "   'classifier:__choice__': 'random_forest',\n",
              "   'classifier:random_forest:bootstrap': 'True',\n",
              "   'classifier:random_forest:criterion': 'gini',\n",
              "   'classifier:random_forest:max_depth': 'None',\n",
              "   'classifier:random_forest:max_features': 0.9260795160807372,\n",
              "   'classifier:random_forest:max_leaf_nodes': 'None',\n",
              "   'classifier:random_forest:min_impurity_decrease': 0.0,\n",
              "   'classifier:random_forest:min_samples_leaf': 17,\n",
              "   'classifier:random_forest:min_samples_split': 7,\n",
              "   'classifier:random_forest:min_weight_fraction_leaf': 0.0,\n",
              "   'classifier:random_forest:n_estimators': 100,\n",
              "   'imputation:strategy': 'mean',\n",
              "   'preprocessor:__choice__': 'no_preprocessing',\n",
              "   'rescaling:__choice__': 'minmax'},\n",
              "  {'balancing:strategy': 'weighting',\n",
              "   'categorical_encoding:__choice__': 'one_hot_encoding',\n",
              "   'categorical_encoding:one_hot_encoding:use_minimum_fraction': 'False',\n",
              "   'classifier:__choice__': 'extra_trees',\n",
              "   'classifier:extra_trees:bootstrap': 'False',\n",
              "   'classifier:extra_trees:criterion': 'gini',\n",
              "   'classifier:extra_trees:max_depth': 'None',\n",
              "   'classifier:extra_trees:max_features': 0.609975998293528,\n",
              "   'classifier:extra_trees:max_leaf_nodes': 'None',\n",
              "   'classifier:extra_trees:min_impurity_decrease': 0.0,\n",
              "   'classifier:extra_trees:min_samples_leaf': 1,\n",
              "   'classifier:extra_trees:min_samples_split': 2,\n",
              "   'classifier:extra_trees:min_weight_fraction_leaf': 0.0,\n",
              "   'classifier:extra_trees:n_estimators': 100,\n",
              "   'imputation:strategy': 'most_frequent',\n",
              "   'preprocessor:__choice__': 'fast_ica',\n",
              "   'preprocessor:fast_ica:algorithm': 'parallel',\n",
              "   'preprocessor:fast_ica:fun': 'logcosh',\n",
              "   'preprocessor:fast_ica:n_components': 2000,\n",
              "   'preprocessor:fast_ica:whiten': 'True',\n",
              "   'rescaling:__choice__': 'robust_scaler',\n",
              "   'rescaling:robust_scaler:q_max': 0.8430415644014919,\n",
              "   'rescaling:robust_scaler:q_min': 0.2863750565331575},\n",
              "  {'balancing:strategy': 'weighting',\n",
              "   'categorical_encoding:__choice__': 'one_hot_encoding',\n",
              "   'categorical_encoding:one_hot_encoding:minimum_fraction': 0.0003173723611800348,\n",
              "   'categorical_encoding:one_hot_encoding:use_minimum_fraction': 'True',\n",
              "   'classifier:__choice__': 'lda',\n",
              "   'classifier:lda:n_components': 244,\n",
              "   'classifier:lda:shrinkage': 'None',\n",
              "   'classifier:lda:tol': 2.3065111488706024e-05,\n",
              "   'imputation:strategy': 'mean',\n",
              "   'preprocessor:__choice__': 'fast_ica',\n",
              "   'preprocessor:fast_ica:algorithm': 'deflation',\n",
              "   'preprocessor:fast_ica:fun': 'exp',\n",
              "   'preprocessor:fast_ica:n_components': 1862,\n",
              "   'preprocessor:fast_ica:whiten': 'True',\n",
              "   'rescaling:__choice__': 'robust_scaler',\n",
              "   'rescaling:robust_scaler:q_max': 0.7851234479882973,\n",
              "   'rescaling:robust_scaler:q_min': 0.2237528085136715},\n",
              "  {'balancing:strategy': 'none',\n",
              "   'categorical_encoding:__choice__': 'one_hot_encoding',\n",
              "   'categorical_encoding:one_hot_encoding:use_minimum_fraction': 'False',\n",
              "   'classifier:__choice__': 'extra_trees',\n",
              "   'classifier:extra_trees:bootstrap': 'False',\n",
              "   'classifier:extra_trees:criterion': 'entropy',\n",
              "   'classifier:extra_trees:max_depth': 'None',\n",
              "   'classifier:extra_trees:max_features': 0.39536192447534535,\n",
              "   'classifier:extra_trees:max_leaf_nodes': 'None',\n",
              "   'classifier:extra_trees:min_impurity_decrease': 0.0,\n",
              "   'classifier:extra_trees:min_samples_leaf': 19,\n",
              "   'classifier:extra_trees:min_samples_split': 3,\n",
              "   'classifier:extra_trees:min_weight_fraction_leaf': 0.0,\n",
              "   'classifier:extra_trees:n_estimators': 100,\n",
              "   'imputation:strategy': 'most_frequent',\n",
              "   'preprocessor:__choice__': 'random_trees_embedding',\n",
              "   'preprocessor:random_trees_embedding:bootstrap': 'False',\n",
              "   'preprocessor:random_trees_embedding:max_depth': 5,\n",
              "   'preprocessor:random_trees_embedding:max_leaf_nodes': 'None',\n",
              "   'preprocessor:random_trees_embedding:min_samples_leaf': 11,\n",
              "   'preprocessor:random_trees_embedding:min_samples_split': 11,\n",
              "   'preprocessor:random_trees_embedding:min_weight_fraction_leaf': 1.0,\n",
              "   'preprocessor:random_trees_embedding:n_estimators': 12,\n",
              "   'rescaling:__choice__': 'robust_scaler',\n",
              "   'rescaling:robust_scaler:q_max': 0.8928631650245873,\n",
              "   'rescaling:robust_scaler:q_min': 0.1581877760687084},\n",
              "  {'balancing:strategy': 'weighting',\n",
              "   'categorical_encoding:__choice__': 'one_hot_encoding',\n",
              "   'categorical_encoding:one_hot_encoding:minimum_fraction': 0.00034835629696198427,\n",
              "   'categorical_encoding:one_hot_encoding:use_minimum_fraction': 'True',\n",
              "   'classifier:__choice__': 'gaussian_nb',\n",
              "   'imputation:strategy': 'mean',\n",
              "   'preprocessor:__choice__': 'no_preprocessing',\n",
              "   'rescaling:__choice__': 'robust_scaler',\n",
              "   'rescaling:robust_scaler:q_max': 0.8245132980938538,\n",
              "   'rescaling:robust_scaler:q_min': 0.08947420373097192},\n",
              "  {'balancing:strategy': 'weighting',\n",
              "   'categorical_encoding:__choice__': 'no_encoding',\n",
              "   'classifier:__choice__': 'extra_trees',\n",
              "   'classifier:extra_trees:bootstrap': 'False',\n",
              "   'classifier:extra_trees:criterion': 'entropy',\n",
              "   'classifier:extra_trees:max_depth': 'None',\n",
              "   'classifier:extra_trees:max_features': 0.9455638720565651,\n",
              "   'classifier:extra_trees:max_leaf_nodes': 'None',\n",
              "   'classifier:extra_trees:min_impurity_decrease': 0.0,\n",
              "   'classifier:extra_trees:min_samples_leaf': 1,\n",
              "   'classifier:extra_trees:min_samples_split': 2,\n",
              "   'classifier:extra_trees:min_weight_fraction_leaf': 0.0,\n",
              "   'classifier:extra_trees:n_estimators': 100,\n",
              "   'imputation:strategy': 'most_frequent',\n",
              "   'preprocessor:__choice__': 'fast_ica',\n",
              "   'preprocessor:fast_ica:algorithm': 'deflation',\n",
              "   'preprocessor:fast_ica:fun': 'cube',\n",
              "   'preprocessor:fast_ica:whiten': 'False',\n",
              "   'rescaling:__choice__': 'robust_scaler',\n",
              "   'rescaling:robust_scaler:q_max': 0.8255464552647293,\n",
              "   'rescaling:robust_scaler:q_min': 0.19162485555463182},\n",
              "  {'balancing:strategy': 'none',\n",
              "   'categorical_encoding:__choice__': 'one_hot_encoding',\n",
              "   'categorical_encoding:one_hot_encoding:use_minimum_fraction': 'False',\n",
              "   'classifier:__choice__': 'gradient_boosting',\n",
              "   'classifier:gradient_boosting:criterion': 'friedman_mse',\n",
              "   'classifier:gradient_boosting:learning_rate': 0.24729845478857812,\n",
              "   'classifier:gradient_boosting:loss': 'deviance',\n",
              "   'classifier:gradient_boosting:max_depth': 3,\n",
              "   'classifier:gradient_boosting:max_features': 0.6564306719064884,\n",
              "   'classifier:gradient_boosting:max_leaf_nodes': 'None',\n",
              "   'classifier:gradient_boosting:min_impurity_decrease': 0.0,\n",
              "   'classifier:gradient_boosting:min_samples_leaf': 15,\n",
              "   'classifier:gradient_boosting:min_samples_split': 14,\n",
              "   'classifier:gradient_boosting:min_weight_fraction_leaf': 0.0,\n",
              "   'classifier:gradient_boosting:n_estimators': 220,\n",
              "   'classifier:gradient_boosting:subsample': 0.8082564085714649,\n",
              "   'imputation:strategy': 'median',\n",
              "   'preprocessor:__choice__': 'feature_agglomeration',\n",
              "   'preprocessor:feature_agglomeration:affinity': 'euclidean',\n",
              "   'preprocessor:feature_agglomeration:linkage': 'complete',\n",
              "   'preprocessor:feature_agglomeration:n_clusters': 332,\n",
              "   'preprocessor:feature_agglomeration:pooling_func': 'max',\n",
              "   'rescaling:__choice__': 'quantile_transformer',\n",
              "   'rescaling:quantile_transformer:n_quantiles': 1000,\n",
              "   'rescaling:quantile_transformer:output_distribution': 'uniform'},\n",
              "  {'balancing:strategy': 'none',\n",
              "   'categorical_encoding:__choice__': 'one_hot_encoding',\n",
              "   'categorical_encoding:one_hot_encoding:minimum_fraction': 0.00012586572428922356,\n",
              "   'categorical_encoding:one_hot_encoding:use_minimum_fraction': 'True',\n",
              "   'classifier:__choice__': 'random_forest',\n",
              "   'classifier:random_forest:bootstrap': 'True',\n",
              "   'classifier:random_forest:criterion': 'gini',\n",
              "   'classifier:random_forest:max_depth': 'None',\n",
              "   'classifier:random_forest:max_features': 0.5240592829918601,\n",
              "   'classifier:random_forest:max_leaf_nodes': 'None',\n",
              "   'classifier:random_forest:min_impurity_decrease': 0.0,\n",
              "   'classifier:random_forest:min_samples_leaf': 10,\n",
              "   'classifier:random_forest:min_samples_split': 16,\n",
              "   'classifier:random_forest:min_weight_fraction_leaf': 0.0,\n",
              "   'classifier:random_forest:n_estimators': 100,\n",
              "   'imputation:strategy': 'mean',\n",
              "   'preprocessor:__choice__': 'no_preprocessing',\n",
              "   'rescaling:__choice__': 'normalize'},\n",
              "  {'balancing:strategy': 'none',\n",
              "   'categorical_encoding:__choice__': 'no_encoding',\n",
              "   'classifier:__choice__': 'gradient_boosting',\n",
              "   'classifier:gradient_boosting:criterion': 'friedman_mse',\n",
              "   'classifier:gradient_boosting:learning_rate': 0.04093642460278944,\n",
              "   'classifier:gradient_boosting:loss': 'deviance',\n",
              "   'classifier:gradient_boosting:max_depth': 7,\n",
              "   'classifier:gradient_boosting:max_features': 0.5495014745530306,\n",
              "   'classifier:gradient_boosting:max_leaf_nodes': 'None',\n",
              "   'classifier:gradient_boosting:min_impurity_decrease': 0.0,\n",
              "   'classifier:gradient_boosting:min_samples_leaf': 20,\n",
              "   'classifier:gradient_boosting:min_samples_split': 18,\n",
              "   'classifier:gradient_boosting:min_weight_fraction_leaf': 0.0,\n",
              "   'classifier:gradient_boosting:n_estimators': 141,\n",
              "   'classifier:gradient_boosting:subsample': 0.6905343807995293,\n",
              "   'imputation:strategy': 'most_frequent',\n",
              "   'preprocessor:__choice__': 'polynomial',\n",
              "   'preprocessor:polynomial:degree': 3,\n",
              "   'preprocessor:polynomial:include_bias': 'True',\n",
              "   'preprocessor:polynomial:interaction_only': 'False',\n",
              "   'rescaling:__choice__': 'robust_scaler',\n",
              "   'rescaling:robust_scaler:q_max': 0.75,\n",
              "   'rescaling:robust_scaler:q_min': 0.25},\n",
              "  {'balancing:strategy': 'none',\n",
              "   'categorical_encoding:__choice__': 'one_hot_encoding',\n",
              "   'categorical_encoding:one_hot_encoding:minimum_fraction': 0.010000000000000004,\n",
              "   'categorical_encoding:one_hot_encoding:use_minimum_fraction': 'True',\n",
              "   'classifier:__choice__': 'extra_trees',\n",
              "   'classifier:extra_trees:bootstrap': 'True',\n",
              "   'classifier:extra_trees:criterion': 'entropy',\n",
              "   'classifier:extra_trees:max_depth': 'None',\n",
              "   'classifier:extra_trees:max_features': 0.7062102387181676,\n",
              "   'classifier:extra_trees:max_leaf_nodes': 'None',\n",
              "   'classifier:extra_trees:min_impurity_decrease': 0.0,\n",
              "   'classifier:extra_trees:min_samples_leaf': 1,\n",
              "   'classifier:extra_trees:min_samples_split': 20,\n",
              "   'classifier:extra_trees:min_weight_fraction_leaf': 0.0,\n",
              "   'classifier:extra_trees:n_estimators': 100,\n",
              "   'imputation:strategy': 'most_frequent',\n",
              "   'preprocessor:__choice__': 'fast_ica',\n",
              "   'preprocessor:fast_ica:algorithm': 'parallel',\n",
              "   'preprocessor:fast_ica:fun': 'exp',\n",
              "   'preprocessor:fast_ica:n_components': 100,\n",
              "   'preprocessor:fast_ica:whiten': 'True',\n",
              "   'rescaling:__choice__': 'robust_scaler',\n",
              "   'rescaling:robust_scaler:q_max': 0.7065776353150109,\n",
              "   'rescaling:robust_scaler:q_min': 0.23782974987118102},\n",
              "  {'balancing:strategy': 'weighting',\n",
              "   'categorical_encoding:__choice__': 'one_hot_encoding',\n",
              "   'categorical_encoding:one_hot_encoding:use_minimum_fraction': 'False',\n",
              "   'classifier:__choice__': 'random_forest',\n",
              "   'classifier:random_forest:bootstrap': 'False',\n",
              "   'classifier:random_forest:criterion': 'gini',\n",
              "   'classifier:random_forest:max_depth': 'None',\n",
              "   'classifier:random_forest:max_features': 0.7983157215145903,\n",
              "   'classifier:random_forest:max_leaf_nodes': 'None',\n",
              "   'classifier:random_forest:min_impurity_decrease': 0.0,\n",
              "   'classifier:random_forest:min_samples_leaf': 4,\n",
              "   'classifier:random_forest:min_samples_split': 15,\n",
              "   'classifier:random_forest:min_weight_fraction_leaf': 0.0,\n",
              "   'classifier:random_forest:n_estimators': 100,\n",
              "   'imputation:strategy': 'most_frequent',\n",
              "   'preprocessor:__choice__': 'select_percentile_classification',\n",
              "   'preprocessor:select_percentile_classification:percentile': 50.0,\n",
              "   'preprocessor:select_percentile_classification:score_func': 'chi2',\n",
              "   'rescaling:__choice__': 'standardize'},\n",
              "  {'balancing:strategy': 'none',\n",
              "   'categorical_encoding:__choice__': 'no_encoding',\n",
              "   'classifier:__choice__': 'gradient_boosting',\n",
              "   'classifier:gradient_boosting:criterion': 'mse',\n",
              "   'classifier:gradient_boosting:learning_rate': 0.24729845478857812,\n",
              "   'classifier:gradient_boosting:loss': 'deviance',\n",
              "   'classifier:gradient_boosting:max_depth': 5,\n",
              "   'classifier:gradient_boosting:max_features': 0.6564306719064884,\n",
              "   'classifier:gradient_boosting:max_leaf_nodes': 'None',\n",
              "   'classifier:gradient_boosting:min_impurity_decrease': 0.0,\n",
              "   'classifier:gradient_boosting:min_samples_leaf': 15,\n",
              "   'classifier:gradient_boosting:min_samples_split': 14,\n",
              "   'classifier:gradient_boosting:min_weight_fraction_leaf': 0.0,\n",
              "   'classifier:gradient_boosting:n_estimators': 220,\n",
              "   'classifier:gradient_boosting:subsample': 0.8082564085714649,\n",
              "   'imputation:strategy': 'mean',\n",
              "   'preprocessor:__choice__': 'feature_agglomeration',\n",
              "   'preprocessor:feature_agglomeration:affinity': 'euclidean',\n",
              "   'preprocessor:feature_agglomeration:linkage': 'complete',\n",
              "   'preprocessor:feature_agglomeration:n_clusters': 332,\n",
              "   'preprocessor:feature_agglomeration:pooling_func': 'max',\n",
              "   'rescaling:__choice__': 'robust_scaler',\n",
              "   'rescaling:robust_scaler:q_max': 0.75,\n",
              "   'rescaling:robust_scaler:q_min': 0.25},\n",
              "  {'balancing:strategy': 'none',\n",
              "   'categorical_encoding:__choice__': 'no_encoding',\n",
              "   'classifier:__choice__': 'extra_trees',\n",
              "   'classifier:extra_trees:bootstrap': 'False',\n",
              "   'classifier:extra_trees:criterion': 'entropy',\n",
              "   'classifier:extra_trees:max_depth': 'None',\n",
              "   'classifier:extra_trees:max_features': 0.9990999604490303,\n",
              "   'classifier:extra_trees:max_leaf_nodes': 'None',\n",
              "   'classifier:extra_trees:min_impurity_decrease': 0.0,\n",
              "   'classifier:extra_trees:min_samples_leaf': 6,\n",
              "   'classifier:extra_trees:min_samples_split': 4,\n",
              "   'classifier:extra_trees:min_weight_fraction_leaf': 0.0,\n",
              "   'classifier:extra_trees:n_estimators': 100,\n",
              "   'imputation:strategy': 'mean',\n",
              "   'preprocessor:__choice__': 'liblinear_svc_preprocessor',\n",
              "   'preprocessor:liblinear_svc_preprocessor:C': 24.136430622242006,\n",
              "   'preprocessor:liblinear_svc_preprocessor:dual': 'False',\n",
              "   'preprocessor:liblinear_svc_preprocessor:fit_intercept': 'True',\n",
              "   'preprocessor:liblinear_svc_preprocessor:intercept_scaling': 1,\n",
              "   'preprocessor:liblinear_svc_preprocessor:loss': 'squared_hinge',\n",
              "   'preprocessor:liblinear_svc_preprocessor:multi_class': 'ovr',\n",
              "   'preprocessor:liblinear_svc_preprocessor:penalty': 'l1',\n",
              "   'preprocessor:liblinear_svc_preprocessor:tol': 0.003077255210337526,\n",
              "   'rescaling:__choice__': 'robust_scaler',\n",
              "   'rescaling:robust_scaler:q_max': 0.8963215641064294,\n",
              "   'rescaling:robust_scaler:q_min': 0.160294089836629},\n",
              "  {'balancing:strategy': 'none',\n",
              "   'categorical_encoding:__choice__': 'one_hot_encoding',\n",
              "   'categorical_encoding:one_hot_encoding:use_minimum_fraction': 'False',\n",
              "   'classifier:__choice__': 'gradient_boosting',\n",
              "   'classifier:gradient_boosting:criterion': 'mae',\n",
              "   'classifier:gradient_boosting:learning_rate': 0.24729845478857812,\n",
              "   'classifier:gradient_boosting:loss': 'deviance',\n",
              "   'classifier:gradient_boosting:max_depth': 3,\n",
              "   'classifier:gradient_boosting:max_features': 0.6564306719064884,\n",
              "   'classifier:gradient_boosting:max_leaf_nodes': 'None',\n",
              "   'classifier:gradient_boosting:min_impurity_decrease': 0.0,\n",
              "   'classifier:gradient_boosting:min_samples_leaf': 15,\n",
              "   'classifier:gradient_boosting:min_samples_split': 14,\n",
              "   'classifier:gradient_boosting:min_weight_fraction_leaf': 0.0,\n",
              "   'classifier:gradient_boosting:n_estimators': 220,\n",
              "   'classifier:gradient_boosting:subsample': 0.8082564085714649,\n",
              "   'imputation:strategy': 'median',\n",
              "   'preprocessor:__choice__': 'fast_ica',\n",
              "   'preprocessor:fast_ica:algorithm': 'parallel',\n",
              "   'preprocessor:fast_ica:fun': 'logcosh',\n",
              "   'preprocessor:fast_ica:whiten': 'False',\n",
              "   'rescaling:__choice__': 'standardize'},\n",
              "  {'balancing:strategy': 'weighting',\n",
              "   'categorical_encoding:__choice__': 'no_encoding',\n",
              "   'classifier:__choice__': 'extra_trees',\n",
              "   'classifier:extra_trees:bootstrap': 'True',\n",
              "   'classifier:extra_trees:criterion': 'entropy',\n",
              "   'classifier:extra_trees:max_depth': 'None',\n",
              "   'classifier:extra_trees:max_features': 0.8338751730606484,\n",
              "   'classifier:extra_trees:max_leaf_nodes': 'None',\n",
              "   'classifier:extra_trees:min_impurity_decrease': 0.0,\n",
              "   'classifier:extra_trees:min_samples_leaf': 17,\n",
              "   'classifier:extra_trees:min_samples_split': 12,\n",
              "   'classifier:extra_trees:min_weight_fraction_leaf': 0.0,\n",
              "   'classifier:extra_trees:n_estimators': 100,\n",
              "   'imputation:strategy': 'mean',\n",
              "   'preprocessor:__choice__': 'feature_agglomeration',\n",
              "   'preprocessor:feature_agglomeration:affinity': 'cosine',\n",
              "   'preprocessor:feature_agglomeration:linkage': 'average',\n",
              "   'preprocessor:feature_agglomeration:n_clusters': 140,\n",
              "   'preprocessor:feature_agglomeration:pooling_func': 'mean',\n",
              "   'rescaling:__choice__': 'none'},\n",
              "  {'balancing:strategy': 'weighting',\n",
              "   'categorical_encoding:__choice__': 'one_hot_encoding',\n",
              "   'categorical_encoding:one_hot_encoding:minimum_fraction': 0.010000000000000004,\n",
              "   'categorical_encoding:one_hot_encoding:use_minimum_fraction': 'True',\n",
              "   'classifier:__choice__': 'gradient_boosting',\n",
              "   'classifier:gradient_boosting:criterion': 'mse',\n",
              "   'classifier:gradient_boosting:learning_rate': 0.018356703878357986,\n",
              "   'classifier:gradient_boosting:loss': 'deviance',\n",
              "   'classifier:gradient_boosting:max_depth': 3,\n",
              "   'classifier:gradient_boosting:max_features': 0.4445141928073373,\n",
              "   'classifier:gradient_boosting:max_leaf_nodes': 'None',\n",
              "   'classifier:gradient_boosting:min_impurity_decrease': 0.0,\n",
              "   'classifier:gradient_boosting:min_samples_leaf': 19,\n",
              "   'classifier:gradient_boosting:min_samples_split': 3,\n",
              "   'classifier:gradient_boosting:min_weight_fraction_leaf': 0.0,\n",
              "   'classifier:gradient_boosting:n_estimators': 234,\n",
              "   'classifier:gradient_boosting:subsample': 0.3870344708308441,\n",
              "   'imputation:strategy': 'mean',\n",
              "   'preprocessor:__choice__': 'no_preprocessing',\n",
              "   'rescaling:__choice__': 'minmax'},\n",
              "  {'balancing:strategy': 'none',\n",
              "   'categorical_encoding:__choice__': 'no_encoding',\n",
              "   'classifier:__choice__': 'adaboost',\n",
              "   'classifier:adaboost:algorithm': 'SAMME.R',\n",
              "   'classifier:adaboost:learning_rate': 0.022579252711776604,\n",
              "   'classifier:adaboost:max_depth': 7,\n",
              "   'classifier:adaboost:n_estimators': 157,\n",
              "   'imputation:strategy': 'mean',\n",
              "   'preprocessor:__choice__': 'extra_trees_preproc_for_classification',\n",
              "   'preprocessor:extra_trees_preproc_for_classification:bootstrap': 'False',\n",
              "   'preprocessor:extra_trees_preproc_for_classification:criterion': 'entropy',\n",
              "   'preprocessor:extra_trees_preproc_for_classification:max_depth': 'None',\n",
              "   'preprocessor:extra_trees_preproc_for_classification:max_features': 0.6459545924335193,\n",
              "   'preprocessor:extra_trees_preproc_for_classification:max_leaf_nodes': 'None',\n",
              "   'preprocessor:extra_trees_preproc_for_classification:min_impurity_decrease': 0.0,\n",
              "   'preprocessor:extra_trees_preproc_for_classification:min_samples_leaf': 7,\n",
              "   'preprocessor:extra_trees_preproc_for_classification:min_samples_split': 14,\n",
              "   'preprocessor:extra_trees_preproc_for_classification:min_weight_fraction_leaf': 0.0,\n",
              "   'preprocessor:extra_trees_preproc_for_classification:n_estimators': 100,\n",
              "   'rescaling:__choice__': 'minmax'},\n",
              "  {'balancing:strategy': 'weighting',\n",
              "   'categorical_encoding:__choice__': 'one_hot_encoding',\n",
              "   'categorical_encoding:one_hot_encoding:use_minimum_fraction': 'False',\n",
              "   'classifier:__choice__': 'gradient_boosting',\n",
              "   'classifier:gradient_boosting:criterion': 'mse',\n",
              "   'classifier:gradient_boosting:learning_rate': 0.24729845478857812,\n",
              "   'classifier:gradient_boosting:loss': 'deviance',\n",
              "   'classifier:gradient_boosting:max_depth': 6,\n",
              "   'classifier:gradient_boosting:max_features': 0.6564306719064884,\n",
              "   'classifier:gradient_boosting:max_leaf_nodes': 'None',\n",
              "   'classifier:gradient_boosting:min_impurity_decrease': 0.0,\n",
              "   'classifier:gradient_boosting:min_samples_leaf': 15,\n",
              "   'classifier:gradient_boosting:min_samples_split': 14,\n",
              "   'classifier:gradient_boosting:min_weight_fraction_leaf': 0.0,\n",
              "   'classifier:gradient_boosting:n_estimators': 220,\n",
              "   'classifier:gradient_boosting:subsample': 0.8082564085714649,\n",
              "   'imputation:strategy': 'median',\n",
              "   'preprocessor:__choice__': 'pca',\n",
              "   'preprocessor:pca:keep_variance': 0.9999,\n",
              "   'preprocessor:pca:whiten': 'False',\n",
              "   'rescaling:__choice__': 'robust_scaler',\n",
              "   'rescaling:robust_scaler:q_max': 0.75,\n",
              "   'rescaling:robust_scaler:q_min': 0.25},\n",
              "  {'balancing:strategy': 'none',\n",
              "   'categorical_encoding:__choice__': 'no_encoding',\n",
              "   'classifier:__choice__': 'k_nearest_neighbors',\n",
              "   'classifier:k_nearest_neighbors:n_neighbors': 5,\n",
              "   'classifier:k_nearest_neighbors:p': 2,\n",
              "   'classifier:k_nearest_neighbors:weights': 'uniform',\n",
              "   'imputation:strategy': 'mean',\n",
              "   'preprocessor:__choice__': 'pca',\n",
              "   'preprocessor:pca:keep_variance': 0.840817378270975,\n",
              "   'preprocessor:pca:whiten': 'True',\n",
              "   'rescaling:__choice__': 'none'},\n",
              "  {'balancing:strategy': 'weighting',\n",
              "   'categorical_encoding:__choice__': 'one_hot_encoding',\n",
              "   'categorical_encoding:one_hot_encoding:use_minimum_fraction': 'False',\n",
              "   'classifier:__choice__': 'gradient_boosting',\n",
              "   'classifier:gradient_boosting:criterion': 'mse',\n",
              "   'classifier:gradient_boosting:learning_rate': 0.24729845478857812,\n",
              "   'classifier:gradient_boosting:loss': 'deviance',\n",
              "   'classifier:gradient_boosting:max_depth': 6,\n",
              "   'classifier:gradient_boosting:max_features': 0.6564306719064884,\n",
              "   'classifier:gradient_boosting:max_leaf_nodes': 'None',\n",
              "   'classifier:gradient_boosting:min_impurity_decrease': 0.0,\n",
              "   'classifier:gradient_boosting:min_samples_leaf': 15,\n",
              "   'classifier:gradient_boosting:min_samples_split': 14,\n",
              "   'classifier:gradient_boosting:min_weight_fraction_leaf': 0.0,\n",
              "   'classifier:gradient_boosting:n_estimators': 220,\n",
              "   'classifier:gradient_boosting:subsample': 0.8082564085714649,\n",
              "   'imputation:strategy': 'median',\n",
              "   'preprocessor:__choice__': 'pca',\n",
              "   'preprocessor:pca:keep_variance': 0.9999,\n",
              "   'preprocessor:pca:whiten': 'False',\n",
              "   'rescaling:__choice__': 'standardize'},\n",
              "  {'balancing:strategy': 'weighting',\n",
              "   'categorical_encoding:__choice__': 'no_encoding',\n",
              "   'classifier:__choice__': 'adaboost',\n",
              "   'classifier:adaboost:algorithm': 'SAMME',\n",
              "   'classifier:adaboost:learning_rate': 0.01819582056969416,\n",
              "   'classifier:adaboost:max_depth': 9,\n",
              "   'classifier:adaboost:n_estimators': 370,\n",
              "   'imputation:strategy': 'median',\n",
              "   'preprocessor:__choice__': 'select_rates',\n",
              "   'preprocessor:select_rates:alpha': 0.24514751291114034,\n",
              "   'preprocessor:select_rates:mode': 'fdr',\n",
              "   'preprocessor:select_rates:score_func': 'chi2',\n",
              "   'rescaling:__choice__': 'quantile_transformer',\n",
              "   'rescaling:quantile_transformer:n_quantiles': 397,\n",
              "   'rescaling:quantile_transformer:output_distribution': 'uniform'}],\n",
              " 'rank_test_scores': array([ 5,  9,  1, 21, 10,  4, 11, 29, 23, 22, 25, 17,  3, 26, 28, 27,  2,\n",
              "        15, 13, 29, 12, 16,  7, 14, 29, 20, 24, 19,  6, 18,  8, 29]),\n",
              " 'status': ['Success',\n",
              "  'Success',\n",
              "  'Success',\n",
              "  'Success',\n",
              "  'Success',\n",
              "  'Success',\n",
              "  'Success',\n",
              "  'Timeout',\n",
              "  'Success',\n",
              "  'Success',\n",
              "  'Success',\n",
              "  'Success',\n",
              "  'Success',\n",
              "  'Success',\n",
              "  'Success',\n",
              "  'Success',\n",
              "  'Success',\n",
              "  'Success',\n",
              "  'Success',\n",
              "  'Timeout',\n",
              "  'Success',\n",
              "  'Success',\n",
              "  'Success',\n",
              "  'Success',\n",
              "  'Timeout',\n",
              "  'Success',\n",
              "  'Success',\n",
              "  'Success',\n",
              "  'Success',\n",
              "  'Success',\n",
              "  'Success',\n",
              "  'Crash']}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 53
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gcE6ShLsnyrG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "y_score = cls.predict_proba(x_test)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2_wmthRoj-5l",
        "colab_type": "code",
        "outputId": "b468638e-74e1-45d1-ba0f-33f0d9a948bc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 581
        }
      },
      "source": [
        "from sklearn.metrics import accuracy_score, matthews_corrcoef, roc_auc_score\n",
        "print(\"The accuracy is %d\", accuracy_score(y_test, y_pred))\n",
        "#print(\"The MCC is %d\", matthews_corrcoef(y_test, y_pred))\n",
        "#print(\"The AUC-ROC Score is %d\", roc_auc_score(y_test, y_score))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-54-f354fbf5a0e1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmetrics\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0maccuracy_score\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmatthews_corrcoef\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mroc_auc_score\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"The accuracy is %d\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maccuracy_score\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;31m#print(\"The MCC is %d\", matthews_corrcoef(y_test, y_pred))\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m#print(\"The AUC-ROC Score is %d\", roc_auc_score(y_test, y_score))\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/sklearn/metrics/classification.py\u001b[0m in \u001b[0;36maccuracy_score\u001b[0;34m(y_true, y_pred, normalize, sample_weight)\u001b[0m\n\u001b[1;32m    174\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    175\u001b[0m     \u001b[0;31m# Compute accuracy for each possible representation\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 176\u001b[0;31m     \u001b[0my_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_true\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_check_targets\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    177\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0my_type\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'multilabel'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    178\u001b[0m         \u001b[0mdiffering_labels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcount_nonzero\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_true\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0my_pred\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/sklearn/metrics/classification.py\u001b[0m in \u001b[0;36m_check_targets\u001b[0;34m(y_true, y_pred)\u001b[0m\n\u001b[1;32m     69\u001b[0m     \u001b[0my_pred\u001b[0m \u001b[0;34m:\u001b[0m \u001b[0marray\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mindicator\u001b[0m \u001b[0mmatrix\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     70\u001b[0m     \"\"\"\n\u001b[0;32m---> 71\u001b[0;31m     \u001b[0mcheck_consistent_length\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     72\u001b[0m     \u001b[0mtype_true\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtype_of_target\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     73\u001b[0m     \u001b[0mtype_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtype_of_target\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_pred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36mcheck_consistent_length\u001b[0;34m(*arrays)\u001b[0m\n\u001b[1;32m    198\u001b[0m     \"\"\"\n\u001b[1;32m    199\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 200\u001b[0;31m     \u001b[0mlengths\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0m_num_samples\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mX\u001b[0m \u001b[0;32min\u001b[0m \u001b[0marrays\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mX\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    201\u001b[0m     \u001b[0muniques\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munique\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlengths\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    202\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0muniques\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    198\u001b[0m     \"\"\"\n\u001b[1;32m    199\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 200\u001b[0;31m     \u001b[0mlengths\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0m_num_samples\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mX\u001b[0m \u001b[0;32min\u001b[0m \u001b[0marrays\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mX\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    201\u001b[0m     \u001b[0muniques\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munique\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlengths\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    202\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0muniques\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36m_num_samples\u001b[0;34m(x)\u001b[0m\n\u001b[1;32m    107\u001b[0m         \u001b[0;31m# Don't get num_samples from an ensembles length!\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    108\u001b[0m         raise TypeError('Expected sequence or array-like, got '\n\u001b[0;32m--> 109\u001b[0;31m                         'estimator %s' % x)\n\u001b[0m\u001b[1;32m    110\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'__len__'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'shape'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    111\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'__array__'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: Expected sequence or array-like, got estimator AutoSklearnClassifier(delete_output_folder_after_terminate=True,\n           delete_tmp_folder_after_terminate=True,\n           disable_evaluator_output=False, ensemble_memory_limit=1024,\n           ensemble_nbest=50, ensemble_size=50, exclude_estimators=None,\n           exclude_preprocessors=None, get_smac_object_callback=None,\n           include_estimators=None, include_preprocessors=None,\n           initial_configurations_via_metalearning=25, logging_config=None,\n           metadata_directory=None, ml_memory_limit=3072, n_jobs=None,\n           output_folder=None, per_run_time_limit=360,\n           resampling_strategy='cv',\n           resampling_strategy_arguments={'folds': 10}, seed=1,\n           shared_mode=False, smac_scenario_args=None,\n           time_left_for_this_task=3600, tmp_folder=None)"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7E-Lhs8uoGOh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.metrics import accuracy_score, matthews_corrcoef, roc_auc_score\n",
        "print(\"The accuracy is %d\", accuracy_score(y_test, y_pred))\n",
        "print(\"The MCC is %d\", matthews_corrcoef(y_test, y_pred))\n",
        "print(\"The AUC-ROC Score is %d\", roc_auc_score(y_test, y_score))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aKqP2KgY1TZm",
        "colab_type": "text"
      },
      "source": [
        "# Auto Sklearn on Set 2"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Gjb2eJH4oZdp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!apt-get install swig -y\n",
        "!pip install Cython numpy\n",
        "\n",
        "# sometimes you have to run the next command twice on colab\n",
        "# I haven't figured out why\n",
        "!pip install auto-sklearn"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Nh_kYvCB1ndJ",
        "colab_type": "code",
        "outputId": "cb04e1bf-9bba-4fb0-ea9e-82bfc07aab34",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        }
      },
      "source": [
        "import autosklearn.classification"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/ensemble/weight_boosting.py:29: DeprecationWarning: numpy.core.umath_tests is an internal NumPy module and should not be imported. It will be removed in a future NumPy release.\n",
            "  from numpy.core.umath_tests import inner1d\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uCZdcA111pBJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "cls2 = autosklearn.classification.AutoSklearnClassifier()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Rocj_7YF2mgj",
        "colab_type": "code",
        "outputId": "b952073a-6dff-4854-e72a-a275dcfea7c8",
        "colab": {
          "resources": {
            "http://localhost:8080/nbextensions/google.colab/files.js": {
              "data": "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",
              "ok": true,
              "headers": [
                [
                  "content-type",
                  "application/javascript"
                ]
              ],
              "status": 200,
              "status_text": ""
            }
          },
          "base_uri": "https://localhost:8080/",
          "height": 74
        }
      },
      "source": [
        "from google.colab import files\n",
        "uploaded = files.upload()\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-b594934c-5156-419d-bf94-3bf2c3144acd\" name=\"files[]\" multiple disabled />\n",
              "     <output id=\"result-b594934c-5156-419d-bf94-3bf2c3144acd\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script src=\"/nbextensions/google.colab/files.js\"></script> "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Saving AComp2.csv to AComp2 (1).csv\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sMpEtjws2uPT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pandas as pd\n",
        "import io\n",
        "df = pd.read_csv(io.BytesIO(uploaded['AComp2.csv']))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "49MoNTcf22HR",
        "colab_type": "code",
        "outputId": "1c53518d-5fed-4090-928d-eb942b2c3ec8",
        "colab": {
          "resources": {
            "http://localhost:8080/nbextensions/google.colab/files.js": {
              "data": "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",
              "ok": true,
              "headers": [
                [
                  "content-type",
                  "application/javascript"
                ]
              ],
              "status": 200,
              "status_text": ""
            }
          },
          "base_uri": "https://localhost:8080/",
          "height": 74
        }
      },
      "source": [
        "from google.colab import files\n",
        "uploaded = files.upload()\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-619065ad-fe4e-4dfd-9166-48dd6b83ed66\" name=\"files[]\" multiple disabled />\n",
              "     <output id=\"result-619065ad-fe4e-4dfd-9166-48dd6b83ed66\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script src=\"/nbextensions/google.colab/files.js\"></script> "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Saving test_AComp2.csv to test_AComp2 (1).csv\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9zgwAQH824L9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df2 = pd.read_csv(io.BytesIO(uploaded['test_AComp2.csv']))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lXkbWx1p3-fA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "x_train = df.iloc[:, 0:20].values\n",
        "y_train = df.iloc[:, 20].values\n",
        "x_test = df2.iloc[:, 0:20].values\n",
        "y_test = df2.iloc[:, 20].values"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LyLV5IKb4SAX",
        "colab_type": "code",
        "outputId": "916cc541-36fe-4da1-837a-8349a7ee0b4e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 4100
        }
      },
      "source": [
        "cls2.fit(x_train, y_train)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/autosklearn/evaluation/train_evaluator.py:197: RuntimeWarning: Mean of empty slice\n",
            "  Y_train_pred = np.nanmean(Y_train_pred_full, axis=0)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[WARNING] [2019-05-30 17:55:18,287:EnsembleBuilder(1):19ed0399fa9f60a2f494cd961a1c6a59] No models better than random - using Dummy Score!\n",
            "[WARNING] [2019-05-30 17:55:18,299:EnsembleBuilder(1):19ed0399fa9f60a2f494cd961a1c6a59] No models better than random - using Dummy Score!\n",
            "[WARNING] [2019-05-30 17:55:20,310:EnsembleBuilder(1):19ed0399fa9f60a2f494cd961a1c6a59] No models better than random - using Dummy Score!\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/autosklearn/evaluation/train_evaluator.py:197: RuntimeWarning: Mean of empty slice\n",
            "  Y_train_pred = np.nanmean(Y_train_pred_full, axis=0)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[WARNING] [2019-05-30 17:55:22,315:EnsembleBuilder(1):19ed0399fa9f60a2f494cd961a1c6a59] No models better than random - using Dummy Score!\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/autosklearn/evaluation/train_evaluator.py:197: RuntimeWarning: Mean of empty slice\n",
            "  Y_train_pred = np.nanmean(Y_train_pred_full, axis=0)\n",
            "/usr/local/lib/python3.6/dist-packages/autosklearn/evaluation/train_evaluator.py:197: RuntimeWarning: Mean of empty slice\n",
            "  Y_train_pred = np.nanmean(Y_train_pred_full, axis=0)\n",
            "/usr/local/lib/python3.6/dist-packages/autosklearn/evaluation/train_evaluator.py:197: RuntimeWarning: Mean of empty slice\n",
            "  Y_train_pred = np.nanmean(Y_train_pred_full, axis=0)\n",
            "/usr/local/lib/python3.6/dist-packages/autosklearn/evaluation/train_evaluator.py:197: RuntimeWarning: Mean of empty slice\n",
            "  Y_train_pred = np.nanmean(Y_train_pred_full, axis=0)\n",
            "/usr/local/lib/python3.6/dist-packages/autosklearn/evaluation/train_evaluator.py:197: RuntimeWarning: Mean of empty slice\n",
            "  Y_train_pred = np.nanmean(Y_train_pred_full, axis=0)\n",
            "/usr/local/lib/python3.6/dist-packages/autosklearn/evaluation/train_evaluator.py:197: RuntimeWarning: Mean of empty slice\n",
            "  Y_train_pred = np.nanmean(Y_train_pred_full, axis=0)\n",
            "/usr/local/lib/python3.6/dist-packages/autosklearn/evaluation/train_evaluator.py:197: RuntimeWarning: Mean of empty slice\n",
            "  Y_train_pred = np.nanmean(Y_train_pred_full, axis=0)\n",
            "/usr/local/lib/python3.6/dist-packages/autosklearn/evaluation/train_evaluator.py:197: RuntimeWarning: Mean of empty slice\n",
            "  Y_train_pred = np.nanmean(Y_train_pred_full, axis=0)\n",
            "/usr/local/lib/python3.6/dist-packages/autosklearn/evaluation/train_evaluator.py:197: RuntimeWarning: Mean of empty slice\n",
            "  Y_train_pred = np.nanmean(Y_train_pred_full, axis=0)\n",
            "/usr/local/lib/python3.6/dist-packages/autosklearn/evaluation/train_evaluator.py:197: RuntimeWarning: Mean of empty slice\n",
            "  Y_train_pred = np.nanmean(Y_train_pred_full, axis=0)\n",
            "/usr/local/lib/python3.6/dist-packages/autosklearn/evaluation/train_evaluator.py:197: RuntimeWarning: Mean of empty slice\n",
            "  Y_train_pred = np.nanmean(Y_train_pred_full, axis=0)\n",
            "/usr/local/lib/python3.6/dist-packages/autosklearn/evaluation/train_evaluator.py:197: RuntimeWarning: Mean of empty slice\n",
            "  Y_train_pred = np.nanmean(Y_train_pred_full, axis=0)\n",
            "/usr/local/lib/python3.6/dist-packages/autosklearn/evaluation/train_evaluator.py:197: RuntimeWarning: Mean of empty slice\n",
            "  Y_train_pred = np.nanmean(Y_train_pred_full, axis=0)\n",
            "/usr/local/lib/python3.6/dist-packages/autosklearn/evaluation/train_evaluator.py:197: RuntimeWarning: Mean of empty slice\n",
            "  Y_train_pred = np.nanmean(Y_train_pred_full, axis=0)\n",
            "/usr/local/lib/python3.6/dist-packages/autosklearn/evaluation/train_evaluator.py:197: RuntimeWarning: Mean of empty slice\n",
            "  Y_train_pred = np.nanmean(Y_train_pred_full, axis=0)\n",
            "/usr/local/lib/python3.6/dist-packages/autosklearn/evaluation/train_evaluator.py:197: RuntimeWarning: Mean of empty slice\n",
            "  Y_train_pred = np.nanmean(Y_train_pred_full, axis=0)\n",
            "/usr/local/lib/python3.6/dist-packages/autosklearn/evaluation/train_evaluator.py:197: RuntimeWarning: Mean of empty slice\n",
            "  Y_train_pred = np.nanmean(Y_train_pred_full, axis=0)\n",
            "/usr/local/lib/python3.6/dist-packages/autosklearn/evaluation/train_evaluator.py:197: RuntimeWarning: Mean of empty slice\n",
            "  Y_train_pred = np.nanmean(Y_train_pred_full, axis=0)\n",
            "/usr/local/lib/python3.6/dist-packages/autosklearn/evaluation/train_evaluator.py:197: RuntimeWarning: Mean of empty slice\n",
            "  Y_train_pred = np.nanmean(Y_train_pred_full, axis=0)\n",
            "/usr/local/lib/python3.6/dist-packages/autosklearn/evaluation/train_evaluator.py:197: RuntimeWarning: Mean of empty slice\n",
            "  Y_train_pred = np.nanmean(Y_train_pred_full, axis=0)\n",
            "/usr/local/lib/python3.6/dist-packages/autosklearn/evaluation/train_evaluator.py:197: RuntimeWarning: Mean of empty slice\n",
            "  Y_train_pred = np.nanmean(Y_train_pred_full, axis=0)\n",
            "/usr/local/lib/python3.6/dist-packages/autosklearn/evaluation/train_evaluator.py:197: RuntimeWarning: Mean of empty slice\n",
            "  Y_train_pred = np.nanmean(Y_train_pred_full, axis=0)\n",
            "/usr/local/lib/python3.6/dist-packages/autosklearn/evaluation/train_evaluator.py:197: RuntimeWarning: Mean of empty slice\n",
            "  Y_train_pred = np.nanmean(Y_train_pred_full, axis=0)\n",
            "/usr/local/lib/python3.6/dist-packages/autosklearn/evaluation/train_evaluator.py:197: RuntimeWarning: Mean of empty slice\n",
            "  Y_train_pred = np.nanmean(Y_train_pred_full, axis=0)\n",
            "/usr/local/lib/python3.6/dist-packages/autosklearn/evaluation/train_evaluator.py:197: RuntimeWarning: Mean of empty slice\n",
            "  Y_train_pred = np.nanmean(Y_train_pred_full, axis=0)\n",
            "/usr/local/lib/python3.6/dist-packages/autosklearn/evaluation/train_evaluator.py:197: RuntimeWarning: Mean of empty slice\n",
            "  Y_train_pred = np.nanmean(Y_train_pred_full, axis=0)\n",
            "/usr/local/lib/python3.6/dist-packages/autosklearn/evaluation/train_evaluator.py:197: RuntimeWarning: Mean of empty slice\n",
            "  Y_train_pred = np.nanmean(Y_train_pred_full, axis=0)\n",
            "/usr/local/lib/python3.6/dist-packages/autosklearn/evaluation/train_evaluator.py:197: RuntimeWarning: Mean of empty slice\n",
            "  Y_train_pred = np.nanmean(Y_train_pred_full, axis=0)\n",
            "/usr/local/lib/python3.6/dist-packages/autosklearn/evaluation/train_evaluator.py:197: RuntimeWarning: Mean of empty slice\n",
            "  Y_train_pred = np.nanmean(Y_train_pred_full, axis=0)\n",
            "/usr/local/lib/python3.6/dist-packages/autosklearn/evaluation/train_evaluator.py:197: RuntimeWarning: Mean of empty slice\n",
            "  Y_train_pred = np.nanmean(Y_train_pred_full, axis=0)\n",
            "/usr/local/lib/python3.6/dist-packages/autosklearn/evaluation/train_evaluator.py:197: RuntimeWarning: Mean of empty slice\n",
            "  Y_train_pred = np.nanmean(Y_train_pred_full, axis=0)\n",
            "/usr/local/lib/python3.6/dist-packages/autosklearn/evaluation/train_evaluator.py:197: RuntimeWarning: Mean of empty slice\n",
            "  Y_train_pred = np.nanmean(Y_train_pred_full, axis=0)\n",
            "/usr/local/lib/python3.6/dist-packages/autosklearn/evaluation/train_evaluator.py:197: RuntimeWarning: Mean of empty slice\n",
            "  Y_train_pred = np.nanmean(Y_train_pred_full, axis=0)\n",
            "/usr/local/lib/python3.6/dist-packages/autosklearn/evaluation/train_evaluator.py:197: RuntimeWarning: Mean of empty slice\n",
            "  Y_train_pred = np.nanmean(Y_train_pred_full, axis=0)\n",
            "/usr/local/lib/python3.6/dist-packages/autosklearn/evaluation/train_evaluator.py:197: RuntimeWarning: Mean of empty slice\n",
            "  Y_train_pred = np.nanmean(Y_train_pred_full, axis=0)\n",
            "/usr/local/lib/python3.6/dist-packages/autosklearn/evaluation/train_evaluator.py:197: RuntimeWarning: Mean of empty slice\n",
            "  Y_train_pred = np.nanmean(Y_train_pred_full, axis=0)\n",
            "/usr/local/lib/python3.6/dist-packages/autosklearn/evaluation/train_evaluator.py:197: RuntimeWarning: Mean of empty slice\n",
            "  Y_train_pred = np.nanmean(Y_train_pred_full, axis=0)\n",
            "/usr/local/lib/python3.6/dist-packages/autosklearn/evaluation/train_evaluator.py:197: RuntimeWarning: Mean of empty slice\n",
            "  Y_train_pred = np.nanmean(Y_train_pred_full, axis=0)\n",
            "/usr/local/lib/python3.6/dist-packages/autosklearn/evaluation/train_evaluator.py:197: RuntimeWarning: Mean of empty slice\n",
            "  Y_train_pred = np.nanmean(Y_train_pred_full, axis=0)\n",
            "/usr/local/lib/python3.6/dist-packages/autosklearn/evaluation/train_evaluator.py:197: RuntimeWarning: Mean of empty slice\n",
            "  Y_train_pred = np.nanmean(Y_train_pred_full, axis=0)\n",
            "/usr/local/lib/python3.6/dist-packages/autosklearn/evaluation/train_evaluator.py:197: RuntimeWarning: Mean of empty slice\n",
            "  Y_train_pred = np.nanmean(Y_train_pred_full, axis=0)\n",
            "/usr/local/lib/python3.6/dist-packages/autosklearn/evaluation/train_evaluator.py:197: RuntimeWarning: Mean of empty slice\n",
            "  Y_train_pred = np.nanmean(Y_train_pred_full, axis=0)\n",
            "/usr/local/lib/python3.6/dist-packages/autosklearn/evaluation/train_evaluator.py:197: RuntimeWarning: Mean of empty slice\n",
            "  Y_train_pred = np.nanmean(Y_train_pred_full, axis=0)\n",
            "/usr/local/lib/python3.6/dist-packages/autosklearn/evaluation/train_evaluator.py:197: RuntimeWarning: Mean of empty slice\n",
            "  Y_train_pred = np.nanmean(Y_train_pred_full, axis=0)\n",
            "/usr/local/lib/python3.6/dist-packages/autosklearn/evaluation/train_evaluator.py:197: RuntimeWarning: Mean of empty slice\n",
            "  Y_train_pred = np.nanmean(Y_train_pred_full, axis=0)\n",
            "/usr/local/lib/python3.6/dist-packages/autosklearn/evaluation/train_evaluator.py:197: RuntimeWarning: Mean of empty slice\n",
            "  Y_train_pred = np.nanmean(Y_train_pred_full, axis=0)\n",
            "/usr/local/lib/python3.6/dist-packages/autosklearn/evaluation/train_evaluator.py:197: RuntimeWarning: Mean of empty slice\n",
            "  Y_train_pred = np.nanmean(Y_train_pred_full, axis=0)\n",
            "/usr/local/lib/python3.6/dist-packages/autosklearn/evaluation/train_evaluator.py:197: RuntimeWarning: Mean of empty slice\n",
            "  Y_train_pred = np.nanmean(Y_train_pred_full, axis=0)\n",
            "/usr/local/lib/python3.6/dist-packages/autosklearn/evaluation/train_evaluator.py:197: RuntimeWarning: Mean of empty slice\n",
            "  Y_train_pred = np.nanmean(Y_train_pred_full, axis=0)\n",
            "/usr/local/lib/python3.6/dist-packages/autosklearn/evaluation/train_evaluator.py:197: RuntimeWarning: Mean of empty slice\n",
            "  Y_train_pred = np.nanmean(Y_train_pred_full, axis=0)\n",
            "/usr/local/lib/python3.6/dist-packages/autosklearn/evaluation/train_evaluator.py:197: RuntimeWarning: Mean of empty slice\n",
            "  Y_train_pred = np.nanmean(Y_train_pred_full, axis=0)\n",
            "/usr/local/lib/python3.6/dist-packages/autosklearn/evaluation/train_evaluator.py:197: RuntimeWarning: Mean of empty slice\n",
            "  Y_train_pred = np.nanmean(Y_train_pred_full, axis=0)\n",
            "/usr/local/lib/python3.6/dist-packages/autosklearn/evaluation/train_evaluator.py:197: RuntimeWarning: Mean of empty slice\n",
            "  Y_train_pred = np.nanmean(Y_train_pred_full, axis=0)\n",
            "/usr/local/lib/python3.6/dist-packages/autosklearn/evaluation/train_evaluator.py:197: RuntimeWarning: Mean of empty slice\n",
            "  Y_train_pred = np.nanmean(Y_train_pred_full, axis=0)\n",
            "/usr/local/lib/python3.6/dist-packages/autosklearn/evaluation/train_evaluator.py:197: RuntimeWarning: Mean of empty slice\n",
            "  Y_train_pred = np.nanmean(Y_train_pred_full, axis=0)\n",
            "/usr/local/lib/python3.6/dist-packages/autosklearn/evaluation/train_evaluator.py:197: RuntimeWarning: Mean of empty slice\n",
            "  Y_train_pred = np.nanmean(Y_train_pred_full, axis=0)\n",
            "/usr/local/lib/python3.6/dist-packages/autosklearn/evaluation/train_evaluator.py:197: RuntimeWarning: Mean of empty slice\n",
            "  Y_train_pred = np.nanmean(Y_train_pred_full, axis=0)\n",
            "/usr/local/lib/python3.6/dist-packages/autosklearn/evaluation/train_evaluator.py:197: RuntimeWarning: Mean of empty slice\n",
            "  Y_train_pred = np.nanmean(Y_train_pred_full, axis=0)\n",
            "/usr/local/lib/python3.6/dist-packages/autosklearn/evaluation/train_evaluator.py:197: RuntimeWarning: Mean of empty slice\n",
            "  Y_train_pred = np.nanmean(Y_train_pred_full, axis=0)\n",
            "/usr/local/lib/python3.6/dist-packages/autosklearn/evaluation/train_evaluator.py:197: RuntimeWarning: Mean of empty slice\n",
            "  Y_train_pred = np.nanmean(Y_train_pred_full, axis=0)\n",
            "/usr/local/lib/python3.6/dist-packages/autosklearn/evaluation/train_evaluator.py:197: RuntimeWarning: Mean of empty slice\n",
            "  Y_train_pred = np.nanmean(Y_train_pred_full, axis=0)\n",
            "/usr/local/lib/python3.6/dist-packages/autosklearn/evaluation/train_evaluator.py:197: RuntimeWarning: Mean of empty slice\n",
            "  Y_train_pred = np.nanmean(Y_train_pred_full, axis=0)\n",
            "/usr/local/lib/python3.6/dist-packages/autosklearn/evaluation/train_evaluator.py:197: RuntimeWarning: Mean of empty slice\n",
            "  Y_train_pred = np.nanmean(Y_train_pred_full, axis=0)\n",
            "/usr/local/lib/python3.6/dist-packages/autosklearn/evaluation/train_evaluator.py:197: RuntimeWarning: Mean of empty slice\n",
            "  Y_train_pred = np.nanmean(Y_train_pred_full, axis=0)\n",
            "/usr/local/lib/python3.6/dist-packages/autosklearn/evaluation/train_evaluator.py:197: RuntimeWarning: Mean of empty slice\n",
            "  Y_train_pred = np.nanmean(Y_train_pred_full, axis=0)\n",
            "/usr/local/lib/python3.6/dist-packages/autosklearn/evaluation/train_evaluator.py:197: RuntimeWarning: Mean of empty slice\n",
            "  Y_train_pred = np.nanmean(Y_train_pred_full, axis=0)\n",
            "/usr/local/lib/python3.6/dist-packages/autosklearn/evaluation/train_evaluator.py:197: RuntimeWarning: Mean of empty slice\n",
            "  Y_train_pred = np.nanmean(Y_train_pred_full, axis=0)\n",
            "/usr/local/lib/python3.6/dist-packages/autosklearn/evaluation/train_evaluator.py:197: RuntimeWarning: Mean of empty slice\n",
            "  Y_train_pred = np.nanmean(Y_train_pred_full, axis=0)\n",
            "/usr/local/lib/python3.6/dist-packages/autosklearn/evaluation/train_evaluator.py:197: RuntimeWarning: Mean of empty slice\n",
            "  Y_train_pred = np.nanmean(Y_train_pred_full, axis=0)\n",
            "/usr/local/lib/python3.6/dist-packages/autosklearn/evaluation/train_evaluator.py:197: RuntimeWarning: Mean of empty slice\n",
            "  Y_train_pred = np.nanmean(Y_train_pred_full, axis=0)\n",
            "/usr/local/lib/python3.6/dist-packages/autosklearn/evaluation/train_evaluator.py:197: RuntimeWarning: Mean of empty slice\n",
            "  Y_train_pred = np.nanmean(Y_train_pred_full, axis=0)\n",
            "/usr/local/lib/python3.6/dist-packages/autosklearn/evaluation/train_evaluator.py:197: RuntimeWarning: Mean of empty slice\n",
            "  Y_train_pred = np.nanmean(Y_train_pred_full, axis=0)\n",
            "/usr/local/lib/python3.6/dist-packages/autosklearn/evaluation/train_evaluator.py:197: RuntimeWarning: Mean of empty slice\n",
            "  Y_train_pred = np.nanmean(Y_train_pred_full, axis=0)\n",
            "/usr/local/lib/python3.6/dist-packages/autosklearn/evaluation/train_evaluator.py:197: RuntimeWarning: Mean of empty slice\n",
            "  Y_train_pred = np.nanmean(Y_train_pred_full, axis=0)\n",
            "/usr/local/lib/python3.6/dist-packages/autosklearn/evaluation/train_evaluator.py:197: RuntimeWarning: Mean of empty slice\n",
            "  Y_train_pred = np.nanmean(Y_train_pred_full, axis=0)\n",
            "/usr/local/lib/python3.6/dist-packages/autosklearn/evaluation/train_evaluator.py:197: RuntimeWarning: Mean of empty slice\n",
            "  Y_train_pred = np.nanmean(Y_train_pred_full, axis=0)\n",
            "/usr/local/lib/python3.6/dist-packages/autosklearn/evaluation/train_evaluator.py:197: RuntimeWarning: Mean of empty slice\n",
            "  Y_train_pred = np.nanmean(Y_train_pred_full, axis=0)\n",
            "/usr/local/lib/python3.6/dist-packages/autosklearn/evaluation/train_evaluator.py:197: RuntimeWarning: Mean of empty slice\n",
            "  Y_train_pred = np.nanmean(Y_train_pred_full, axis=0)\n",
            "/usr/local/lib/python3.6/dist-packages/autosklearn/evaluation/train_evaluator.py:197: RuntimeWarning: Mean of empty slice\n",
            "  Y_train_pred = np.nanmean(Y_train_pred_full, axis=0)\n",
            "/usr/local/lib/python3.6/dist-packages/autosklearn/evaluation/train_evaluator.py:197: RuntimeWarning: Mean of empty slice\n",
            "  Y_train_pred = np.nanmean(Y_train_pred_full, axis=0)\n",
            "/usr/local/lib/python3.6/dist-packages/autosklearn/evaluation/train_evaluator.py:197: RuntimeWarning: Mean of empty slice\n",
            "  Y_train_pred = np.nanmean(Y_train_pred_full, axis=0)\n",
            "/usr/local/lib/python3.6/dist-packages/autosklearn/evaluation/train_evaluator.py:197: RuntimeWarning: Mean of empty slice\n",
            "  Y_train_pred = np.nanmean(Y_train_pred_full, axis=0)\n",
            "/usr/local/lib/python3.6/dist-packages/autosklearn/evaluation/train_evaluator.py:197: RuntimeWarning: Mean of empty slice\n",
            "  Y_train_pred = np.nanmean(Y_train_pred_full, axis=0)\n",
            "/usr/local/lib/python3.6/dist-packages/autosklearn/evaluation/train_evaluator.py:197: RuntimeWarning: Mean of empty slice\n",
            "  Y_train_pred = np.nanmean(Y_train_pred_full, axis=0)\n",
            "/usr/local/lib/python3.6/dist-packages/autosklearn/evaluation/train_evaluator.py:197: RuntimeWarning: Mean of empty slice\n",
            "  Y_train_pred = np.nanmean(Y_train_pred_full, axis=0)\n",
            "/usr/local/lib/python3.6/dist-packages/autosklearn/evaluation/train_evaluator.py:197: RuntimeWarning: Mean of empty slice\n",
            "  Y_train_pred = np.nanmean(Y_train_pred_full, axis=0)\n",
            "/usr/local/lib/python3.6/dist-packages/autosklearn/evaluation/train_evaluator.py:197: RuntimeWarning: Mean of empty slice\n",
            "  Y_train_pred = np.nanmean(Y_train_pred_full, axis=0)\n",
            "/usr/local/lib/python3.6/dist-packages/autosklearn/evaluation/train_evaluator.py:197: RuntimeWarning: Mean of empty slice\n",
            "  Y_train_pred = np.nanmean(Y_train_pred_full, axis=0)\n",
            "/usr/local/lib/python3.6/dist-packages/autosklearn/evaluation/train_evaluator.py:197: RuntimeWarning: Mean of empty slice\n",
            "  Y_train_pred = np.nanmean(Y_train_pred_full, axis=0)\n",
            "/usr/local/lib/python3.6/dist-packages/autosklearn/evaluation/train_evaluator.py:197: RuntimeWarning: Mean of empty slice\n",
            "  Y_train_pred = np.nanmean(Y_train_pred_full, axis=0)\n",
            "/usr/local/lib/python3.6/dist-packages/autosklearn/evaluation/train_evaluator.py:197: RuntimeWarning: Mean of empty slice\n",
            "  Y_train_pred = np.nanmean(Y_train_pred_full, axis=0)\n",
            "/usr/local/lib/python3.6/dist-packages/autosklearn/evaluation/train_evaluator.py:197: RuntimeWarning: Mean of empty slice\n",
            "  Y_train_pred = np.nanmean(Y_train_pred_full, axis=0)\n",
            "/usr/local/lib/python3.6/dist-packages/autosklearn/evaluation/train_evaluator.py:197: RuntimeWarning: Mean of empty slice\n",
            "  Y_train_pred = np.nanmean(Y_train_pred_full, axis=0)\n",
            "/usr/local/lib/python3.6/dist-packages/autosklearn/evaluation/train_evaluator.py:197: RuntimeWarning: Mean of empty slice\n",
            "  Y_train_pred = np.nanmean(Y_train_pred_full, axis=0)\n",
            "/usr/local/lib/python3.6/dist-packages/autosklearn/evaluation/train_evaluator.py:197: RuntimeWarning: Mean of empty slice\n",
            "  Y_train_pred = np.nanmean(Y_train_pred_full, axis=0)\n",
            "/usr/local/lib/python3.6/dist-packages/autosklearn/evaluation/train_evaluator.py:197: RuntimeWarning: Mean of empty slice\n",
            "  Y_train_pred = np.nanmean(Y_train_pred_full, axis=0)\n",
            "/usr/local/lib/python3.6/dist-packages/autosklearn/evaluation/train_evaluator.py:197: RuntimeWarning: Mean of empty slice\n",
            "  Y_train_pred = np.nanmean(Y_train_pred_full, axis=0)\n",
            "/usr/local/lib/python3.6/dist-packages/autosklearn/evaluation/train_evaluator.py:197: RuntimeWarning: Mean of empty slice\n",
            "  Y_train_pred = np.nanmean(Y_train_pred_full, axis=0)\n",
            "/usr/local/lib/python3.6/dist-packages/autosklearn/evaluation/train_evaluator.py:197: RuntimeWarning: Mean of empty slice\n",
            "  Y_train_pred = np.nanmean(Y_train_pred_full, axis=0)\n",
            "/usr/local/lib/python3.6/dist-packages/autosklearn/evaluation/train_evaluator.py:197: RuntimeWarning: Mean of empty slice\n",
            "  Y_train_pred = np.nanmean(Y_train_pred_full, axis=0)\n",
            "/usr/local/lib/python3.6/dist-packages/autosklearn/evaluation/train_evaluator.py:197: RuntimeWarning: Mean of empty slice\n",
            "  Y_train_pred = np.nanmean(Y_train_pred_full, axis=0)\n",
            "/usr/local/lib/python3.6/dist-packages/autosklearn/evaluation/train_evaluator.py:197: RuntimeWarning: Mean of empty slice\n",
            "  Y_train_pred = np.nanmean(Y_train_pred_full, axis=0)\n",
            "/usr/local/lib/python3.6/dist-packages/autosklearn/evaluation/train_evaluator.py:197: RuntimeWarning: Mean of empty slice\n",
            "  Y_train_pred = np.nanmean(Y_train_pred_full, axis=0)\n",
            "/usr/local/lib/python3.6/dist-packages/autosklearn/evaluation/train_evaluator.py:197: RuntimeWarning: Mean of empty slice\n",
            "  Y_train_pred = np.nanmean(Y_train_pred_full, axis=0)\n",
            "/usr/local/lib/python3.6/dist-packages/autosklearn/evaluation/train_evaluator.py:197: RuntimeWarning: Mean of empty slice\n",
            "  Y_train_pred = np.nanmean(Y_train_pred_full, axis=0)\n",
            "/usr/local/lib/python3.6/dist-packages/autosklearn/evaluation/train_evaluator.py:197: RuntimeWarning: Mean of empty slice\n",
            "  Y_train_pred = np.nanmean(Y_train_pred_full, axis=0)\n",
            "/usr/local/lib/python3.6/dist-packages/autosklearn/evaluation/train_evaluator.py:197: RuntimeWarning: Mean of empty slice\n",
            "  Y_train_pred = np.nanmean(Y_train_pred_full, axis=0)\n",
            "/usr/local/lib/python3.6/dist-packages/autosklearn/evaluation/train_evaluator.py:197: RuntimeWarning: Mean of empty slice\n",
            "  Y_train_pred = np.nanmean(Y_train_pred_full, axis=0)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "1\n",
            "['/tmp/autosklearn_tmp_131_3838/.auto-sklearn/ensembles/1.0000000000.ensemble', '/tmp/autosklearn_tmp_131_3838/.auto-sklearn/ensembles/1.0000000001.ensemble', '/tmp/autosklearn_tmp_131_3838/.auto-sklearn/ensembles/1.0000000002.ensemble', '/tmp/autosklearn_tmp_131_3838/.auto-sklearn/ensembles/1.0000000003.ensemble', '/tmp/autosklearn_tmp_131_3838/.auto-sklearn/ensembles/1.0000000004.ensemble', '/tmp/autosklearn_tmp_131_3838/.auto-sklearn/ensembles/1.0000000005.ensemble', '/tmp/autosklearn_tmp_131_3838/.auto-sklearn/ensembles/1.0000000006.ensemble', '/tmp/autosklearn_tmp_131_3838/.auto-sklearn/ensembles/1.0000000007.ensemble', '/tmp/autosklearn_tmp_131_3838/.auto-sklearn/ensembles/1.0000000008.ensemble', '/tmp/autosklearn_tmp_131_3838/.auto-sklearn/ensembles/1.0000000009.ensemble', '/tmp/autosklearn_tmp_131_3838/.auto-sklearn/ensembles/1.0000000010.ensemble', '/tmp/autosklearn_tmp_131_3838/.auto-sklearn/ensembles/1.0000000011.ensemble', '/tmp/autosklearn_tmp_131_3838/.auto-sklearn/ensembles/1.0000000012.ensemble', '/tmp/autosklearn_tmp_131_3838/.auto-sklearn/ensembles/1.0000000013.ensemble', '/tmp/autosklearn_tmp_131_3838/.auto-sklearn/ensembles/1.0000000014.ensemble', '/tmp/autosklearn_tmp_131_3838/.auto-sklearn/ensembles/1.0000000015.ensemble', '/tmp/autosklearn_tmp_131_3838/.auto-sklearn/ensembles/1.0000000016.ensemble', '/tmp/autosklearn_tmp_131_3838/.auto-sklearn/ensembles/1.0000000017.ensemble', '/tmp/autosklearn_tmp_131_3838/.auto-sklearn/ensembles/1.0000000018.ensemble', '/tmp/autosklearn_tmp_131_3838/.auto-sklearn/ensembles/1.0000000019.ensemble', '/tmp/autosklearn_tmp_131_3838/.auto-sklearn/ensembles/1.0000000020.ensemble', '/tmp/autosklearn_tmp_131_3838/.auto-sklearn/ensembles/1.0000000021.ensemble', '/tmp/autosklearn_tmp_131_3838/.auto-sklearn/ensembles/1.0000000022.ensemble', '/tmp/autosklearn_tmp_131_3838/.auto-sklearn/ensembles/1.0000000023.ensemble', '/tmp/autosklearn_tmp_131_3838/.auto-sklearn/ensembles/1.0000000024.ensemble', '/tmp/autosklearn_tmp_131_3838/.auto-sklearn/ensembles/1.0000000025.ensemble', '/tmp/autosklearn_tmp_131_3838/.auto-sklearn/ensembles/1.0000000026.ensemble', '/tmp/autosklearn_tmp_131_3838/.auto-sklearn/ensembles/1.0000000027.ensemble', '/tmp/autosklearn_tmp_131_3838/.auto-sklearn/ensembles/1.0000000028.ensemble', '/tmp/autosklearn_tmp_131_3838/.auto-sklearn/ensembles/1.0000000029.ensemble', '/tmp/autosklearn_tmp_131_3838/.auto-sklearn/ensembles/1.0000000030.ensemble', '/tmp/autosklearn_tmp_131_3838/.auto-sklearn/ensembles/1.0000000031.ensemble', '/tmp/autosklearn_tmp_131_3838/.auto-sklearn/ensembles/1.0000000032.ensemble', '/tmp/autosklearn_tmp_131_3838/.auto-sklearn/ensembles/1.0000000033.ensemble', '/tmp/autosklearn_tmp_131_3838/.auto-sklearn/ensembles/1.0000000034.ensemble', '/tmp/autosklearn_tmp_131_3838/.auto-sklearn/ensembles/1.0000000035.ensemble', '/tmp/autosklearn_tmp_131_3838/.auto-sklearn/ensembles/1.0000000036.ensemble', '/tmp/autosklearn_tmp_131_3838/.auto-sklearn/ensembles/1.0000000037.ensemble', '/tmp/autosklearn_tmp_131_3838/.auto-sklearn/ensembles/1.0000000038.ensemble', '/tmp/autosklearn_tmp_131_3838/.auto-sklearn/ensembles/1.0000000039.ensemble', '/tmp/autosklearn_tmp_131_3838/.auto-sklearn/ensembles/1.0000000040.ensemble', '/tmp/autosklearn_tmp_131_3838/.auto-sklearn/ensembles/1.0000000041.ensemble', '/tmp/autosklearn_tmp_131_3838/.auto-sklearn/ensembles/1.0000000042.ensemble', '/tmp/autosklearn_tmp_131_3838/.auto-sklearn/ensembles/1.0000000043.ensemble', '/tmp/autosklearn_tmp_131_3838/.auto-sklearn/ensembles/1.0000000044.ensemble', '/tmp/autosklearn_tmp_131_3838/.auto-sklearn/ensembles/1.0000000045.ensemble', '/tmp/autosklearn_tmp_131_3838/.auto-sklearn/ensembles/1.0000000046.ensemble', '/tmp/autosklearn_tmp_131_3838/.auto-sklearn/ensembles/1.0000000047.ensemble', '/tmp/autosklearn_tmp_131_3838/.auto-sklearn/ensembles/1.0000000048.ensemble', '/tmp/autosklearn_tmp_131_3838/.auto-sklearn/ensembles/1.0000000049.ensemble', '/tmp/autosklearn_tmp_131_3838/.auto-sklearn/ensembles/1.0000000050.ensemble', '/tmp/autosklearn_tmp_131_3838/.auto-sklearn/ensembles/1.0000000051.ensemble', '/tmp/autosklearn_tmp_131_3838/.auto-sklearn/ensembles/1.0000000052.ensemble', '/tmp/autosklearn_tmp_131_3838/.auto-sklearn/ensembles/1.0000000053.ensemble', '/tmp/autosklearn_tmp_131_3838/.auto-sklearn/ensembles/1.0000000054.ensemble', '/tmp/autosklearn_tmp_131_3838/.auto-sklearn/ensembles/1.0000000055.ensemble', '/tmp/autosklearn_tmp_131_3838/.auto-sklearn/ensembles/1.0000000056.ensemble', '/tmp/autosklearn_tmp_131_3838/.auto-sklearn/ensembles/1.0000000057.ensemble', '/tmp/autosklearn_tmp_131_3838/.auto-sklearn/ensembles/1.0000000058.ensemble', '/tmp/autosklearn_tmp_131_3838/.auto-sklearn/ensembles/1.0000000059.ensemble', '/tmp/autosklearn_tmp_131_3838/.auto-sklearn/ensembles/1.0000000060.ensemble', '/tmp/autosklearn_tmp_131_3838/.auto-sklearn/ensembles/1.0000000061.ensemble', '/tmp/autosklearn_tmp_131_3838/.auto-sklearn/ensembles/1.0000000062.ensemble', '/tmp/autosklearn_tmp_131_3838/.auto-sklearn/ensembles/1.0000000063.ensemble', '/tmp/autosklearn_tmp_131_3838/.auto-sklearn/ensembles/1.0000000064.ensemble', '/tmp/autosklearn_tmp_131_3838/.auto-sklearn/ensembles/1.0000000065.ensemble', '/tmp/autosklearn_tmp_131_3838/.auto-sklearn/ensembles/1.0000000066.ensemble', '/tmp/autosklearn_tmp_131_3838/.auto-sklearn/ensembles/1.0000000067.ensemble', '/tmp/autosklearn_tmp_131_3838/.auto-sklearn/ensembles/1.0000000068.ensemble', '/tmp/autosklearn_tmp_131_3838/.auto-sklearn/ensembles/1.0000000069.ensemble', '/tmp/autosklearn_tmp_131_3838/.auto-sklearn/ensembles/1.0000000070.ensemble', '/tmp/autosklearn_tmp_131_3838/.auto-sklearn/ensembles/1.0000000071.ensemble', '/tmp/autosklearn_tmp_131_3838/.auto-sklearn/ensembles/1.0000000072.ensemble', '/tmp/autosklearn_tmp_131_3838/.auto-sklearn/ensembles/1.0000000073.ensemble', '/tmp/autosklearn_tmp_131_3838/.auto-sklearn/ensembles/1.0000000074.ensemble', '/tmp/autosklearn_tmp_131_3838/.auto-sklearn/ensembles/1.0000000075.ensemble', '/tmp/autosklearn_tmp_131_3838/.auto-sklearn/ensembles/1.0000000076.ensemble', '/tmp/autosklearn_tmp_131_3838/.auto-sklearn/ensembles/1.0000000077.ensemble', '/tmp/autosklearn_tmp_131_3838/.auto-sklearn/ensembles/1.0000000078.ensemble', '/tmp/autosklearn_tmp_131_3838/.auto-sklearn/ensembles/1.0000000079.ensemble', '/tmp/autosklearn_tmp_131_3838/.auto-sklearn/ensembles/1.0000000080.ensemble', '/tmp/autosklearn_tmp_131_3838/.auto-sklearn/ensembles/1.0000000081.ensemble', '/tmp/autosklearn_tmp_131_3838/.auto-sklearn/ensembles/1.0000000082.ensemble', '/tmp/autosklearn_tmp_131_3838/.auto-sklearn/ensembles/1.0000000083.ensemble', '/tmp/autosklearn_tmp_131_3838/.auto-sklearn/ensembles/1.0000000084.ensemble', '/tmp/autosklearn_tmp_131_3838/.auto-sklearn/ensembles/1.0000000085.ensemble']\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "AutoSklearnClassifier(delete_output_folder_after_terminate=True,\n",
              "           delete_tmp_folder_after_terminate=True,\n",
              "           disable_evaluator_output=False, ensemble_memory_limit=1024,\n",
              "           ensemble_nbest=50, ensemble_size=50, exclude_estimators=None,\n",
              "           exclude_preprocessors=None, get_smac_object_callback=None,\n",
              "           include_estimators=None, include_preprocessors=None,\n",
              "           initial_configurations_via_metalearning=25, logging_config=None,\n",
              "           metadata_directory=None, ml_memory_limit=3072, n_jobs=None,\n",
              "           output_folder=None, per_run_time_limit=360,\n",
              "           resampling_strategy='holdout',\n",
              "           resampling_strategy_arguments=None, seed=1, shared_mode=False,\n",
              "           smac_scenario_args=None, time_left_for_this_task=3600,\n",
              "           tmp_folder=None)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8E7QweLRGp10",
        "colab_type": "code",
        "outputId": "f43c0ca8-bccf-4669-c518-0e8b33fb2aa3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "y_score = cls2.predict_proba(x_test)\n",
        "y_score.shape"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(4030, 2)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pETdybHMG9mH",
        "colab_type": "code",
        "outputId": "4001738d-b53e-4c9c-9d50-424315c8f4e9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "import numpy as np\n",
        "y_score2 = np.argmax(y_score, axis = 1)\n",
        "y_score2"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([1, 1, 1, ..., 0, 0, 0])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nKRtTPi-GlzV",
        "colab_type": "code",
        "outputId": "9318b307-ff24-4cbe-879f-4c8dd5d11cf4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "source": [
        "from sklearn.metrics import accuracy_score, matthews_corrcoef, roc_auc_score\n",
        "print(\"The accuracy is\", accuracy_score(y_test, y_pred))\n",
        "print(\"The MCC is\", matthews_corrcoef(y_test, y_pred))\n",
        "print(\"The AUC-ROC Score is\", roc_auc_score(y_test, y_score2))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The accuracy is 0.9707196029776675\n",
            "The MCC is 0.9415894928700321\n",
            "The AUC-ROC Score is 0.9707196029776676\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OKwp_Lp5It6y",
        "colab_type": "text"
      },
      "source": [
        "# Auto Sklearn on Set 3"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DDCjrnLCYnnn",
        "colab_type": "code",
        "outputId": "1e1adc2d-531c-4a2d-da20-1b82fbd3b0fd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 887
        }
      },
      "source": [
        "!apt-get install swig -y\n",
        "!pip install Cython numpy\n",
        "\n",
        "# sometimes you have to run the next command twice on colab\n",
        "# I haven't figured out why\n",
        "!pip install auto-sklearn"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Reading package lists... Done\n",
            "Building dependency tree       \n",
            "Reading state information... Done\n",
            "swig is already the newest version (3.0.12-1).\n",
            "The following package was automatically installed and is no longer required:\n",
            "  libnvidia-common-410\n",
            "Use 'apt autoremove' to remove it.\n",
            "0 upgraded, 0 newly installed, 0 to remove and 6 not upgraded.\n",
            "Requirement already satisfied: Cython in /usr/local/lib/python3.6/dist-packages (0.29.7)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (1.16.3)\n",
            "Requirement already satisfied: auto-sklearn in /usr/local/lib/python3.6/dist-packages (0.5.2)\n",
            "Requirement already satisfied: scikit-learn<0.20,>=0.19 in /usr/local/lib/python3.6/dist-packages (from auto-sklearn) (0.19.2)\n",
            "Requirement already satisfied: liac-arff in /usr/local/lib/python3.6/dist-packages (from auto-sklearn) (2.4.0)\n",
            "Requirement already satisfied: xgboost>=0.80 in /usr/local/lib/python3.6/dist-packages (from auto-sklearn) (0.90)\n",
            "Requirement already satisfied: numpy>=1.9.0 in /usr/local/lib/python3.6/dist-packages (from auto-sklearn) (1.16.3)\n",
            "Requirement already satisfied: pynisher>=0.4.2 in /usr/local/lib/python3.6/dist-packages (from auto-sklearn) (0.5.0)\n",
            "Requirement already satisfied: scipy>=0.14.1 in /usr/local/lib/python3.6/dist-packages (from auto-sklearn) (1.3.0)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.6/dist-packages (from auto-sklearn) (0.12.5)\n",
            "Requirement already satisfied: ConfigSpace<0.5,>=0.4.0 in /usr/local/lib/python3.6/dist-packages (from auto-sklearn) (0.4.10)\n",
            "Requirement already satisfied: lockfile in /usr/local/lib/python3.6/dist-packages (from auto-sklearn) (0.12.2)\n",
            "Requirement already satisfied: pyrfr<0.8,>=0.7 in /usr/local/lib/python3.6/dist-packages (from auto-sklearn) (0.7.4)\n",
            "Requirement already satisfied: nose in /usr/local/lib/python3.6/dist-packages (from auto-sklearn) (1.3.7)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.6/dist-packages (from auto-sklearn) (5.4.8)\n",
            "Requirement already satisfied: Cython in /usr/local/lib/python3.6/dist-packages (from auto-sklearn) (0.29.7)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.6/dist-packages (from auto-sklearn) (3.13)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.6/dist-packages (from auto-sklearn) (0.24.2)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from auto-sklearn) (41.0.1)\n",
            "Requirement already satisfied: smac==0.8 in /usr/local/lib/python3.6/dist-packages (from auto-sklearn) (0.8.0)\n",
            "Requirement already satisfied: docutils>=0.3 in /usr/local/lib/python3.6/dist-packages (from pynisher>=0.4.2->auto-sklearn) (0.14)\n",
            "Requirement already satisfied: pyparsing in /usr/local/lib/python3.6/dist-packages (from ConfigSpace<0.5,>=0.4.0->auto-sklearn) (2.4.0)\n",
            "Requirement already satisfied: typing in /usr/local/lib/python3.6/dist-packages (from ConfigSpace<0.5,>=0.4.0->auto-sklearn) (3.6.6)\n",
            "Requirement already satisfied: pytz>=2011k in /usr/local/lib/python3.6/dist-packages (from pandas->auto-sklearn) (2018.9)\n",
            "Requirement already satisfied: python-dateutil>=2.5.0 in /usr/local/lib/python3.6/dist-packages (from pandas->auto-sklearn) (2.5.3)\n",
            "Requirement already satisfied: sphinx in /usr/local/lib/python3.6/dist-packages (from smac==0.8->auto-sklearn) (1.8.5)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from smac==0.8->auto-sklearn) (1.12.0)\n",
            "Requirement already satisfied: sphinx-rtd-theme in /usr/local/lib/python3.6/dist-packages (from smac==0.8->auto-sklearn) (0.4.3)\n",
            "Requirement already satisfied: alabaster<0.8,>=0.7 in /usr/local/lib/python3.6/dist-packages (from sphinx->smac==0.8->auto-sklearn) (0.7.12)\n",
            "Requirement already satisfied: babel!=2.0,>=1.3 in /usr/local/lib/python3.6/dist-packages (from sphinx->smac==0.8->auto-sklearn) (2.6.0)\n",
            "Requirement already satisfied: Pygments>=2.0 in /usr/local/lib/python3.6/dist-packages (from sphinx->smac==0.8->auto-sklearn) (2.1.3)\n",
            "Requirement already satisfied: sphinxcontrib-websupport in /usr/local/lib/python3.6/dist-packages (from sphinx->smac==0.8->auto-sklearn) (1.1.2)\n",
            "Requirement already satisfied: imagesize in /usr/local/lib/python3.6/dist-packages (from sphinx->smac==0.8->auto-sklearn) (1.1.0)\n",
            "Requirement already satisfied: Jinja2>=2.3 in /usr/local/lib/python3.6/dist-packages (from sphinx->smac==0.8->auto-sklearn) (2.10.1)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.6/dist-packages (from sphinx->smac==0.8->auto-sklearn) (19.0)\n",
            "Requirement already satisfied: requests>=2.0.0 in /usr/local/lib/python3.6/dist-packages (from sphinx->smac==0.8->auto-sklearn) (2.21.0)\n",
            "Requirement already satisfied: snowballstemmer>=1.1 in /usr/local/lib/python3.6/dist-packages (from sphinx->smac==0.8->auto-sklearn) (1.2.1)\n",
            "Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.6/dist-packages (from Jinja2>=2.3->sphinx->smac==0.8->auto-sklearn) (1.1.1)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests>=2.0.0->sphinx->smac==0.8->auto-sklearn) (2019.3.9)\n",
            "Requirement already satisfied: urllib3<1.25,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests>=2.0.0->sphinx->smac==0.8->auto-sklearn) (1.24.3)\n",
            "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests>=2.0.0->sphinx->smac==0.8->auto-sklearn) (3.0.4)\n",
            "Requirement already satisfied: idna<2.9,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests>=2.0.0->sphinx->smac==0.8->auto-sklearn) (2.8)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J52NBF6WZHEp",
        "colab_type": "code",
        "outputId": "d700491b-0cf6-4e89-b009-4b54b9260a40",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        }
      },
      "source": [
        "import autosklearn.classification"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/ensemble/weight_boosting.py:29: DeprecationWarning: numpy.core.umath_tests is an internal NumPy module and should not be imported. It will be removed in a future NumPy release.\n",
            "  from numpy.core.umath_tests import inner1d\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PnsLKyTBGuzP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "cls3 = autosklearn.classification.AutoSklearnClassifier()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gXmRji_aIxGh",
        "colab_type": "code",
        "outputId": "87886e57-1ad6-4a72-b30b-6bfda4582021",
        "colab": {
          "resources": {
            "http://localhost:8080/nbextensions/google.colab/files.js": {
              "data": "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",
              "ok": true,
              "headers": [
                [
                  "content-type",
                  "application/javascript"
                ]
              ],
              "status": 200,
              "status_text": ""
            }
          },
          "base_uri": "https://localhost:8080/",
          "height": 74
        }
      },
      "source": [
        "from google.colab import files\n",
        "uploaded = files.upload()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-cbda9b53-80bd-4efd-96f6-43e2154b2762\" name=\"files[]\" multiple disabled />\n",
              "     <output id=\"result-cbda9b53-80bd-4efd-96f6-43e2154b2762\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script src=\"/nbextensions/google.colab/files.js\"></script> "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Saving AComp3.csv to AComp3 (2).csv\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q8pHjfgjI4kd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pandas as pd\n",
        "import io\n",
        "df = pd.read_csv(io.BytesIO(uploaded['AComp3.csv']))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7iuA_3RII8Er",
        "colab_type": "code",
        "outputId": "449d0b9e-b832-4a0f-b71d-14230be44385",
        "colab": {
          "resources": {
            "http://localhost:8080/nbextensions/google.colab/files.js": {
              "data": "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",
              "ok": true,
              "headers": [
                [
                  "content-type",
                  "application/javascript"
                ]
              ],
              "status": 200,
              "status_text": ""
            }
          },
          "base_uri": "https://localhost:8080/",
          "height": 74
        }
      },
      "source": [
        "from google.colab import files\n",
        "uploaded = files.upload()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-fd1ba600-4fa4-4768-a983-05e0eeac09c8\" name=\"files[]\" multiple disabled />\n",
              "     <output id=\"result-fd1ba600-4fa4-4768-a983-05e0eeac09c8\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script src=\"/nbextensions/google.colab/files.js\"></script> "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Saving test_AComp3.csv to test_AComp3 (1).csv\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t_LauiCdI9mw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pandas as pd\n",
        "import io\n",
        "df2 = pd.read_csv(io.BytesIO(uploaded['test_AComp3.csv']))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0tJzikVCJJTw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "x_train = df.iloc[:, 0:20].values\n",
        "y_train = df.iloc[:, 20].values\n",
        "x_test = df2.iloc[:, 0:20].values\n",
        "y_test = df2.iloc[:, 20].values"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WpwSDlYkKEiS",
        "colab_type": "code",
        "outputId": "f5343ef6-e910-4ac2-a11f-e7f62a713b30",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 2128
        }
      },
      "source": [
        "cls3.fit(x_train, y_train)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/autosklearn/evaluation/train_evaluator.py:197: RuntimeWarning: Mean of empty slice\n",
            "  Y_train_pred = np.nanmean(Y_train_pred_full, axis=0)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[WARNING] [2019-05-30 20:20:32,604:EnsembleBuilder(1):83852040cc63cc26b780dc20e184806e] No models better than random - using Dummy Score!\n",
            "[WARNING] [2019-05-30 20:20:32,615:EnsembleBuilder(1):83852040cc63cc26b780dc20e184806e] No models better than random - using Dummy Score!\n",
            "[WARNING] [2019-05-30 20:20:34,619:EnsembleBuilder(1):83852040cc63cc26b780dc20e184806e] No models better than random - using Dummy Score!\n",
            "[WARNING] [2019-05-30 20:20:36,624:EnsembleBuilder(1):83852040cc63cc26b780dc20e184806e] No models better than random - using Dummy Score!\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/autosklearn/evaluation/train_evaluator.py:197: RuntimeWarning: Mean of empty slice\n",
            "  Y_train_pred = np.nanmean(Y_train_pred_full, axis=0)\n",
            "/usr/local/lib/python3.6/dist-packages/autosklearn/evaluation/train_evaluator.py:197: RuntimeWarning: Mean of empty slice\n",
            "  Y_train_pred = np.nanmean(Y_train_pred_full, axis=0)\n",
            "/usr/local/lib/python3.6/dist-packages/autosklearn/evaluation/train_evaluator.py:197: RuntimeWarning: Mean of empty slice\n",
            "  Y_train_pred = np.nanmean(Y_train_pred_full, axis=0)\n",
            "/usr/local/lib/python3.6/dist-packages/autosklearn/evaluation/train_evaluator.py:197: RuntimeWarning: Mean of empty slice\n",
            "  Y_train_pred = np.nanmean(Y_train_pred_full, axis=0)\n",
            "/usr/local/lib/python3.6/dist-packages/autosklearn/evaluation/train_evaluator.py:197: RuntimeWarning: Mean of empty slice\n",
            "  Y_train_pred = np.nanmean(Y_train_pred_full, axis=0)\n",
            "/usr/local/lib/python3.6/dist-packages/autosklearn/evaluation/train_evaluator.py:197: RuntimeWarning: Mean of empty slice\n",
            "  Y_train_pred = np.nanmean(Y_train_pred_full, axis=0)\n",
            "/usr/local/lib/python3.6/dist-packages/autosklearn/evaluation/train_evaluator.py:197: RuntimeWarning: Mean of empty slice\n",
            "  Y_train_pred = np.nanmean(Y_train_pred_full, axis=0)\n",
            "/usr/local/lib/python3.6/dist-packages/autosklearn/evaluation/train_evaluator.py:197: RuntimeWarning: Mean of empty slice\n",
            "  Y_train_pred = np.nanmean(Y_train_pred_full, axis=0)\n",
            "/usr/local/lib/python3.6/dist-packages/autosklearn/evaluation/train_evaluator.py:197: RuntimeWarning: Mean of empty slice\n",
            "  Y_train_pred = np.nanmean(Y_train_pred_full, axis=0)\n",
            "/usr/local/lib/python3.6/dist-packages/autosklearn/evaluation/train_evaluator.py:197: RuntimeWarning: Mean of empty slice\n",
            "  Y_train_pred = np.nanmean(Y_train_pred_full, axis=0)\n",
            "/usr/local/lib/python3.6/dist-packages/autosklearn/evaluation/train_evaluator.py:197: RuntimeWarning: Mean of empty slice\n",
            "  Y_train_pred = np.nanmean(Y_train_pred_full, axis=0)\n",
            "/usr/local/lib/python3.6/dist-packages/autosklearn/evaluation/train_evaluator.py:197: RuntimeWarning: Mean of empty slice\n",
            "  Y_train_pred = np.nanmean(Y_train_pred_full, axis=0)\n",
            "/usr/local/lib/python3.6/dist-packages/autosklearn/evaluation/train_evaluator.py:197: RuntimeWarning: Mean of empty slice\n",
            "  Y_train_pred = np.nanmean(Y_train_pred_full, axis=0)\n",
            "/usr/local/lib/python3.6/dist-packages/autosklearn/evaluation/train_evaluator.py:197: RuntimeWarning: Mean of empty slice\n",
            "  Y_train_pred = np.nanmean(Y_train_pred_full, axis=0)\n",
            "/usr/local/lib/python3.6/dist-packages/autosklearn/evaluation/train_evaluator.py:197: RuntimeWarning: Mean of empty slice\n",
            "  Y_train_pred = np.nanmean(Y_train_pred_full, axis=0)\n",
            "/usr/local/lib/python3.6/dist-packages/autosklearn/evaluation/train_evaluator.py:197: RuntimeWarning: Mean of empty slice\n",
            "  Y_train_pred = np.nanmean(Y_train_pred_full, axis=0)\n",
            "/usr/local/lib/python3.6/dist-packages/autosklearn/evaluation/train_evaluator.py:197: RuntimeWarning: Mean of empty slice\n",
            "  Y_train_pred = np.nanmean(Y_train_pred_full, axis=0)\n",
            "/usr/local/lib/python3.6/dist-packages/autosklearn/evaluation/train_evaluator.py:197: RuntimeWarning: Mean of empty slice\n",
            "  Y_train_pred = np.nanmean(Y_train_pred_full, axis=0)\n",
            "/usr/local/lib/python3.6/dist-packages/autosklearn/evaluation/train_evaluator.py:197: RuntimeWarning: Mean of empty slice\n",
            "  Y_train_pred = np.nanmean(Y_train_pred_full, axis=0)\n",
            "/usr/local/lib/python3.6/dist-packages/autosklearn/evaluation/train_evaluator.py:197: RuntimeWarning: Mean of empty slice\n",
            "  Y_train_pred = np.nanmean(Y_train_pred_full, axis=0)\n",
            "/usr/local/lib/python3.6/dist-packages/autosklearn/evaluation/train_evaluator.py:197: RuntimeWarning: Mean of empty slice\n",
            "  Y_train_pred = np.nanmean(Y_train_pred_full, axis=0)\n",
            "/usr/local/lib/python3.6/dist-packages/autosklearn/evaluation/train_evaluator.py:197: RuntimeWarning: Mean of empty slice\n",
            "  Y_train_pred = np.nanmean(Y_train_pred_full, axis=0)\n",
            "/usr/local/lib/python3.6/dist-packages/autosklearn/evaluation/train_evaluator.py:197: RuntimeWarning: Mean of empty slice\n",
            "  Y_train_pred = np.nanmean(Y_train_pred_full, axis=0)\n",
            "/usr/local/lib/python3.6/dist-packages/autosklearn/evaluation/train_evaluator.py:197: RuntimeWarning: Mean of empty slice\n",
            "  Y_train_pred = np.nanmean(Y_train_pred_full, axis=0)\n",
            "/usr/local/lib/python3.6/dist-packages/autosklearn/evaluation/train_evaluator.py:197: RuntimeWarning: Mean of empty slice\n",
            "  Y_train_pred = np.nanmean(Y_train_pred_full, axis=0)\n",
            "/usr/local/lib/python3.6/dist-packages/autosklearn/evaluation/train_evaluator.py:197: RuntimeWarning: Mean of empty slice\n",
            "  Y_train_pred = np.nanmean(Y_train_pred_full, axis=0)\n",
            "/usr/local/lib/python3.6/dist-packages/autosklearn/evaluation/train_evaluator.py:197: RuntimeWarning: Mean of empty slice\n",
            "  Y_train_pred = np.nanmean(Y_train_pred_full, axis=0)\n",
            "/usr/local/lib/python3.6/dist-packages/autosklearn/evaluation/train_evaluator.py:197: RuntimeWarning: Mean of empty slice\n",
            "  Y_train_pred = np.nanmean(Y_train_pred_full, axis=0)\n",
            "/usr/local/lib/python3.6/dist-packages/autosklearn/evaluation/train_evaluator.py:197: RuntimeWarning: Mean of empty slice\n",
            "  Y_train_pred = np.nanmean(Y_train_pred_full, axis=0)\n",
            "/usr/local/lib/python3.6/dist-packages/autosklearn/evaluation/train_evaluator.py:197: RuntimeWarning: Mean of empty slice\n",
            "  Y_train_pred = np.nanmean(Y_train_pred_full, axis=0)\n",
            "/usr/local/lib/python3.6/dist-packages/autosklearn/evaluation/train_evaluator.py:197: RuntimeWarning: Mean of empty slice\n",
            "  Y_train_pred = np.nanmean(Y_train_pred_full, axis=0)\n",
            "/usr/local/lib/python3.6/dist-packages/autosklearn/evaluation/train_evaluator.py:197: RuntimeWarning: Mean of empty slice\n",
            "  Y_train_pred = np.nanmean(Y_train_pred_full, axis=0)\n",
            "/usr/local/lib/python3.6/dist-packages/autosklearn/evaluation/train_evaluator.py:197: RuntimeWarning: Mean of empty slice\n",
            "  Y_train_pred = np.nanmean(Y_train_pred_full, axis=0)\n",
            "/usr/local/lib/python3.6/dist-packages/autosklearn/evaluation/train_evaluator.py:197: RuntimeWarning: Mean of empty slice\n",
            "  Y_train_pred = np.nanmean(Y_train_pred_full, axis=0)\n",
            "/usr/local/lib/python3.6/dist-packages/autosklearn/evaluation/train_evaluator.py:197: RuntimeWarning: Mean of empty slice\n",
            "  Y_train_pred = np.nanmean(Y_train_pred_full, axis=0)\n",
            "/usr/local/lib/python3.6/dist-packages/autosklearn/evaluation/train_evaluator.py:197: RuntimeWarning: Mean of empty slice\n",
            "  Y_train_pred = np.nanmean(Y_train_pred_full, axis=0)\n",
            "/usr/local/lib/python3.6/dist-packages/autosklearn/evaluation/train_evaluator.py:197: RuntimeWarning: Mean of empty slice\n",
            "  Y_train_pred = np.nanmean(Y_train_pred_full, axis=0)\n",
            "/usr/local/lib/python3.6/dist-packages/autosklearn/evaluation/train_evaluator.py:197: RuntimeWarning: Mean of empty slice\n",
            "  Y_train_pred = np.nanmean(Y_train_pred_full, axis=0)\n",
            "/usr/local/lib/python3.6/dist-packages/autosklearn/evaluation/train_evaluator.py:197: RuntimeWarning: Mean of empty slice\n",
            "  Y_train_pred = np.nanmean(Y_train_pred_full, axis=0)\n",
            "/usr/local/lib/python3.6/dist-packages/autosklearn/evaluation/train_evaluator.py:197: RuntimeWarning: Mean of empty slice\n",
            "  Y_train_pred = np.nanmean(Y_train_pred_full, axis=0)\n",
            "/usr/local/lib/python3.6/dist-packages/autosklearn/evaluation/train_evaluator.py:197: RuntimeWarning: Mean of empty slice\n",
            "  Y_train_pred = np.nanmean(Y_train_pred_full, axis=0)\n",
            "/usr/local/lib/python3.6/dist-packages/autosklearn/evaluation/train_evaluator.py:197: RuntimeWarning: Mean of empty slice\n",
            "  Y_train_pred = np.nanmean(Y_train_pred_full, axis=0)\n",
            "/usr/local/lib/python3.6/dist-packages/autosklearn/evaluation/train_evaluator.py:197: RuntimeWarning: Mean of empty slice\n",
            "  Y_train_pred = np.nanmean(Y_train_pred_full, axis=0)\n",
            "/usr/local/lib/python3.6/dist-packages/autosklearn/evaluation/train_evaluator.py:197: RuntimeWarning: Mean of empty slice\n",
            "  Y_train_pred = np.nanmean(Y_train_pred_full, axis=0)\n",
            "/usr/local/lib/python3.6/dist-packages/autosklearn/evaluation/train_evaluator.py:197: RuntimeWarning: Mean of empty slice\n",
            "  Y_train_pred = np.nanmean(Y_train_pred_full, axis=0)\n",
            "/usr/local/lib/python3.6/dist-packages/autosklearn/evaluation/train_evaluator.py:197: RuntimeWarning: Mean of empty slice\n",
            "  Y_train_pred = np.nanmean(Y_train_pred_full, axis=0)\n",
            "/usr/local/lib/python3.6/dist-packages/autosklearn/evaluation/train_evaluator.py:197: RuntimeWarning: Mean of empty slice\n",
            "  Y_train_pred = np.nanmean(Y_train_pred_full, axis=0)\n",
            "/usr/local/lib/python3.6/dist-packages/autosklearn/evaluation/train_evaluator.py:197: RuntimeWarning: Mean of empty slice\n",
            "  Y_train_pred = np.nanmean(Y_train_pred_full, axis=0)\n",
            "/usr/local/lib/python3.6/dist-packages/autosklearn/evaluation/train_evaluator.py:197: RuntimeWarning: Mean of empty slice\n",
            "  Y_train_pred = np.nanmean(Y_train_pred_full, axis=0)\n",
            "/usr/local/lib/python3.6/dist-packages/autosklearn/evaluation/train_evaluator.py:197: RuntimeWarning: Mean of empty slice\n",
            "  Y_train_pred = np.nanmean(Y_train_pred_full, axis=0)\n",
            "/usr/local/lib/python3.6/dist-packages/autosklearn/evaluation/train_evaluator.py:197: RuntimeWarning: Mean of empty slice\n",
            "  Y_train_pred = np.nanmean(Y_train_pred_full, axis=0)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "1\n",
            "['/tmp/autosklearn_tmp_2821_3933/.auto-sklearn/ensembles/1.0000000000.ensemble', '/tmp/autosklearn_tmp_2821_3933/.auto-sklearn/ensembles/1.0000000001.ensemble', '/tmp/autosklearn_tmp_2821_3933/.auto-sklearn/ensembles/1.0000000002.ensemble', '/tmp/autosklearn_tmp_2821_3933/.auto-sklearn/ensembles/1.0000000003.ensemble', '/tmp/autosklearn_tmp_2821_3933/.auto-sklearn/ensembles/1.0000000004.ensemble', '/tmp/autosklearn_tmp_2821_3933/.auto-sklearn/ensembles/1.0000000005.ensemble', '/tmp/autosklearn_tmp_2821_3933/.auto-sklearn/ensembles/1.0000000006.ensemble', '/tmp/autosklearn_tmp_2821_3933/.auto-sklearn/ensembles/1.0000000007.ensemble', '/tmp/autosklearn_tmp_2821_3933/.auto-sklearn/ensembles/1.0000000008.ensemble', '/tmp/autosklearn_tmp_2821_3933/.auto-sklearn/ensembles/1.0000000009.ensemble', '/tmp/autosklearn_tmp_2821_3933/.auto-sklearn/ensembles/1.0000000010.ensemble', '/tmp/autosklearn_tmp_2821_3933/.auto-sklearn/ensembles/1.0000000011.ensemble', '/tmp/autosklearn_tmp_2821_3933/.auto-sklearn/ensembles/1.0000000012.ensemble', '/tmp/autosklearn_tmp_2821_3933/.auto-sklearn/ensembles/1.0000000013.ensemble', '/tmp/autosklearn_tmp_2821_3933/.auto-sklearn/ensembles/1.0000000014.ensemble', '/tmp/autosklearn_tmp_2821_3933/.auto-sklearn/ensembles/1.0000000015.ensemble', '/tmp/autosklearn_tmp_2821_3933/.auto-sklearn/ensembles/1.0000000016.ensemble', '/tmp/autosklearn_tmp_2821_3933/.auto-sklearn/ensembles/1.0000000017.ensemble', '/tmp/autosklearn_tmp_2821_3933/.auto-sklearn/ensembles/1.0000000018.ensemble', '/tmp/autosklearn_tmp_2821_3933/.auto-sklearn/ensembles/1.0000000019.ensemble', '/tmp/autosklearn_tmp_2821_3933/.auto-sklearn/ensembles/1.0000000020.ensemble', '/tmp/autosklearn_tmp_2821_3933/.auto-sklearn/ensembles/1.0000000021.ensemble', '/tmp/autosklearn_tmp_2821_3933/.auto-sklearn/ensembles/1.0000000022.ensemble', '/tmp/autosklearn_tmp_2821_3933/.auto-sklearn/ensembles/1.0000000023.ensemble', '/tmp/autosklearn_tmp_2821_3933/.auto-sklearn/ensembles/1.0000000024.ensemble', '/tmp/autosklearn_tmp_2821_3933/.auto-sklearn/ensembles/1.0000000025.ensemble', '/tmp/autosklearn_tmp_2821_3933/.auto-sklearn/ensembles/1.0000000026.ensemble', '/tmp/autosklearn_tmp_2821_3933/.auto-sklearn/ensembles/1.0000000027.ensemble', '/tmp/autosklearn_tmp_2821_3933/.auto-sklearn/ensembles/1.0000000028.ensemble', '/tmp/autosklearn_tmp_2821_3933/.auto-sklearn/ensembles/1.0000000029.ensemble', '/tmp/autosklearn_tmp_2821_3933/.auto-sklearn/ensembles/1.0000000030.ensemble', '/tmp/autosklearn_tmp_2821_3933/.auto-sklearn/ensembles/1.0000000031.ensemble', '/tmp/autosklearn_tmp_2821_3933/.auto-sklearn/ensembles/1.0000000032.ensemble', '/tmp/autosklearn_tmp_2821_3933/.auto-sklearn/ensembles/1.0000000033.ensemble', '/tmp/autosklearn_tmp_2821_3933/.auto-sklearn/ensembles/1.0000000034.ensemble', '/tmp/autosklearn_tmp_2821_3933/.auto-sklearn/ensembles/1.0000000035.ensemble', '/tmp/autosklearn_tmp_2821_3933/.auto-sklearn/ensembles/1.0000000036.ensemble', '/tmp/autosklearn_tmp_2821_3933/.auto-sklearn/ensembles/1.0000000037.ensemble', '/tmp/autosklearn_tmp_2821_3933/.auto-sklearn/ensembles/1.0000000038.ensemble', '/tmp/autosklearn_tmp_2821_3933/.auto-sklearn/ensembles/1.0000000039.ensemble', '/tmp/autosklearn_tmp_2821_3933/.auto-sklearn/ensembles/1.0000000040.ensemble', '/tmp/autosklearn_tmp_2821_3933/.auto-sklearn/ensembles/1.0000000041.ensemble', '/tmp/autosklearn_tmp_2821_3933/.auto-sklearn/ensembles/1.0000000042.ensemble', '/tmp/autosklearn_tmp_2821_3933/.auto-sklearn/ensembles/1.0000000043.ensemble', '/tmp/autosklearn_tmp_2821_3933/.auto-sklearn/ensembles/1.0000000044.ensemble', '/tmp/autosklearn_tmp_2821_3933/.auto-sklearn/ensembles/1.0000000045.ensemble', '/tmp/autosklearn_tmp_2821_3933/.auto-sklearn/ensembles/1.0000000046.ensemble']\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "AutoSklearnClassifier(delete_output_folder_after_terminate=True,\n",
              "           delete_tmp_folder_after_terminate=True,\n",
              "           disable_evaluator_output=False, ensemble_memory_limit=1024,\n",
              "           ensemble_nbest=50, ensemble_size=50, exclude_estimators=None,\n",
              "           exclude_preprocessors=None, get_smac_object_callback=None,\n",
              "           include_estimators=None, include_preprocessors=None,\n",
              "           initial_configurations_via_metalearning=25, logging_config=None,\n",
              "           metadata_directory=None, ml_memory_limit=3072, n_jobs=None,\n",
              "           output_folder=None, per_run_time_limit=360,\n",
              "           resampling_strategy='holdout',\n",
              "           resampling_strategy_arguments=None, seed=1, shared_mode=False,\n",
              "           smac_scenario_args=None, time_left_for_this_task=3600,\n",
              "           tmp_folder=None)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9mR_HQtWKR23",
        "colab_type": "code",
        "outputId": "89557da8-244d-4028-a3a0-6e4c3745abf4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "y_score = cls3.predict_proba(x_test)\n",
        "y_score.shape"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(4030, 2)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "65h1mJG3K9qr",
        "colab_type": "code",
        "outputId": "a40325f8-0eeb-4d36-d9da-0f4fd11cad85",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "import numpy as np\n",
        "y_score3 = np.argmax(y_score, axis = 1)\n",
        "y_score3"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([1, 1, 1, ..., 0, 0, 0])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NWQ-QzJWLEZv",
        "colab_type": "code",
        "outputId": "0c7e5729-46e7-4a64-dd36-38938e654811",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "y_pred3 = cls3.predict(x_test)\n",
        "y_pred3"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([1, 1, 1, ..., 0, 0, 0])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PmdabvCsLNgr",
        "colab_type": "code",
        "outputId": "8d9321c4-22b6-432d-d0fb-1af78555d73e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "source": [
        "from sklearn.metrics import accuracy_score, matthews_corrcoef, roc_auc_score\n",
        "print(\"The accuracy is\", accuracy_score(y_test, y_pred3))\n",
        "print(\"The MCC is\", matthews_corrcoef(y_test, y_pred3))\n",
        "print(\"The AUC-ROC Score is\", roc_auc_score(y_test, y_score3))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The accuracy is 0.9719602977667494\n",
            "The MCC is 0.9441998114631105\n",
            "The AUC-ROC Score is 0.9719602977667494\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VIpMe-W9whKG",
        "colab_type": "text"
      },
      "source": [
        "# Auto Sklearn on Set 4"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V7ns19lCZ7CO",
        "colab_type": "code",
        "outputId": "6ca9d2d0-c85a-4524-dec5-8335901cb582",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 887
        }
      },
      "source": [
        "!apt-get install swig -y\n",
        "!pip install Cython numpy\n",
        "\n",
        "# sometimes you have to run the next command twice on colab\n",
        "# I haven't figured out why\n",
        "!pip install auto-sklearn"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\rReading package lists... 0%\r\rReading package lists... 0%\r\rReading package lists... 0%\r\rReading package lists... 7%\r\rReading package lists... 7%\r\rReading package lists... 8%\r\rReading package lists... 8%\r\rReading package lists... 72%\r\rReading package lists... 72%\r\rReading package lists... 73%\r\rReading package lists... 73%\r\rReading package lists... 79%\r\rReading package lists... 79%\r\rReading package lists... 79%\r\rReading package lists... 79%\r\rReading package lists... 85%\r\rReading package lists... 88%\r\rReading package lists... 88%\r\rReading package lists... 88%\r\rReading package lists... 88%\r\rReading package lists... 88%\r\rReading package lists... 88%\r\rReading package lists... 88%\r\rReading package lists... 88%\r\rReading package lists... 91%\r\rReading package lists... 91%\r\rReading package lists... 91%\r\rReading package lists... 91%\r\rReading package lists... 93%\r\rReading package lists... 93%\r\rReading package lists... 93%\r\rReading package lists... 93%\r\rReading package lists... 94%\r\rReading package lists... 94%\r\rReading package lists... 94%\r\rReading package lists... 94%\r\rReading package lists... 94%\r\rReading package lists... 94%\r\rReading package lists... 98%\r\rReading package lists... 98%\r\rReading package lists... 98%\r\rReading package lists... 98%\r\rReading package lists... Done\r\n",
            "\rBuilding dependency tree... 0%\r\rBuilding dependency tree... 0%\r\rBuilding dependency tree... 50%\r\rBuilding dependency tree... 50%\r\rBuilding dependency tree       \r\n",
            "\rReading state information... 0%\r\rReading state information... 0%\r\rReading state information... Done\r\n",
            "swig is already the newest version (3.0.12-1).\n",
            "The following package was automatically installed and is no longer required:\n",
            "  libnvidia-common-410\n",
            "Use 'apt autoremove' to remove it.\n",
            "0 upgraded, 0 newly installed, 0 to remove and 6 not upgraded.\n",
            "Requirement already satisfied: Cython in /usr/local/lib/python3.6/dist-packages (0.29.7)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (1.16.3)\n",
            "Requirement already satisfied: auto-sklearn in /usr/local/lib/python3.6/dist-packages (0.5.2)\n",
            "Requirement already satisfied: smac==0.8 in /usr/local/lib/python3.6/dist-packages (from auto-sklearn) (0.8.0)\n",
            "Requirement already satisfied: nose in /usr/local/lib/python3.6/dist-packages (from auto-sklearn) (1.3.7)\n",
            "Requirement already satisfied: xgboost>=0.80 in /usr/local/lib/python3.6/dist-packages (from auto-sklearn) (0.90)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from auto-sklearn) (41.0.1)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.6/dist-packages (from auto-sklearn) (5.4.8)\n",
            "Requirement already satisfied: pyrfr<0.8,>=0.7 in /usr/local/lib/python3.6/dist-packages (from auto-sklearn) (0.7.4)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.6/dist-packages (from auto-sklearn) (3.13)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.6/dist-packages (from auto-sklearn) (0.12.5)\n",
            "Requirement already satisfied: liac-arff in /usr/local/lib/python3.6/dist-packages (from auto-sklearn) (2.4.0)\n",
            "Requirement already satisfied: scikit-learn<0.20,>=0.19 in /usr/local/lib/python3.6/dist-packages (from auto-sklearn) (0.19.2)\n",
            "Requirement already satisfied: numpy>=1.9.0 in /usr/local/lib/python3.6/dist-packages (from auto-sklearn) (1.16.3)\n",
            "Requirement already satisfied: ConfigSpace<0.5,>=0.4.0 in /usr/local/lib/python3.6/dist-packages (from auto-sklearn) (0.4.10)\n",
            "Requirement already satisfied: lockfile in /usr/local/lib/python3.6/dist-packages (from auto-sklearn) (0.12.2)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.6/dist-packages (from auto-sklearn) (0.24.2)\n",
            "Requirement already satisfied: pynisher>=0.4.2 in /usr/local/lib/python3.6/dist-packages (from auto-sklearn) (0.5.0)\n",
            "Requirement already satisfied: scipy>=0.14.1 in /usr/local/lib/python3.6/dist-packages (from auto-sklearn) (1.3.0)\n",
            "Requirement already satisfied: Cython in /usr/local/lib/python3.6/dist-packages (from auto-sklearn) (0.29.7)\n",
            "Requirement already satisfied: sphinx-rtd-theme in /usr/local/lib/python3.6/dist-packages (from smac==0.8->auto-sklearn) (0.4.3)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from smac==0.8->auto-sklearn) (1.12.0)\n",
            "Requirement already satisfied: sphinx in /usr/local/lib/python3.6/dist-packages (from smac==0.8->auto-sklearn) (1.8.5)\n",
            "Requirement already satisfied: typing in /usr/local/lib/python3.6/dist-packages (from smac==0.8->auto-sklearn) (3.6.6)\n",
            "Requirement already satisfied: pyparsing in /usr/local/lib/python3.6/dist-packages (from ConfigSpace<0.5,>=0.4.0->auto-sklearn) (2.4.0)\n",
            "Requirement already satisfied: pytz>=2011k in /usr/local/lib/python3.6/dist-packages (from pandas->auto-sklearn) (2018.9)\n",
            "Requirement already satisfied: python-dateutil>=2.5.0 in /usr/local/lib/python3.6/dist-packages (from pandas->auto-sklearn) (2.5.3)\n",
            "Requirement already satisfied: docutils>=0.3 in /usr/local/lib/python3.6/dist-packages (from pynisher>=0.4.2->auto-sklearn) (0.14)\n",
            "Requirement already satisfied: requests>=2.0.0 in /usr/local/lib/python3.6/dist-packages (from sphinx->smac==0.8->auto-sklearn) (2.21.0)\n",
            "Requirement already satisfied: snowballstemmer>=1.1 in /usr/local/lib/python3.6/dist-packages (from sphinx->smac==0.8->auto-sklearn) (1.2.1)\n",
            "Requirement already satisfied: Pygments>=2.0 in /usr/local/lib/python3.6/dist-packages (from sphinx->smac==0.8->auto-sklearn) (2.1.3)\n",
            "Requirement already satisfied: alabaster<0.8,>=0.7 in /usr/local/lib/python3.6/dist-packages (from sphinx->smac==0.8->auto-sklearn) (0.7.12)\n",
            "Requirement already satisfied: sphinxcontrib-websupport in /usr/local/lib/python3.6/dist-packages (from sphinx->smac==0.8->auto-sklearn) (1.1.2)\n",
            "Requirement already satisfied: Jinja2>=2.3 in /usr/local/lib/python3.6/dist-packages (from sphinx->smac==0.8->auto-sklearn) (2.10.1)\n",
            "Requirement already satisfied: imagesize in /usr/local/lib/python3.6/dist-packages (from sphinx->smac==0.8->auto-sklearn) (1.1.0)\n",
            "Requirement already satisfied: babel!=2.0,>=1.3 in /usr/local/lib/python3.6/dist-packages (from sphinx->smac==0.8->auto-sklearn) (2.6.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.6/dist-packages (from sphinx->smac==0.8->auto-sklearn) (19.0)\n",
            "Requirement already satisfied: idna<2.9,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests>=2.0.0->sphinx->smac==0.8->auto-sklearn) (2.8)\n",
            "Requirement already satisfied: urllib3<1.25,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests>=2.0.0->sphinx->smac==0.8->auto-sklearn) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests>=2.0.0->sphinx->smac==0.8->auto-sklearn) (2019.3.9)\n",
            "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests>=2.0.0->sphinx->smac==0.8->auto-sklearn) (3.0.4)\n",
            "Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.6/dist-packages (from Jinja2>=2.3->sphinx->smac==0.8->auto-sklearn) (1.1.1)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v0ShcCZSwrIU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import autosklearn.classification\n",
        "cls4 = autosklearn.classification.AutoSklearnClassifier()\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bELJ6s4vw4JU",
        "colab_type": "code",
        "outputId": "e7e46104-2205-4fb7-d560-9198051bd069",
        "colab": {
          "resources": {
            "http://localhost:8080/nbextensions/google.colab/files.js": {
              "data": "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",
              "ok": true,
              "headers": [
                [
                  "content-type",
                  "application/javascript"
                ]
              ],
              "status": 200,
              "status_text": ""
            }
          },
          "base_uri": "https://localhost:8080/",
          "height": 74
        }
      },
      "source": [
        "from google.colab import files\n",
        "uploaded = files.upload()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-5b35dcdd-f014-4362-b25e-bfb0e9b1888f\" name=\"files[]\" multiple disabled />\n",
              "     <output id=\"result-5b35dcdd-f014-4362-b25e-bfb0e9b1888f\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script src=\"/nbextensions/google.colab/files.js\"></script> "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Saving AComp4.csv to AComp4.csv\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sN02FJ_3xDPy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pandas as pd\n",
        "import io\n",
        "df = pd.read_csv(io.BytesIO(uploaded['AComp4.csv']))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pzeJV4PaxIV0",
        "colab_type": "code",
        "outputId": "29d0e8fd-ddad-49bc-de8a-fdc62b6edf20",
        "colab": {
          "resources": {
            "http://localhost:8080/nbextensions/google.colab/files.js": {
              "data": "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",
              "ok": true,
              "headers": [
                [
                  "content-type",
                  "application/javascript"
                ]
              ],
              "status": 200,
              "status_text": ""
            }
          },
          "base_uri": "https://localhost:8080/",
          "height": 74
        }
      },
      "source": [
        "from google.colab import files\n",
        "uploaded = files.upload()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-8840dab9-4c6a-4886-ae64-40eba92e56e7\" name=\"files[]\" multiple disabled />\n",
              "     <output id=\"result-8840dab9-4c6a-4886-ae64-40eba92e56e7\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script src=\"/nbextensions/google.colab/files.js\"></script> "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Saving test_AComp4.csv to test_AComp4.csv\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JpxZuidCxJnm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pandas as pd\n",
        "import io\n",
        "df2 = pd.read_csv(io.BytesIO(uploaded['test_AComp4.csv']))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WatoBBljxSdA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "x_train = df.iloc[:, 0:20].values\n",
        "y_train = df.iloc[:, 20].values\n",
        "x_test = df2.iloc[:, 0:20].values\n",
        "y_test = df2.iloc[:, 20].values"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mehNy9NzxWzN",
        "colab_type": "code",
        "outputId": "8b2b599f-9576-43da-f1dd-cef3e2e008dc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 2128
        }
      },
      "source": [
        "cls4.fit(x_train, y_train)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/autosklearn/evaluation/train_evaluator.py:197: RuntimeWarning: Mean of empty slice\n",
            "  Y_train_pred = np.nanmean(Y_train_pred_full, axis=0)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[WARNING] [2019-05-30 22:06:33,372:EnsembleBuilder(1):965d4bab2c291e42aa3377f55ecd87f0] No models better than random - using Dummy Score!\n",
            "[WARNING] [2019-05-30 22:06:33,388:EnsembleBuilder(1):965d4bab2c291e42aa3377f55ecd87f0] No models better than random - using Dummy Score!\n",
            "[WARNING] [2019-05-30 22:06:35,393:EnsembleBuilder(1):965d4bab2c291e42aa3377f55ecd87f0] No models better than random - using Dummy Score!\n",
            "[WARNING] [2019-05-30 22:06:37,398:EnsembleBuilder(1):965d4bab2c291e42aa3377f55ecd87f0] No models better than random - using Dummy Score!\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/autosklearn/evaluation/train_evaluator.py:197: RuntimeWarning: Mean of empty slice\n",
            "  Y_train_pred = np.nanmean(Y_train_pred_full, axis=0)\n",
            "/usr/local/lib/python3.6/dist-packages/autosklearn/evaluation/train_evaluator.py:197: RuntimeWarning: Mean of empty slice\n",
            "  Y_train_pred = np.nanmean(Y_train_pred_full, axis=0)\n",
            "/usr/local/lib/python3.6/dist-packages/autosklearn/evaluation/train_evaluator.py:197: RuntimeWarning: Mean of empty slice\n",
            "  Y_train_pred = np.nanmean(Y_train_pred_full, axis=0)\n",
            "/usr/local/lib/python3.6/dist-packages/autosklearn/evaluation/train_evaluator.py:197: RuntimeWarning: Mean of empty slice\n",
            "  Y_train_pred = np.nanmean(Y_train_pred_full, axis=0)\n",
            "/usr/local/lib/python3.6/dist-packages/autosklearn/evaluation/train_evaluator.py:197: RuntimeWarning: Mean of empty slice\n",
            "  Y_train_pred = np.nanmean(Y_train_pred_full, axis=0)\n",
            "/usr/local/lib/python3.6/dist-packages/autosklearn/evaluation/train_evaluator.py:197: RuntimeWarning: Mean of empty slice\n",
            "  Y_train_pred = np.nanmean(Y_train_pred_full, axis=0)\n",
            "/usr/local/lib/python3.6/dist-packages/autosklearn/evaluation/train_evaluator.py:197: RuntimeWarning: Mean of empty slice\n",
            "  Y_train_pred = np.nanmean(Y_train_pred_full, axis=0)\n",
            "/usr/local/lib/python3.6/dist-packages/autosklearn/evaluation/train_evaluator.py:197: RuntimeWarning: Mean of empty slice\n",
            "  Y_train_pred = np.nanmean(Y_train_pred_full, axis=0)\n",
            "/usr/local/lib/python3.6/dist-packages/autosklearn/evaluation/train_evaluator.py:197: RuntimeWarning: Mean of empty slice\n",
            "  Y_train_pred = np.nanmean(Y_train_pred_full, axis=0)\n",
            "/usr/local/lib/python3.6/dist-packages/autosklearn/evaluation/train_evaluator.py:197: RuntimeWarning: Mean of empty slice\n",
            "  Y_train_pred = np.nanmean(Y_train_pred_full, axis=0)\n",
            "/usr/local/lib/python3.6/dist-packages/autosklearn/evaluation/train_evaluator.py:197: RuntimeWarning: Mean of empty slice\n",
            "  Y_train_pred = np.nanmean(Y_train_pred_full, axis=0)\n",
            "/usr/local/lib/python3.6/dist-packages/autosklearn/evaluation/train_evaluator.py:197: RuntimeWarning: Mean of empty slice\n",
            "  Y_train_pred = np.nanmean(Y_train_pred_full, axis=0)\n",
            "/usr/local/lib/python3.6/dist-packages/autosklearn/evaluation/train_evaluator.py:197: RuntimeWarning: Mean of empty slice\n",
            "  Y_train_pred = np.nanmean(Y_train_pred_full, axis=0)\n",
            "/usr/local/lib/python3.6/dist-packages/autosklearn/evaluation/train_evaluator.py:197: RuntimeWarning: Mean of empty slice\n",
            "  Y_train_pred = np.nanmean(Y_train_pred_full, axis=0)\n",
            "/usr/local/lib/python3.6/dist-packages/autosklearn/evaluation/train_evaluator.py:197: RuntimeWarning: Mean of empty slice\n",
            "  Y_train_pred = np.nanmean(Y_train_pred_full, axis=0)\n",
            "/usr/local/lib/python3.6/dist-packages/autosklearn/evaluation/train_evaluator.py:197: RuntimeWarning: Mean of empty slice\n",
            "  Y_train_pred = np.nanmean(Y_train_pred_full, axis=0)\n",
            "/usr/local/lib/python3.6/dist-packages/autosklearn/evaluation/train_evaluator.py:197: RuntimeWarning: Mean of empty slice\n",
            "  Y_train_pred = np.nanmean(Y_train_pred_full, axis=0)\n",
            "/usr/local/lib/python3.6/dist-packages/autosklearn/evaluation/train_evaluator.py:197: RuntimeWarning: Mean of empty slice\n",
            "  Y_train_pred = np.nanmean(Y_train_pred_full, axis=0)\n",
            "/usr/local/lib/python3.6/dist-packages/autosklearn/evaluation/train_evaluator.py:197: RuntimeWarning: Mean of empty slice\n",
            "  Y_train_pred = np.nanmean(Y_train_pred_full, axis=0)\n",
            "/usr/local/lib/python3.6/dist-packages/autosklearn/evaluation/train_evaluator.py:197: RuntimeWarning: Mean of empty slice\n",
            "  Y_train_pred = np.nanmean(Y_train_pred_full, axis=0)\n",
            "/usr/local/lib/python3.6/dist-packages/autosklearn/evaluation/train_evaluator.py:197: RuntimeWarning: Mean of empty slice\n",
            "  Y_train_pred = np.nanmean(Y_train_pred_full, axis=0)\n",
            "/usr/local/lib/python3.6/dist-packages/autosklearn/evaluation/train_evaluator.py:197: RuntimeWarning: Mean of empty slice\n",
            "  Y_train_pred = np.nanmean(Y_train_pred_full, axis=0)\n",
            "/usr/local/lib/python3.6/dist-packages/autosklearn/evaluation/train_evaluator.py:197: RuntimeWarning: Mean of empty slice\n",
            "  Y_train_pred = np.nanmean(Y_train_pred_full, axis=0)\n",
            "/usr/local/lib/python3.6/dist-packages/autosklearn/evaluation/train_evaluator.py:197: RuntimeWarning: Mean of empty slice\n",
            "  Y_train_pred = np.nanmean(Y_train_pred_full, axis=0)\n",
            "/usr/local/lib/python3.6/dist-packages/autosklearn/evaluation/train_evaluator.py:197: RuntimeWarning: Mean of empty slice\n",
            "  Y_train_pred = np.nanmean(Y_train_pred_full, axis=0)\n",
            "/usr/local/lib/python3.6/dist-packages/autosklearn/evaluation/train_evaluator.py:197: RuntimeWarning: Mean of empty slice\n",
            "  Y_train_pred = np.nanmean(Y_train_pred_full, axis=0)\n",
            "/usr/local/lib/python3.6/dist-packages/autosklearn/evaluation/train_evaluator.py:197: RuntimeWarning: Mean of empty slice\n",
            "  Y_train_pred = np.nanmean(Y_train_pred_full, axis=0)\n",
            "/usr/local/lib/python3.6/dist-packages/autosklearn/evaluation/train_evaluator.py:197: RuntimeWarning: Mean of empty slice\n",
            "  Y_train_pred = np.nanmean(Y_train_pred_full, axis=0)\n",
            "/usr/local/lib/python3.6/dist-packages/autosklearn/evaluation/train_evaluator.py:197: RuntimeWarning: Mean of empty slice\n",
            "  Y_train_pred = np.nanmean(Y_train_pred_full, axis=0)\n",
            "/usr/local/lib/python3.6/dist-packages/autosklearn/evaluation/train_evaluator.py:197: RuntimeWarning: Mean of empty slice\n",
            "  Y_train_pred = np.nanmean(Y_train_pred_full, axis=0)\n",
            "/usr/local/lib/python3.6/dist-packages/autosklearn/evaluation/train_evaluator.py:197: RuntimeWarning: Mean of empty slice\n",
            "  Y_train_pred = np.nanmean(Y_train_pred_full, axis=0)\n",
            "/usr/local/lib/python3.6/dist-packages/autosklearn/evaluation/train_evaluator.py:197: RuntimeWarning: Mean of empty slice\n",
            "  Y_train_pred = np.nanmean(Y_train_pred_full, axis=0)\n",
            "/usr/local/lib/python3.6/dist-packages/autosklearn/evaluation/train_evaluator.py:197: RuntimeWarning: Mean of empty slice\n",
            "  Y_train_pred = np.nanmean(Y_train_pred_full, axis=0)\n",
            "/usr/local/lib/python3.6/dist-packages/autosklearn/evaluation/train_evaluator.py:197: RuntimeWarning: Mean of empty slice\n",
            "  Y_train_pred = np.nanmean(Y_train_pred_full, axis=0)\n",
            "/usr/local/lib/python3.6/dist-packages/autosklearn/evaluation/train_evaluator.py:197: RuntimeWarning: Mean of empty slice\n",
            "  Y_train_pred = np.nanmean(Y_train_pred_full, axis=0)\n",
            "/usr/local/lib/python3.6/dist-packages/autosklearn/evaluation/train_evaluator.py:197: RuntimeWarning: Mean of empty slice\n",
            "  Y_train_pred = np.nanmean(Y_train_pred_full, axis=0)\n",
            "/usr/local/lib/python3.6/dist-packages/autosklearn/evaluation/train_evaluator.py:197: RuntimeWarning: Mean of empty slice\n",
            "  Y_train_pred = np.nanmean(Y_train_pred_full, axis=0)\n",
            "/usr/local/lib/python3.6/dist-packages/autosklearn/evaluation/train_evaluator.py:197: RuntimeWarning: Mean of empty slice\n",
            "  Y_train_pred = np.nanmean(Y_train_pred_full, axis=0)\n",
            "/usr/local/lib/python3.6/dist-packages/autosklearn/evaluation/train_evaluator.py:197: RuntimeWarning: Mean of empty slice\n",
            "  Y_train_pred = np.nanmean(Y_train_pred_full, axis=0)\n",
            "/usr/local/lib/python3.6/dist-packages/autosklearn/evaluation/train_evaluator.py:197: RuntimeWarning: Mean of empty slice\n",
            "  Y_train_pred = np.nanmean(Y_train_pred_full, axis=0)\n",
            "/usr/local/lib/python3.6/dist-packages/autosklearn/evaluation/train_evaluator.py:197: RuntimeWarning: Mean of empty slice\n",
            "  Y_train_pred = np.nanmean(Y_train_pred_full, axis=0)\n",
            "/usr/local/lib/python3.6/dist-packages/autosklearn/evaluation/train_evaluator.py:197: RuntimeWarning: Mean of empty slice\n",
            "  Y_train_pred = np.nanmean(Y_train_pred_full, axis=0)\n",
            "/usr/local/lib/python3.6/dist-packages/autosklearn/evaluation/train_evaluator.py:197: RuntimeWarning: Mean of empty slice\n",
            "  Y_train_pred = np.nanmean(Y_train_pred_full, axis=0)\n",
            "/usr/local/lib/python3.6/dist-packages/autosklearn/evaluation/train_evaluator.py:197: RuntimeWarning: Mean of empty slice\n",
            "  Y_train_pred = np.nanmean(Y_train_pred_full, axis=0)\n",
            "/usr/local/lib/python3.6/dist-packages/autosklearn/evaluation/train_evaluator.py:197: RuntimeWarning: Mean of empty slice\n",
            "  Y_train_pred = np.nanmean(Y_train_pred_full, axis=0)\n",
            "/usr/local/lib/python3.6/dist-packages/autosklearn/evaluation/train_evaluator.py:197: RuntimeWarning: Mean of empty slice\n",
            "  Y_train_pred = np.nanmean(Y_train_pred_full, axis=0)\n",
            "/usr/local/lib/python3.6/dist-packages/autosklearn/evaluation/train_evaluator.py:197: RuntimeWarning: Mean of empty slice\n",
            "  Y_train_pred = np.nanmean(Y_train_pred_full, axis=0)\n",
            "/usr/local/lib/python3.6/dist-packages/autosklearn/evaluation/train_evaluator.py:197: RuntimeWarning: Mean of empty slice\n",
            "  Y_train_pred = np.nanmean(Y_train_pred_full, axis=0)\n",
            "/usr/local/lib/python3.6/dist-packages/autosklearn/evaluation/train_evaluator.py:197: RuntimeWarning: Mean of empty slice\n",
            "  Y_train_pred = np.nanmean(Y_train_pred_full, axis=0)\n",
            "/usr/local/lib/python3.6/dist-packages/autosklearn/evaluation/train_evaluator.py:197: RuntimeWarning: Mean of empty slice\n",
            "  Y_train_pred = np.nanmean(Y_train_pred_full, axis=0)\n",
            "/usr/local/lib/python3.6/dist-packages/autosklearn/evaluation/train_evaluator.py:197: RuntimeWarning: Mean of empty slice\n",
            "  Y_train_pred = np.nanmean(Y_train_pred_full, axis=0)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "1\n",
            "['/tmp/autosklearn_tmp_3200_6077/.auto-sklearn/ensembles/1.0000000000.ensemble', '/tmp/autosklearn_tmp_3200_6077/.auto-sklearn/ensembles/1.0000000001.ensemble', '/tmp/autosklearn_tmp_3200_6077/.auto-sklearn/ensembles/1.0000000002.ensemble', '/tmp/autosklearn_tmp_3200_6077/.auto-sklearn/ensembles/1.0000000003.ensemble', '/tmp/autosklearn_tmp_3200_6077/.auto-sklearn/ensembles/1.0000000004.ensemble', '/tmp/autosklearn_tmp_3200_6077/.auto-sklearn/ensembles/1.0000000005.ensemble', '/tmp/autosklearn_tmp_3200_6077/.auto-sklearn/ensembles/1.0000000006.ensemble', '/tmp/autosklearn_tmp_3200_6077/.auto-sklearn/ensembles/1.0000000007.ensemble', '/tmp/autosklearn_tmp_3200_6077/.auto-sklearn/ensembles/1.0000000008.ensemble', '/tmp/autosklearn_tmp_3200_6077/.auto-sklearn/ensembles/1.0000000009.ensemble', '/tmp/autosklearn_tmp_3200_6077/.auto-sklearn/ensembles/1.0000000010.ensemble', '/tmp/autosklearn_tmp_3200_6077/.auto-sklearn/ensembles/1.0000000011.ensemble', '/tmp/autosklearn_tmp_3200_6077/.auto-sklearn/ensembles/1.0000000012.ensemble', '/tmp/autosklearn_tmp_3200_6077/.auto-sklearn/ensembles/1.0000000013.ensemble', '/tmp/autosklearn_tmp_3200_6077/.auto-sklearn/ensembles/1.0000000014.ensemble', '/tmp/autosklearn_tmp_3200_6077/.auto-sklearn/ensembles/1.0000000015.ensemble', '/tmp/autosklearn_tmp_3200_6077/.auto-sklearn/ensembles/1.0000000016.ensemble', '/tmp/autosklearn_tmp_3200_6077/.auto-sklearn/ensembles/1.0000000017.ensemble', '/tmp/autosklearn_tmp_3200_6077/.auto-sklearn/ensembles/1.0000000018.ensemble', '/tmp/autosklearn_tmp_3200_6077/.auto-sklearn/ensembles/1.0000000019.ensemble', '/tmp/autosklearn_tmp_3200_6077/.auto-sklearn/ensembles/1.0000000020.ensemble', '/tmp/autosklearn_tmp_3200_6077/.auto-sklearn/ensembles/1.0000000021.ensemble', '/tmp/autosklearn_tmp_3200_6077/.auto-sklearn/ensembles/1.0000000022.ensemble', '/tmp/autosklearn_tmp_3200_6077/.auto-sklearn/ensembles/1.0000000023.ensemble', '/tmp/autosklearn_tmp_3200_6077/.auto-sklearn/ensembles/1.0000000024.ensemble', '/tmp/autosklearn_tmp_3200_6077/.auto-sklearn/ensembles/1.0000000025.ensemble', '/tmp/autosklearn_tmp_3200_6077/.auto-sklearn/ensembles/1.0000000026.ensemble', '/tmp/autosklearn_tmp_3200_6077/.auto-sklearn/ensembles/1.0000000027.ensemble', '/tmp/autosklearn_tmp_3200_6077/.auto-sklearn/ensembles/1.0000000028.ensemble', '/tmp/autosklearn_tmp_3200_6077/.auto-sklearn/ensembles/1.0000000029.ensemble', '/tmp/autosklearn_tmp_3200_6077/.auto-sklearn/ensembles/1.0000000030.ensemble', '/tmp/autosklearn_tmp_3200_6077/.auto-sklearn/ensembles/1.0000000031.ensemble', '/tmp/autosklearn_tmp_3200_6077/.auto-sklearn/ensembles/1.0000000032.ensemble', '/tmp/autosklearn_tmp_3200_6077/.auto-sklearn/ensembles/1.0000000033.ensemble', '/tmp/autosklearn_tmp_3200_6077/.auto-sklearn/ensembles/1.0000000034.ensemble', '/tmp/autosklearn_tmp_3200_6077/.auto-sklearn/ensembles/1.0000000035.ensemble', '/tmp/autosklearn_tmp_3200_6077/.auto-sklearn/ensembles/1.0000000036.ensemble', '/tmp/autosklearn_tmp_3200_6077/.auto-sklearn/ensembles/1.0000000037.ensemble', '/tmp/autosklearn_tmp_3200_6077/.auto-sklearn/ensembles/1.0000000038.ensemble', '/tmp/autosklearn_tmp_3200_6077/.auto-sklearn/ensembles/1.0000000039.ensemble', '/tmp/autosklearn_tmp_3200_6077/.auto-sklearn/ensembles/1.0000000040.ensemble', '/tmp/autosklearn_tmp_3200_6077/.auto-sklearn/ensembles/1.0000000041.ensemble', '/tmp/autosklearn_tmp_3200_6077/.auto-sklearn/ensembles/1.0000000042.ensemble', '/tmp/autosklearn_tmp_3200_6077/.auto-sklearn/ensembles/1.0000000043.ensemble', '/tmp/autosklearn_tmp_3200_6077/.auto-sklearn/ensembles/1.0000000044.ensemble', '/tmp/autosklearn_tmp_3200_6077/.auto-sklearn/ensembles/1.0000000045.ensemble', '/tmp/autosklearn_tmp_3200_6077/.auto-sklearn/ensembles/1.0000000046.ensemble', '/tmp/autosklearn_tmp_3200_6077/.auto-sklearn/ensembles/1.0000000047.ensemble', '/tmp/autosklearn_tmp_3200_6077/.auto-sklearn/ensembles/1.0000000048.ensemble']\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "AutoSklearnClassifier(delete_output_folder_after_terminate=True,\n",
              "           delete_tmp_folder_after_terminate=True,\n",
              "           disable_evaluator_output=False, ensemble_memory_limit=1024,\n",
              "           ensemble_nbest=50, ensemble_size=50, exclude_estimators=None,\n",
              "           exclude_preprocessors=None, get_smac_object_callback=None,\n",
              "           include_estimators=None, include_preprocessors=None,\n",
              "           initial_configurations_via_metalearning=25, logging_config=None,\n",
              "           metadata_directory=None, ml_memory_limit=3072, n_jobs=None,\n",
              "           output_folder=None, per_run_time_limit=360,\n",
              "           resampling_strategy='holdout',\n",
              "           resampling_strategy_arguments=None, seed=1, shared_mode=False,\n",
              "           smac_scenario_args=None, time_left_for_this_task=3600,\n",
              "           tmp_folder=None)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q3k174BFxaS6",
        "colab_type": "code",
        "outputId": "63fc2e84-50cb-4d0e-e3fa-578ad374963c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "y_score = cls4.predict_proba(x_test)\n",
        "y_score.shape"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(4030, 2)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LmRWR69VxaOx",
        "colab_type": "code",
        "outputId": "d78f919b-dd92-4175-8ee7-c10498a33e77",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "import numpy as np\n",
        "y_score4 = np.argmax(y_score, axis = 1)\n",
        "y_score4"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([1, 1, 1, ..., 0, 0, 0])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VZx8Gw4ixaIk",
        "colab_type": "code",
        "outputId": "cb6e35e5-39d5-481c-810d-a2bb4cd2ac60",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "y_pred4 = cls4.predict(x_test)\n",
        "y_pred4"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([1, 1, 1, ..., 0, 0, 0])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OuHlyrwDxq9O",
        "colab_type": "code",
        "outputId": "99788261-ae5a-47db-b5c4-bd389f2cf696",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "source": [
        "from sklearn.metrics import accuracy_score, matthews_corrcoef, roc_auc_score\n",
        "print(\"The accuracy is\", accuracy_score(y_test, y_pred4))\n",
        "print(\"The MCC is\", matthews_corrcoef(y_test, y_pred4))\n",
        "print(\"The AUC-ROC Score is\", roc_auc_score(y_test, y_score4))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The accuracy is 0.9660049627791564\n",
            "The MCC is 0.9321670897959992\n",
            "The AUC-ROC Score is 0.9660049627791563\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WEX1z_jdCKyd",
        "colab_type": "text"
      },
      "source": [
        " # Auto Sklearn on Set 5\n",
        " "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IbVc2qEbxq5P",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!apt-get install swig -y\n",
        "!pip install Cython numpy\n",
        "\n",
        "# sometimes you have to run the next command twice on colab\n",
        "# I haven't figured out why\n",
        "!pip install auto-sklearn"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cZEijzzKCe4j",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import autosklearn.classification\n",
        "cls5 = autosklearn.classification.AutoSklearnClassifier()\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d1095Xz2Chjx",
        "colab_type": "code",
        "outputId": "e2191d9e-cdb6-4fc4-fb88-9aece3f8b622",
        "colab": {
          "resources": {
            "http://localhost:8080/nbextensions/google.colab/files.js": {
              "data": "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",
              "ok": true,
              "headers": [
                [
                  "content-type",
                  "application/javascript"
                ]
              ],
              "status": 200,
              "status_text": ""
            }
          },
          "base_uri": "https://localhost:8080/",
          "height": 74
        }
      },
      "source": [
        "from google.colab import files\n",
        "uploaded = files.upload()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-a548e30a-c900-49e2-afc8-38b953a9c289\" name=\"files[]\" multiple disabled />\n",
              "     <output id=\"result-a548e30a-c900-49e2-afc8-38b953a9c289\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script src=\"/nbextensions/google.colab/files.js\"></script> "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Saving AComp5.csv to AComp5 (2).csv\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HJ1lJ2X4Ckwh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pandas as pd\n",
        "import io\n",
        "df = pd.read_csv(io.BytesIO(uploaded['AComp5.csv']))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3aFdoDaECqw7",
        "colab_type": "code",
        "outputId": "5a76d2f4-88b4-4610-892a-21444f40ad77",
        "colab": {
          "resources": {
            "http://localhost:8080/nbextensions/google.colab/files.js": {
              "data": "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",
              "ok": true,
              "headers": [
                [
                  "content-type",
                  "application/javascript"
                ]
              ],
              "status": 200,
              "status_text": ""
            }
          },
          "base_uri": "https://localhost:8080/",
          "height": 74
        }
      },
      "source": [
        "from google.colab import files\n",
        "uploaded = files.upload()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-796dc6f1-aba9-4e36-bc94-1d9901bf06f0\" name=\"files[]\" multiple disabled />\n",
              "     <output id=\"result-796dc6f1-aba9-4e36-bc94-1d9901bf06f0\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script src=\"/nbextensions/google.colab/files.js\"></script> "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Saving test_AComp5.csv to test_AComp5 (2).csv\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ME4aDqTrCun-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pandas as pd\n",
        "import io\n",
        "df2 = pd.read_csv(io.BytesIO(uploaded['test_AComp5.csv']))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3YFtEMeEDTI6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "x_train = df.iloc[:, 0:20].values\n",
        "y_train = df.iloc[:, 20].values\n",
        "x_test = df2.iloc[:, 0:20].values\n",
        "y_test = df2.iloc[:, 20].values"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7sOBV0ZbDbra",
        "colab_type": "code",
        "outputId": "e22b58e5-6dc6-42be-8efe-879a00990721",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 2400
        }
      },
      "source": [
        "cls5.fit(x_train, y_train)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/autosklearn/evaluation/train_evaluator.py:197: RuntimeWarning: Mean of empty slice\n",
            "  Y_train_pred = np.nanmean(Y_train_pred_full, axis=0)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[WARNING] [2019-05-31 02:56:19,563:EnsembleBuilder(1):13cfff176b765038d2bbacc2b7825d96] No models better than random - using Dummy Score!\n",
            "[WARNING] [2019-05-31 02:56:19,572:EnsembleBuilder(1):13cfff176b765038d2bbacc2b7825d96] No models better than random - using Dummy Score!\n",
            "[WARNING] [2019-05-31 02:56:21,577:EnsembleBuilder(1):13cfff176b765038d2bbacc2b7825d96] No models better than random - using Dummy Score!\n",
            "[WARNING] [2019-05-31 02:56:23,589:EnsembleBuilder(1):13cfff176b765038d2bbacc2b7825d96] No models better than random - using Dummy Score!\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/autosklearn/evaluation/train_evaluator.py:197: RuntimeWarning: Mean of empty slice\n",
            "  Y_train_pred = np.nanmean(Y_train_pred_full, axis=0)\n",
            "/usr/local/lib/python3.6/dist-packages/autosklearn/evaluation/train_evaluator.py:197: RuntimeWarning: Mean of empty slice\n",
            "  Y_train_pred = np.nanmean(Y_train_pred_full, axis=0)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[WARNING] [2019-05-31 02:56:26,658:smac.intensification.intensification.Intensifier] Challenger was the same as the current incumbent; Skipping challenger\n",
            "[WARNING] [2019-05-31 02:56:26,658:smac.intensification.intensification.Intensifier] Challenger was the same as the current incumbent; Skipping challenger\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/autosklearn/evaluation/train_evaluator.py:197: RuntimeWarning: Mean of empty slice\n",
            "  Y_train_pred = np.nanmean(Y_train_pred_full, axis=0)\n",
            "/usr/local/lib/python3.6/dist-packages/autosklearn/evaluation/train_evaluator.py:197: RuntimeWarning: Mean of empty slice\n",
            "  Y_train_pred = np.nanmean(Y_train_pred_full, axis=0)\n",
            "/usr/local/lib/python3.6/dist-packages/autosklearn/evaluation/train_evaluator.py:197: RuntimeWarning: Mean of empty slice\n",
            "  Y_train_pred = np.nanmean(Y_train_pred_full, axis=0)\n",
            "/usr/local/lib/python3.6/dist-packages/autosklearn/evaluation/train_evaluator.py:197: RuntimeWarning: Mean of empty slice\n",
            "  Y_train_pred = np.nanmean(Y_train_pred_full, axis=0)\n",
            "/usr/local/lib/python3.6/dist-packages/autosklearn/evaluation/train_evaluator.py:197: RuntimeWarning: Mean of empty slice\n",
            "  Y_train_pred = np.nanmean(Y_train_pred_full, axis=0)\n",
            "/usr/local/lib/python3.6/dist-packages/autosklearn/evaluation/train_evaluator.py:197: RuntimeWarning: Mean of empty slice\n",
            "  Y_train_pred = np.nanmean(Y_train_pred_full, axis=0)\n",
            "/usr/local/lib/python3.6/dist-packages/autosklearn/evaluation/train_evaluator.py:197: RuntimeWarning: Mean of empty slice\n",
            "  Y_train_pred = np.nanmean(Y_train_pred_full, axis=0)\n",
            "/usr/local/lib/python3.6/dist-packages/autosklearn/evaluation/train_evaluator.py:197: RuntimeWarning: Mean of empty slice\n",
            "  Y_train_pred = np.nanmean(Y_train_pred_full, axis=0)\n",
            "/usr/local/lib/python3.6/dist-packages/autosklearn/evaluation/train_evaluator.py:197: RuntimeWarning: Mean of empty slice\n",
            "  Y_train_pred = np.nanmean(Y_train_pred_full, axis=0)\n",
            "/usr/local/lib/python3.6/dist-packages/autosklearn/evaluation/train_evaluator.py:197: RuntimeWarning: Mean of empty slice\n",
            "  Y_train_pred = np.nanmean(Y_train_pred_full, axis=0)\n",
            "/usr/local/lib/python3.6/dist-packages/autosklearn/evaluation/train_evaluator.py:197: RuntimeWarning: Mean of empty slice\n",
            "  Y_train_pred = np.nanmean(Y_train_pred_full, axis=0)\n",
            "/usr/local/lib/python3.6/dist-packages/autosklearn/evaluation/train_evaluator.py:197: RuntimeWarning: Mean of empty slice\n",
            "  Y_train_pred = np.nanmean(Y_train_pred_full, axis=0)\n",
            "/usr/local/lib/python3.6/dist-packages/autosklearn/evaluation/train_evaluator.py:197: RuntimeWarning: Mean of empty slice\n",
            "  Y_train_pred = np.nanmean(Y_train_pred_full, axis=0)\n",
            "/usr/local/lib/python3.6/dist-packages/autosklearn/evaluation/train_evaluator.py:197: RuntimeWarning: Mean of empty slice\n",
            "  Y_train_pred = np.nanmean(Y_train_pred_full, axis=0)\n",
            "/usr/local/lib/python3.6/dist-packages/autosklearn/evaluation/train_evaluator.py:197: RuntimeWarning: Mean of empty slice\n",
            "  Y_train_pred = np.nanmean(Y_train_pred_full, axis=0)\n",
            "/usr/local/lib/python3.6/dist-packages/autosklearn/evaluation/train_evaluator.py:197: RuntimeWarning: Mean of empty slice\n",
            "  Y_train_pred = np.nanmean(Y_train_pred_full, axis=0)\n",
            "/usr/local/lib/python3.6/dist-packages/autosklearn/evaluation/train_evaluator.py:197: RuntimeWarning: Mean of empty slice\n",
            "  Y_train_pred = np.nanmean(Y_train_pred_full, axis=0)\n",
            "/usr/local/lib/python3.6/dist-packages/autosklearn/evaluation/train_evaluator.py:197: RuntimeWarning: Mean of empty slice\n",
            "  Y_train_pred = np.nanmean(Y_train_pred_full, axis=0)\n",
            "/usr/local/lib/python3.6/dist-packages/autosklearn/evaluation/train_evaluator.py:197: RuntimeWarning: Mean of empty slice\n",
            "  Y_train_pred = np.nanmean(Y_train_pred_full, axis=0)\n",
            "/usr/local/lib/python3.6/dist-packages/autosklearn/evaluation/train_evaluator.py:197: RuntimeWarning: Mean of empty slice\n",
            "  Y_train_pred = np.nanmean(Y_train_pred_full, axis=0)\n",
            "/usr/local/lib/python3.6/dist-packages/autosklearn/evaluation/train_evaluator.py:197: RuntimeWarning: Mean of empty slice\n",
            "  Y_train_pred = np.nanmean(Y_train_pred_full, axis=0)\n",
            "/usr/local/lib/python3.6/dist-packages/autosklearn/evaluation/train_evaluator.py:197: RuntimeWarning: Mean of empty slice\n",
            "  Y_train_pred = np.nanmean(Y_train_pred_full, axis=0)\n",
            "/usr/local/lib/python3.6/dist-packages/autosklearn/evaluation/train_evaluator.py:197: RuntimeWarning: Mean of empty slice\n",
            "  Y_train_pred = np.nanmean(Y_train_pred_full, axis=0)\n",
            "/usr/local/lib/python3.6/dist-packages/autosklearn/evaluation/train_evaluator.py:197: RuntimeWarning: Mean of empty slice\n",
            "  Y_train_pred = np.nanmean(Y_train_pred_full, axis=0)\n",
            "/usr/local/lib/python3.6/dist-packages/autosklearn/evaluation/train_evaluator.py:197: RuntimeWarning: Mean of empty slice\n",
            "  Y_train_pred = np.nanmean(Y_train_pred_full, axis=0)\n",
            "/usr/local/lib/python3.6/dist-packages/autosklearn/evaluation/train_evaluator.py:197: RuntimeWarning: Mean of empty slice\n",
            "  Y_train_pred = np.nanmean(Y_train_pred_full, axis=0)\n",
            "/usr/local/lib/python3.6/dist-packages/autosklearn/evaluation/train_evaluator.py:197: RuntimeWarning: Mean of empty slice\n",
            "  Y_train_pred = np.nanmean(Y_train_pred_full, axis=0)\n",
            "/usr/local/lib/python3.6/dist-packages/autosklearn/evaluation/train_evaluator.py:197: RuntimeWarning: Mean of empty slice\n",
            "  Y_train_pred = np.nanmean(Y_train_pred_full, axis=0)\n",
            "/usr/local/lib/python3.6/dist-packages/autosklearn/evaluation/train_evaluator.py:197: RuntimeWarning: Mean of empty slice\n",
            "  Y_train_pred = np.nanmean(Y_train_pred_full, axis=0)\n",
            "/usr/local/lib/python3.6/dist-packages/autosklearn/evaluation/train_evaluator.py:197: RuntimeWarning: Mean of empty slice\n",
            "  Y_train_pred = np.nanmean(Y_train_pred_full, axis=0)\n",
            "/usr/local/lib/python3.6/dist-packages/autosklearn/evaluation/train_evaluator.py:197: RuntimeWarning: Mean of empty slice\n",
            "  Y_train_pred = np.nanmean(Y_train_pred_full, axis=0)\n",
            "/usr/local/lib/python3.6/dist-packages/autosklearn/evaluation/train_evaluator.py:197: RuntimeWarning: Mean of empty slice\n",
            "  Y_train_pred = np.nanmean(Y_train_pred_full, axis=0)\n",
            "/usr/local/lib/python3.6/dist-packages/autosklearn/evaluation/train_evaluator.py:197: RuntimeWarning: Mean of empty slice\n",
            "  Y_train_pred = np.nanmean(Y_train_pred_full, axis=0)\n",
            "/usr/local/lib/python3.6/dist-packages/autosklearn/evaluation/train_evaluator.py:197: RuntimeWarning: Mean of empty slice\n",
            "  Y_train_pred = np.nanmean(Y_train_pred_full, axis=0)\n",
            "/usr/local/lib/python3.6/dist-packages/autosklearn/evaluation/train_evaluator.py:197: RuntimeWarning: Mean of empty slice\n",
            "  Y_train_pred = np.nanmean(Y_train_pred_full, axis=0)\n",
            "/usr/local/lib/python3.6/dist-packages/autosklearn/evaluation/train_evaluator.py:197: RuntimeWarning: Mean of empty slice\n",
            "  Y_train_pred = np.nanmean(Y_train_pred_full, axis=0)\n",
            "/usr/local/lib/python3.6/dist-packages/autosklearn/evaluation/train_evaluator.py:197: RuntimeWarning: Mean of empty slice\n",
            "  Y_train_pred = np.nanmean(Y_train_pred_full, axis=0)\n",
            "/usr/local/lib/python3.6/dist-packages/autosklearn/evaluation/train_evaluator.py:197: RuntimeWarning: Mean of empty slice\n",
            "  Y_train_pred = np.nanmean(Y_train_pred_full, axis=0)\n",
            "/usr/local/lib/python3.6/dist-packages/autosklearn/evaluation/train_evaluator.py:197: RuntimeWarning: Mean of empty slice\n",
            "  Y_train_pred = np.nanmean(Y_train_pred_full, axis=0)\n",
            "/usr/local/lib/python3.6/dist-packages/autosklearn/evaluation/train_evaluator.py:197: RuntimeWarning: Mean of empty slice\n",
            "  Y_train_pred = np.nanmean(Y_train_pred_full, axis=0)\n",
            "/usr/local/lib/python3.6/dist-packages/autosklearn/evaluation/train_evaluator.py:197: RuntimeWarning: Mean of empty slice\n",
            "  Y_train_pred = np.nanmean(Y_train_pred_full, axis=0)\n",
            "/usr/local/lib/python3.6/dist-packages/autosklearn/evaluation/train_evaluator.py:197: RuntimeWarning: Mean of empty slice\n",
            "  Y_train_pred = np.nanmean(Y_train_pred_full, axis=0)\n",
            "/usr/local/lib/python3.6/dist-packages/autosklearn/evaluation/train_evaluator.py:197: RuntimeWarning: Mean of empty slice\n",
            "  Y_train_pred = np.nanmean(Y_train_pred_full, axis=0)\n",
            "/usr/local/lib/python3.6/dist-packages/autosklearn/evaluation/train_evaluator.py:197: RuntimeWarning: Mean of empty slice\n",
            "  Y_train_pred = np.nanmean(Y_train_pred_full, axis=0)\n",
            "/usr/local/lib/python3.6/dist-packages/autosklearn/evaluation/train_evaluator.py:197: RuntimeWarning: Mean of empty slice\n",
            "  Y_train_pred = np.nanmean(Y_train_pred_full, axis=0)\n",
            "/usr/local/lib/python3.6/dist-packages/autosklearn/evaluation/train_evaluator.py:197: RuntimeWarning: Mean of empty slice\n",
            "  Y_train_pred = np.nanmean(Y_train_pred_full, axis=0)\n",
            "/usr/local/lib/python3.6/dist-packages/autosklearn/evaluation/train_evaluator.py:197: RuntimeWarning: Mean of empty slice\n",
            "  Y_train_pred = np.nanmean(Y_train_pred_full, axis=0)\n",
            "/usr/local/lib/python3.6/dist-packages/autosklearn/evaluation/train_evaluator.py:197: RuntimeWarning: Mean of empty slice\n",
            "  Y_train_pred = np.nanmean(Y_train_pred_full, axis=0)\n",
            "/usr/local/lib/python3.6/dist-packages/autosklearn/evaluation/train_evaluator.py:197: RuntimeWarning: Mean of empty slice\n",
            "  Y_train_pred = np.nanmean(Y_train_pred_full, axis=0)\n",
            "/usr/local/lib/python3.6/dist-packages/autosklearn/evaluation/train_evaluator.py:197: RuntimeWarning: Mean of empty slice\n",
            "  Y_train_pred = np.nanmean(Y_train_pred_full, axis=0)\n",
            "/usr/local/lib/python3.6/dist-packages/autosklearn/evaluation/train_evaluator.py:197: RuntimeWarning: Mean of empty slice\n",
            "  Y_train_pred = np.nanmean(Y_train_pred_full, axis=0)\n",
            "/usr/local/lib/python3.6/dist-packages/autosklearn/evaluation/train_evaluator.py:197: RuntimeWarning: Mean of empty slice\n",
            "  Y_train_pred = np.nanmean(Y_train_pred_full, axis=0)\n",
            "/usr/local/lib/python3.6/dist-packages/autosklearn/evaluation/train_evaluator.py:197: RuntimeWarning: Mean of empty slice\n",
            "  Y_train_pred = np.nanmean(Y_train_pred_full, axis=0)\n",
            "/usr/local/lib/python3.6/dist-packages/autosklearn/evaluation/train_evaluator.py:197: RuntimeWarning: Mean of empty slice\n",
            "  Y_train_pred = np.nanmean(Y_train_pred_full, axis=0)\n",
            "/usr/local/lib/python3.6/dist-packages/autosklearn/evaluation/train_evaluator.py:197: RuntimeWarning: Mean of empty slice\n",
            "  Y_train_pred = np.nanmean(Y_train_pred_full, axis=0)\n",
            "/usr/local/lib/python3.6/dist-packages/autosklearn/evaluation/train_evaluator.py:197: RuntimeWarning: Mean of empty slice\n",
            "  Y_train_pred = np.nanmean(Y_train_pred_full, axis=0)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "1\n",
            "['/tmp/autosklearn_tmp_4103_5627/.auto-sklearn/ensembles/1.0000000000.ensemble', '/tmp/autosklearn_tmp_4103_5627/.auto-sklearn/ensembles/1.0000000001.ensemble', '/tmp/autosklearn_tmp_4103_5627/.auto-sklearn/ensembles/1.0000000002.ensemble', '/tmp/autosklearn_tmp_4103_5627/.auto-sklearn/ensembles/1.0000000003.ensemble', '/tmp/autosklearn_tmp_4103_5627/.auto-sklearn/ensembles/1.0000000004.ensemble', '/tmp/autosklearn_tmp_4103_5627/.auto-sklearn/ensembles/1.0000000005.ensemble', '/tmp/autosklearn_tmp_4103_5627/.auto-sklearn/ensembles/1.0000000006.ensemble', '/tmp/autosklearn_tmp_4103_5627/.auto-sklearn/ensembles/1.0000000007.ensemble', '/tmp/autosklearn_tmp_4103_5627/.auto-sklearn/ensembles/1.0000000008.ensemble', '/tmp/autosklearn_tmp_4103_5627/.auto-sklearn/ensembles/1.0000000009.ensemble', '/tmp/autosklearn_tmp_4103_5627/.auto-sklearn/ensembles/1.0000000010.ensemble', '/tmp/autosklearn_tmp_4103_5627/.auto-sklearn/ensembles/1.0000000011.ensemble', '/tmp/autosklearn_tmp_4103_5627/.auto-sklearn/ensembles/1.0000000012.ensemble', '/tmp/autosklearn_tmp_4103_5627/.auto-sklearn/ensembles/1.0000000013.ensemble', '/tmp/autosklearn_tmp_4103_5627/.auto-sklearn/ensembles/1.0000000014.ensemble', '/tmp/autosklearn_tmp_4103_5627/.auto-sklearn/ensembles/1.0000000015.ensemble', '/tmp/autosklearn_tmp_4103_5627/.auto-sklearn/ensembles/1.0000000016.ensemble', '/tmp/autosklearn_tmp_4103_5627/.auto-sklearn/ensembles/1.0000000017.ensemble', '/tmp/autosklearn_tmp_4103_5627/.auto-sklearn/ensembles/1.0000000018.ensemble', '/tmp/autosklearn_tmp_4103_5627/.auto-sklearn/ensembles/1.0000000019.ensemble', '/tmp/autosklearn_tmp_4103_5627/.auto-sklearn/ensembles/1.0000000020.ensemble', '/tmp/autosklearn_tmp_4103_5627/.auto-sklearn/ensembles/1.0000000021.ensemble', '/tmp/autosklearn_tmp_4103_5627/.auto-sklearn/ensembles/1.0000000022.ensemble', '/tmp/autosklearn_tmp_4103_5627/.auto-sklearn/ensembles/1.0000000023.ensemble', '/tmp/autosklearn_tmp_4103_5627/.auto-sklearn/ensembles/1.0000000024.ensemble', '/tmp/autosklearn_tmp_4103_5627/.auto-sklearn/ensembles/1.0000000025.ensemble', '/tmp/autosklearn_tmp_4103_5627/.auto-sklearn/ensembles/1.0000000026.ensemble', '/tmp/autosklearn_tmp_4103_5627/.auto-sklearn/ensembles/1.0000000027.ensemble', '/tmp/autosklearn_tmp_4103_5627/.auto-sklearn/ensembles/1.0000000028.ensemble', '/tmp/autosklearn_tmp_4103_5627/.auto-sklearn/ensembles/1.0000000029.ensemble', '/tmp/autosklearn_tmp_4103_5627/.auto-sklearn/ensembles/1.0000000030.ensemble', '/tmp/autosklearn_tmp_4103_5627/.auto-sklearn/ensembles/1.0000000031.ensemble', '/tmp/autosklearn_tmp_4103_5627/.auto-sklearn/ensembles/1.0000000032.ensemble', '/tmp/autosklearn_tmp_4103_5627/.auto-sklearn/ensembles/1.0000000033.ensemble', '/tmp/autosklearn_tmp_4103_5627/.auto-sklearn/ensembles/1.0000000034.ensemble', '/tmp/autosklearn_tmp_4103_5627/.auto-sklearn/ensembles/1.0000000035.ensemble', '/tmp/autosklearn_tmp_4103_5627/.auto-sklearn/ensembles/1.0000000036.ensemble', '/tmp/autosklearn_tmp_4103_5627/.auto-sklearn/ensembles/1.0000000037.ensemble', '/tmp/autosklearn_tmp_4103_5627/.auto-sklearn/ensembles/1.0000000038.ensemble', '/tmp/autosklearn_tmp_4103_5627/.auto-sklearn/ensembles/1.0000000039.ensemble', '/tmp/autosklearn_tmp_4103_5627/.auto-sklearn/ensembles/1.0000000040.ensemble', '/tmp/autosklearn_tmp_4103_5627/.auto-sklearn/ensembles/1.0000000041.ensemble', '/tmp/autosklearn_tmp_4103_5627/.auto-sklearn/ensembles/1.0000000042.ensemble', '/tmp/autosklearn_tmp_4103_5627/.auto-sklearn/ensembles/1.0000000043.ensemble', '/tmp/autosklearn_tmp_4103_5627/.auto-sklearn/ensembles/1.0000000044.ensemble', '/tmp/autosklearn_tmp_4103_5627/.auto-sklearn/ensembles/1.0000000045.ensemble', '/tmp/autosklearn_tmp_4103_5627/.auto-sklearn/ensembles/1.0000000046.ensemble', '/tmp/autosklearn_tmp_4103_5627/.auto-sklearn/ensembles/1.0000000047.ensemble', '/tmp/autosklearn_tmp_4103_5627/.auto-sklearn/ensembles/1.0000000048.ensemble', '/tmp/autosklearn_tmp_4103_5627/.auto-sklearn/ensembles/1.0000000049.ensemble', '/tmp/autosklearn_tmp_4103_5627/.auto-sklearn/ensembles/1.0000000050.ensemble', '/tmp/autosklearn_tmp_4103_5627/.auto-sklearn/ensembles/1.0000000051.ensemble', '/tmp/autosklearn_tmp_4103_5627/.auto-sklearn/ensembles/1.0000000052.ensemble', '/tmp/autosklearn_tmp_4103_5627/.auto-sklearn/ensembles/1.0000000053.ensemble', '/tmp/autosklearn_tmp_4103_5627/.auto-sklearn/ensembles/1.0000000054.ensemble', '/tmp/autosklearn_tmp_4103_5627/.auto-sklearn/ensembles/1.0000000055.ensemble', '/tmp/autosklearn_tmp_4103_5627/.auto-sklearn/ensembles/1.0000000056.ensemble']\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "AutoSklearnClassifier(delete_output_folder_after_terminate=True,\n",
              "           delete_tmp_folder_after_terminate=True,\n",
              "           disable_evaluator_output=False, ensemble_memory_limit=1024,\n",
              "           ensemble_nbest=50, ensemble_size=50, exclude_estimators=None,\n",
              "           exclude_preprocessors=None, get_smac_object_callback=None,\n",
              "           include_estimators=None, include_preprocessors=None,\n",
              "           initial_configurations_via_metalearning=25, logging_config=None,\n",
              "           metadata_directory=None, ml_memory_limit=3072, n_jobs=None,\n",
              "           output_folder=None, per_run_time_limit=360,\n",
              "           resampling_strategy='holdout',\n",
              "           resampling_strategy_arguments=None, seed=1, shared_mode=False,\n",
              "           smac_scenario_args=None, time_left_for_this_task=3600,\n",
              "           tmp_folder=None)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NBY8dvy4Dnvn",
        "colab_type": "code",
        "outputId": "1d560cc7-e8d2-428b-e147-3a80a00df8fb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "y_score = cls5.predict_proba(x_test)\n",
        "y_score.shape"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(4030, 2)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gjQJ1gdDDns_",
        "colab_type": "code",
        "outputId": "b1e028f6-c7ab-4871-905a-76415e94abfc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "import numpy as np\n",
        "y_score5 = np.argmax(y_score, axis = 1)\n",
        "y_score5"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([1, 1, 1, ..., 0, 0, 0])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YNeaJ_2mDnqk",
        "colab_type": "code",
        "outputId": "a8bb1d3c-6b5b-44f2-96b3-61808abf3afd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "y_pred5 = cls5.predict(x_test)\n",
        "y_pred5"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([1, 1, 1, ..., 0, 0, 0])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Wf1Jk4-6DvLK",
        "colab_type": "code",
        "outputId": "cc680113-06b3-4b66-c2fc-e799693f2f63",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "source": [
        "from sklearn.metrics import accuracy_score, matthews_corrcoef, roc_auc_score\n",
        "print(\"The accuracy is\", accuracy_score(y_test, y_pred5))\n",
        "print(\"The MCC is\", matthews_corrcoef(y_test, y_pred5))\n",
        "print(\"The AUC-ROC Score is\", roc_auc_score(y_test, y_score5))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The accuracy is 0.9652605459057072\n",
            "The MCC is 0.930638454010571\n",
            "The AUC-ROC Score is 0.9652605459057071\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GCv1E1tuDvIE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}